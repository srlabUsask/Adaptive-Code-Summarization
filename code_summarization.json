[
	{
		"id": "http://zotero.org/users/15747588/items/34AZ344I",
		"type": "paper-conference",
		"event-title": "Deep Learning for Code Workshop",
		"title": "Code summarization: Do transformers really understand code?",
		"author": [
			{
				"family": "Sontakke",
				"given": "Ankita Nandkishor"
			},
			{
				"family": "Patwardhan",
				"given": "Manasi"
			},
			{
				"family": "Vig",
				"given": "Lovekesh"
			},
			{
				"family": "Medicherla",
				"given": "Raveendra Kumar"
			},
			{
				"family": "Naik",
				"given": "Ravindra"
			},
			{
				"family": "Shroff",
				"given": "Gautam"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/6ITLAHGD",
		"type": "paper-conference",
		"abstract": "Source code summarization aims to generate natural language descriptions of code snippets. Many existing studies learn the syntactic and semantic knowledge of code snippets from their token sequences and Abstract Syntax Trees (ASTs). They use the learned code representations as input to code summarization models, which can accordingly generate summaries describing source code. Traditional models traverse ASTs as sequences or split ASTs into paths as input. However, the former loses the structural properties of ASTs, and the latter destroys the overall structure of ASTs. Therefore, comprehensively capturing the structural features of ASTs in learning code representations for source code summarization remains a challenging problem to be solved. In this paper, we propose M2TS, a Multi-scale Multi-modal approach based on Transformer for source code Summarization. M2TS uses a multi-scale AST feature extraction method, which can extract the structures of ASTs more completely and accurately at multiple local and global levels. To complement missing semantic information in ASTs, we also obtain code token features, and further combine them with the extracted AST features using a cross modality fusion method that not only fuses the syntactic and contextual semantic information of source code, but also highlights the key features of each modality. We conduct experiments on two Java and one Python datasets, and the experimental results demonstrate that M2TS outperforms current state-of-the-art methods. We release our code at https://github.com/TranSMS/M2TS.",
		"collection-title": "ICPC '22",
		"container-title": "Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension",
		"DOI": "10.1145/3524610.3527907",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9298-3",
		"note": "event-place: Virtual Event",
		"page": "24–35",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "M2TS: multi-scale multi-modal approach based on transformer for source code summarization",
		"URL": "https://doi.org/10.1145/3524610.3527907",
		"author": [
			{
				"family": "Gao",
				"given": "Yuexiu"
			},
			{
				"family": "Lyu",
				"given": "Chen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/FIT53TIB",
		"type": "paper-conference",
		"container-title": "2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)",
		"DOI": "10.1109/SANER53432.2022.00112",
		"page": "935-946",
		"title": "Assemble Foundation Models for Automatic Code Summarization",
		"author": [
			{
				"family": "Gu",
				"given": "Jian"
			},
			{
				"family": "Salza",
				"given": "Pasquale"
			},
			{
				"family": "Gall",
				"given": "Harald C."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/NA3XXDY9",
		"type": "paper-conference",
		"abstract": "Code summarization, the task of generating useful comments given the code, has long been of interest. Most of the existing code summarization models are trained and validated on widely-used code comment benchmark datasets. However, little is known about the quality of the benchmark datasets built from real-world projects. Are the benchmark datasets as good as expected? To bridge the gap, we conduct a systematic research to assess and improve the quality of four benchmark datasets widely used for code summarization tasks. First, we propose an automated code-comment cleaning tool that can accurately detect noisy data caused by inappropriate data preprocessing operations from existing benchmark datasets. Then, we apply the tool to further assess the data quality of the four benchmark datasets, based on the detected noises. Finally, we conduct comparative experiments to investigate the impact of noisy data on the performance of code summarization models. The results show that these data preprocessing noises widely exist in all four benchmark datasets, and removing these noisy data leads to a significant improvement on the performance of code summarization. We believe that the findings and insights will enable a better understanding of data quality in code summarization tasks, and pave the way for relevant research and practice.",
		"collection-title": "ESEC/FSE 2022",
		"container-title": "Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
		"DOI": "10.1145/3540250.3549145",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9413-0",
		"note": "event-place: Singapore, Singapore",
		"page": "107–119",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Are we building on the rock? on the importance of data preprocessing for code summarization",
		"URL": "https://doi.org/10.1145/3540250.3549145",
		"author": [
			{
				"family": "Shi",
				"given": "Lin"
			},
			{
				"family": "Mu",
				"given": "Fangwen"
			},
			{
				"family": "Chen",
				"given": "Xiao"
			},
			{
				"family": "Wang",
				"given": "Song"
			},
			{
				"family": "Wang",
				"given": "Junjie"
			},
			{
				"family": "Yang",
				"given": "Ye"
			},
			{
				"family": "Li",
				"given": "Ge"
			},
			{
				"family": "Xia",
				"given": "Xin"
			},
			{
				"family": "Wang",
				"given": "Qing"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/RDDZXTR2",
		"type": "article-journal",
		"container-title": "IEEE Transactions on Software Engineering",
		"DOI": "10.1109/TSE.2024.3422274",
		"issue": "8",
		"page": "2077-2095",
		"title": "Esale: Enhancing Code-Summary Alignment Learning for Source Code Summarization",
		"volume": "50",
		"author": [
			{
				"family": "Fang",
				"given": "Chunrong"
			},
			{
				"family": "Sun",
				"given": "Weisong"
			},
			{
				"family": "Chen",
				"given": "Yuchen"
			},
			{
				"family": "Chen",
				"given": "Xiao"
			},
			{
				"family": "Wei",
				"given": "Zhao"
			},
			{
				"family": "Zhang",
				"given": "Quanjun"
			},
			{
				"family": "You",
				"given": "Yudu"
			},
			{
				"family": "Luo",
				"given": "Bin"
			},
			{
				"family": "Liu",
				"given": "Yang"
			},
			{
				"family": "Chen",
				"given": "Zhenyu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/3G5MAFRS",
		"type": "paper-conference",
		"container-title": "2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
		"DOI": "10.1109/ASE51524.2021.9678724",
		"page": "155-166",
		"title": "EditSum: A Retrieve-and-Edit Framework for Source Code Summarization",
		"author": [
			{
				"family": "Li",
				"given": "Jia Allen"
			},
			{
				"family": "Li",
				"given": "Yongmin"
			},
			{
				"family": "Li",
				"given": "Ge"
			},
			{
				"family": "Hu",
				"given": "Xing"
			},
			{
				"family": "Xia",
				"given": "Xin"
			},
			{
				"family": "Jin",
				"given": "Zhi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/YFQRNVAB",
		"type": "article-journal",
		"abstract": "With the fast development of large software projects, automatic code summarization techniques, which summarize the main functionalities of a piece of code using natural languages as comments, play essential roles in helping developers understand and maintain large software projects. Many research efforts have been devoted to building automatic code summarization approaches. Typical code summarization approaches are based on deep learning models. They transform the task into a sequence-to-sequence task, which inputs source code and outputs summarizations in natural languages. All code summarization models impose different input size limits, such as 50 to 10,000, for the input source code. However, how the input size limit affects the performance of code summarization models still remains under-explored. In this article, we first conduct an empirical study to investigate the impacts of different input size limits on the quality of generated code comments. To our surprise, experiments on multiple models and datasets reveal that setting a low input size limit, such as 20, does not necessarily reduce the quality of generated comments.Based on this finding, we further propose to use function signatures instead of full source code to summarize the main functionalities first and then input the function signatures into code summarization models. Experiments and statistical results show that inputs with signatures are, on average, more than 2 percentage points better than inputs without signatures and thus demonstrate the effectiveness of involving function signatures in code summarization. We also invite programmers to do a questionnaire to evaluate the quality of code summaries generated by two inputs with different truncation levels. The results show that function signatures generate, on average, 9.2% more high-quality comments than full code.",
		"container-title": "ACM Trans. Softw. Eng. Methodol.",
		"DOI": "10.1145/3652156",
		"ISSN": "1049-331X",
		"issue": "6",
		"title": "Do Code Summarization Models Process Too Much Information? Function Signature May Be All That Is Needed",
		"URL": "https://doi.org/10.1145/3652156",
		"volume": "33",
		"author": [
			{
				"family": "Ding",
				"given": "Xi"
			},
			{
				"family": "Peng",
				"given": "Rui"
			},
			{
				"family": "Chen",
				"given": "Xiangping"
			},
			{
				"family": "Huang",
				"given": "Yuan"
			},
			{
				"family": "Bian",
				"given": "Jing"
			},
			{
				"family": "Zheng",
				"given": "Zibin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024",
					6
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/P6T8C24C",
		"type": "paper-conference",
		"abstract": "In recent years, research in the domain of source code summarization has adopted data-driven techniques pioneered in machine translation (MT). Automatic evaluation metrics such as BLEU, METEOR, and ROUGE, are fundamental to the evaluation of MT systems and have been adopted as proxies of human evaluation in the code summarization domain. However, the extent to which automatic metrics agree with the gold standard of human evaluation has not been evaluated on code summarization tasks. Despite this, marginal improvements in metric scores are often used to discriminate between the performance of competing summarization models. In this paper, we present a critical exploration of the applicability and interpretation of automatic metrics as evaluation techniques for code summarization tasks. We conduct an empirical study with 226 human annotators to assess the degree to which automatic metrics reflect human evaluation. Results indicate that metric improvements of less than 2 points do not guarantee systematic improvements in summarization quality, and are unreliable as proxies of human evaluation. When the difference between metric scores for two summarization approaches increases but remains within 5 points, some metrics such as METEOR and chrF become highly reliable proxies, whereas others, such as corpus BLEU, remain unreliable. Based on these findings, we make several recommendations for the use of automatic metrics to discriminate model performance in code summarization.",
		"collection-title": "ESEC/FSE 2021",
		"container-title": "Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
		"DOI": "10.1145/3468264.3468588",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8562-6",
		"note": "event-place: Athens, Greece",
		"page": "1105–1116",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reassessing automatic evaluation metrics for code summarization tasks",
		"URL": "https://doi.org/10.1145/3468264.3468588",
		"author": [
			{
				"family": "Roy",
				"given": "Devjeet"
			},
			{
				"family": "Fakhoury",
				"given": "Sarah"
			},
			{
				"family": "Arnaoudova",
				"given": "Venera"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/CDKHJ69A",
		"type": "paper-conference",
		"abstract": "Source code summarization involves creating brief descriptions of source code in natural language. These descriptions are a key component of software documentation such as JavaDocs. Automatic code summarization is a prized target of software engineering research, due to the high value summaries have to programmers and the simultaneously high cost of writing and maintaining documentation by hand. Current work is almost all based on machine models trained via big data input. Large datasets of examples of code and summaries of that code are used to train an e.g. encoder-decoder neural model. Then the output predictions of the model are evaluated against a set of reference summaries. The input is code not seen by the model, and the prediction is compared to a reference. The means by which a prediction is compared to a reference is essentially word overlap, calculated via a metric such as BLEU or ROUGE. The problem with using word overlap is that not all words in a sentence have the same importance, and many words have synonyms. The result is that calculated similarity may not match the perceived similarity by human readers. In this paper, we conduct an experiment to measure the degree to which various word overlap metrics correlate to human-rated similarity of predicted and reference summaries. We evaluate alternatives based on current work in semantic similarity metrics and propose recommendations for evaluation of source code summarization.",
		"collection-title": "ICPC '22",
		"container-title": "Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension",
		"DOI": "10.1145/3524610.3527909",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9298-3",
		"note": "event-place: Virtual Event",
		"page": "36–47",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Semantic similarity metrics for evaluating source code summarization",
		"URL": "https://doi.org/10.1145/3524610.3527909",
		"author": [
			{
				"family": "Haque",
				"given": "Sakib"
			},
			{
				"family": "Eberhart",
				"given": "Zachary"
			},
			{
				"family": "Bansal",
				"given": "Aakash"
			},
			{
				"family": "McMillan",
				"given": "Collin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/7EJSWKIE",
		"type": "article-journal",
		"abstract": "Code summaries help developers comprehend programs and reduce their time to infer the program functionalities during software maintenance. Recent efforts resort to deep learning techniques such as sequence-to-sequence models for generating accurate code summaries, among which Transformer-based approaches have achieved promising performance. However, effectively integrating the code structure information into the Transformer is under-explored in this task domain. In this article, we propose a novel approach named SG-Trans to incorporate code structural properties into Transformer. Specifically, we inject the local symbolic information (e.g., code tokens and statements) and global syntactic structure (e.g., dataflow graph) into the self-attention module of Transformer as inductive bias. To further capture the hierarchical characteristics of code, the local information and global structure are designed to distribute in the attention heads of lower layers and high layers of Transformer. Extensive evaluation shows the superior performance of SG-Trans over the state-of-the-art approaches. Compared with the best-performing baseline, SG-Trans still improves 1.4% and 2.0% on two benchmark datasets, respectively, in terms of METEOR score, a metric widely used for measuring generation quality.",
		"container-title": "ACM Trans. Softw. Eng. Methodol.",
		"DOI": "10.1145/3522674",
		"ISSN": "1049-331X",
		"issue": "1",
		"title": "Code Structure–Guided Transformer for Source Code Summarization",
		"URL": "https://doi.org/10.1145/3522674",
		"volume": "32",
		"author": [
			{
				"family": "Gao",
				"given": "Shuzheng"
			},
			{
				"family": "Gao",
				"given": "Cuiyun"
			},
			{
				"family": "He",
				"given": "Yulan"
			},
			{
				"family": "Zeng",
				"given": "Jichuan"
			},
			{
				"family": "Nie",
				"given": "Lunyiu"
			},
			{
				"family": "Xia",
				"given": "Xin"
			},
			{
				"family": "Lyu",
				"given": "Michael"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					2
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/GA4T8Y9Y",
		"type": "paper-conference",
		"container-title": "2023 IEEE/ACM 31st International Conference on Program Comprehension (ICPC)",
		"DOI": "10.1109/ICPC58990.2023.00026",
		"page": "113-124",
		"title": "Interpretation-based Code Summarization",
		"author": [
			{
				"family": "Geng",
				"given": "Mingyang"
			},
			{
				"family": "Wang",
				"given": "Shangwen"
			},
			{
				"family": "Dong",
				"given": "Dezun"
			},
			{
				"family": "Wang",
				"given": "Haotian"
			},
			{
				"family": "Cao",
				"given": "Shaomeng"
			},
			{
				"family": "Zhang",
				"given": "Kechi"
			},
			{
				"family": "Jin",
				"given": "Zhi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/G2HXM7LK",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2012.14710",
		"journalAbbreviation": "arXiv preprint arXiv:2012.14710",
		"title": "Code summarization with structure-induced transformer",
		"author": [
			{
				"family": "Wu",
				"given": "Hongqiu"
			},
			{
				"family": "Zhao",
				"given": "Hai"
			},
			{
				"family": "Zhang",
				"given": "Min"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/TM7PU9WQ",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2407.07959",
		"journalAbbreviation": "arXiv preprint arXiv:2407.07959",
		"title": "Source code summarization in the era of large language models",
		"author": [
			{
				"family": "Sun",
				"given": "Weisong"
			},
			{
				"family": "Miao",
				"given": "Yun"
			},
			{
				"family": "Li",
				"given": "Yuekang"
			},
			{
				"family": "Zhang",
				"given": "Hongyu"
			},
			{
				"family": "Fang",
				"given": "Chunrong"
			},
			{
				"family": "Liu",
				"given": "Yi"
			},
			{
				"family": "Deng",
				"given": "Gelei"
			},
			{
				"family": "Liu",
				"given": "Yang"
			},
			{
				"family": "Chen",
				"given": "Zhenyu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/LMY6QWDR",
		"type": "paper-conference",
		"abstract": "Software developers spend a great deal of time reading and understanding code that is poorly-documented, written by other developers, or developed using differing styles. During the past decade, researchers have investigated techniques for automatically documenting code to improve comprehensibility. In particular, recent advances in deep learning have led to sophisticated summary generation techniques that convert functions or methods to simple English strings that succinctly describe that code's behavior. However, automatic summarization techniques are assessed using internal metrics such as BLEU scores, which measure natural language properties in translational models, or ROUGE scores, which measure overlap with human-written text. Unfortunately, these metrics do not necessarily capture how machine-generated code summaries actually affect human comprehension or developer productivity.We conducted a human study involving both university students and professional developers (n = 45). Participants reviewed Java methods and summaries and answered established program comprehension questions. In addition, participants completed coding tasks given summaries as specifications. Critically, the experiment controlled the source of the summaries: for a given method, some participants were shown human-written text and some were shown machine-generated text.We found that participants performed significantly better (p = 0.029) using human-written summaries versus machine-generated summaries. However, we found no evidence to support that participants perceive human- and machine-generated summaries to have different qualities. In addition, participants' performance showed no correlation with the BLEU and ROUGE scores often used to assess the quality of machine-generated summaries. These results suggest a need for revised metrics to assess and guide automatic summarization techniques.",
		"collection-title": "ICPC '20",
		"container-title": "Proceedings of the 28th International Conference on Program Comprehension",
		"DOI": "10.1145/3387904.3389258",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7958-8",
		"note": "event-place: Seoul, Republic of Korea",
		"page": "2–13",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A Human Study of Comprehension and Code Summarization",
		"URL": "https://doi.org/10.1145/3387904.3389258",
		"author": [
			{
				"family": "Stapleton",
				"given": "Sean"
			},
			{
				"family": "Gambhir",
				"given": "Yashmeet"
			},
			{
				"family": "LeClair",
				"given": "Alexander"
			},
			{
				"family": "Eberhart",
				"given": "Zachary"
			},
			{
				"family": "Weimer",
				"given": "Westley"
			},
			{
				"family": "Leach",
				"given": "Kevin"
			},
			{
				"family": "Huang",
				"given": "Yu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/8IQHYIM9",
		"type": "article-journal",
		"container-title": "IEEE Transactions on Software Engineering",
		"DOI": "10.1109/TSE.2017.2664836",
		"issue": "12",
		"page": "1095-1109",
		"title": "Autofolding for Source Code Summarization",
		"volume": "43",
		"author": [
			{
				"family": "Fowkes",
				"given": "Jaroslav"
			},
			{
				"family": "Chanthirasegaran",
				"given": "Pankajan"
			},
			{
				"family": "Ranca",
				"given": "Razvan"
			},
			{
				"family": "Allamanis",
				"given": "Miltiadis"
			},
			{
				"family": "Lapata",
				"given": "Mirella"
			},
			{
				"family": "Sutton",
				"given": "Charles"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/T86JKVQP",
		"type": "paper-conference",
		"container-title": "Advances in Neural Information Processing Systems",
		"publisher": "Curran Associates, Inc.",
		"title": "Code Generation as a Dual Task of Code Summarization",
		"URL": "https://proceedings.neurips.cc/paper_files/paper/2019/file/e52ad5c9f751f599492b4f087ed7ecfc-Paper.pdf",
		"volume": "32",
		"author": [
			{
				"family": "Wei",
				"given": "Bolin"
			},
			{
				"family": "Li",
				"given": "Ge"
			},
			{
				"family": "Xia",
				"given": "Xin"
			},
			{
				"family": "Fu",
				"given": "Zhiyi"
			},
			{
				"family": "Jin",
				"given": "Zhi"
			}
		],
		"editor": [
			{
				"family": "Wallach",
				"given": "H."
			},
			{
				"family": "Larochelle",
				"given": "H."
			},
			{
				"family": "Beygelzimer",
				"given": "A."
			},
			{
				"family": "Alché-Buc",
				"given": "F.",
				"dropping-particle": "d'"
			},
			{
				"family": "Fox",
				"given": "E."
			},
			{
				"family": "Garnett",
				"given": "R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/LBQWHURN",
		"type": "article-journal",
		"abstract": "Code summarization plays a pivotal role in the field of software engineering by offering developers a concise natural language comprehension of source code semantics. As software complexity continues to escalate, code summarization confronts various challenges, including discrepancies between source code and summarization, the absence of crucial or up-to-date information, and the inefficiency and resource demands of manual summarization. To address these challenges, Automatic Source Code Summarization (ASCS) has garnered widespread attention. This paper presents a comprehensive review and synthesis of ASCS research. It aims to provide an in-depth understanding of the core issues and challenges inherent in each phase of ASCS, illustrated with specific examples and application scenarios. Around of the core phases of ASCS including data collection, source code modeling, the generation of code summaries, and the assessment of their quality, the paper thoroughly compiles and assesses existing datasets, categorizes and examines prevalent source code modeling techniques, and delves into the methods for generating and evaluating the quality of code summaries. Concluding with an exploration of future research avenues and emerging trends, this paper serves as a guide for readers to grasp the cutting-edge developments in this field, enriched by the analysis of pivotal research contributions.",
		"container-title": "Empirical Software Engineering",
		"DOI": "10.1007/s10664-024-10553-6",
		"ISSN": "1573-7616",
		"issue": "6",
		"journalAbbreviation": "Empirical Software Engineering",
		"page": "162",
		"title": "A review of automatic source code summarization",
		"URL": "https://doi.org/10.1007/s10664-024-10553-6",
		"volume": "29",
		"author": [
			{
				"family": "Zhang",
				"given": "Xuejun"
			},
			{
				"family": "Hou",
				"given": "Xia"
			},
			{
				"family": "Qiao",
				"given": "Xiuming"
			},
			{
				"family": "Song",
				"given": "Wenfeng"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					7
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/IFSUUVMM",
		"type": "paper-conference",
		"abstract": "Source code summaries are important for program comprehension and maintenance. However, there are plenty of programs with missing, outdated, or mismatched summaries. Recently, deep learning techniques have been exploited to automatically generate summaries for given code snippets. To achieve a profound understanding of how far we are from solving this problem and provide suggestions to future research, in this paper, we conduct a systematic and in-depth analysis of 5 state-of-the-art neural code summarization models on 6 widely used BLEU variants, 4 pre-processing operations and their combinations, and 3 widely used datasets. The evaluation results show that some important factors have a great influence on the model evaluation, especially on the performance of models and the ranking among the models. However, these factors might be easily overlooked. Specifically, (1) the BLEU metric widely used in existing work of evaluating code summarization models has many variants. Ignoring the differences among these variants could greatly affect the validity of the claimed results. Besides, we discover and resolve an important and previously unknown bug in BLEU calculation in a commonly-used software package. Furthermore, we conduct human evaluations and find that the metric BLEU-DC is most correlated to human perception; (2) code preprocessing choices can have a large (from -18% to +25%) impact on the summarization performance and should not be neglected. We also explore the aggregation of pre-processing combinations and boost the performance of models; (3) some important characteristics of datasets (corpus sizes, data splitting methods, and duplication ratios) have a significant impact on model evaluation. Based on the experimental results, we give actionable suggestions for evaluating code summarization and choosing the best method in different scenarios. We also build a shared code summarization toolbox to facilitate future research.",
		"collection-title": "ICSE '22",
		"container-title": "Proceedings of the 44th International Conference on Software Engineering",
		"DOI": "10.1145/3510003.3510060",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9221-1",
		"note": "event-place: Pittsburgh, Pennsylvania",
		"page": "1597–1608",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the evaluation of neural code summarization",
		"URL": "https://doi.org/10.1145/3510003.3510060",
		"author": [
			{
				"family": "Shi",
				"given": "Ensheng"
			},
			{
				"family": "Wang",
				"given": "Yanlin"
			},
			{
				"family": "Du",
				"given": "Lun"
			},
			{
				"family": "Chen",
				"given": "Junjie"
			},
			{
				"family": "Han",
				"given": "Shi"
			},
			{
				"family": "Zhang",
				"given": "Hongyu"
			},
			{
				"family": "Zhang",
				"given": "Dongmei"
			},
			{
				"family": "Sun",
				"given": "Hongbin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/PUBIEAHQ",
		"type": "article-journal",
		"abstract": "Source code summarization aims to generate concise descriptions for code snippets in a natural language, thereby facilitates program comprehension and software maintenance. In this paper, we propose a novel approach–GSCS–to automatically generate summaries for Java methods, which leverages both semantic and structural information of the code snippets. To this end, GSCS utilizes Graph Attention Networks to process the tokenized abstract syntax tree of the program, which employ a multi-head attention mechanism to learn node features in diverse representation sub-spaces, and aggregate features by assigning different weights to its neighbor nodes. GSCS further harnesses an additional RNN-based sequence model to obtain the semantic features and optimizes the structure by combining its output with a transformed embedding layer. We evaluate our approach on two widely-adopted Java datasets; the experiment results confirm that GSCS outperforms the state-of-the-art baselines.",
		"container-title": "Journal of Systems and Software",
		"DOI": "https://doi.org/10.1016/j.jss.2022.111257",
		"ISSN": "0164-1212",
		"page": "111257",
		"title": "Automatic source code summarization with graph attention networks",
		"URL": "https://www.sciencedirect.com/science/article/pii/S0164121222000279",
		"volume": "188",
		"author": [
			{
				"family": "Zhou",
				"given": "Yu"
			},
			{
				"family": "Shen",
				"given": "Juanjuan"
			},
			{
				"family": "Zhang",
				"given": "Xiaoqing"
			},
			{
				"family": "Yang",
				"given": "Wenhua"
			},
			{
				"family": "Han",
				"given": "Tingting"
			},
			{
				"family": "Chen",
				"given": "Taolue"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/3SYTRAHA",
		"type": "paper-conference",
		"container-title": "Advances in Neural Information Processing Systems",
		"page": "56660–56672",
		"publisher": "Curran Associates, Inc.",
		"title": "On-the-Fly Adapting Code Summarization on Trainable Cost-Effective Language Models",
		"URL": "https://proceedings.neurips.cc/paper_files/paper/2023/file/b16e6de5fbbdcb2df237aa66b302bc17-Paper-Conference.pdf",
		"volume": "36",
		"author": [
			{
				"family": "Cai",
				"given": "Yufan"
			},
			{
				"family": "Lin",
				"given": "Yun"
			},
			{
				"family": "Liu",
				"given": "Chenyan"
			},
			{
				"family": "Wu",
				"given": "Jinglian"
			},
			{
				"family": "Zhang",
				"given": "Yifan"
			},
			{
				"family": "Liu",
				"given": "Yiming"
			},
			{
				"family": "Gong",
				"given": "Yeyun"
			},
			{
				"family": "Dong",
				"given": "Jin Song"
			}
		],
		"editor": [
			{
				"family": "Oh",
				"given": "A."
			},
			{
				"family": "Naumann",
				"given": "T."
			},
			{
				"family": "Globerson",
				"given": "A."
			},
			{
				"family": "Saenko",
				"given": "K."
			},
			{
				"family": "Hardt",
				"given": "M."
			},
			{
				"family": "Levine",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/LXXRQX9E",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2206.00804",
		"journalAbbreviation": "arXiv preprint arXiv:2206.00804",
		"title": "Learning code summarization from a small and local dataset",
		"author": [
			{
				"family": "Ahmed",
				"given": "Toufique"
			},
			{
				"family": "Devanbu",
				"given": "Premkumar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/UNC2DABD",
		"type": "paper-conference",
		"abstract": "We present a novel tool, TASSAL, that automatically creates a summary of each source file in a project by folding its least salient code regions. The intended use-case for our tool is the first-look problem: to help developers who are unfamiliar with a new codebase and are attempting to understand it. TASSAL is intended to aid developers in this task by folding away less informative regions of code and allowing them to focus their efforts on the most informative ones. While modern code editors do provide code folding to selectively hide blocks of code, it is impractical to use as folding decisions must be made manually or based on simple rules. We find through a case study that TASSAL is strongly preferred by experienced developers over simple folding baselines, demonstrating its usefulness. In short, we strongly believe TASSAL can aid program comprehension by turning code folding into a usable and valuable tool. A video highlighting the main features of TASSAL can be found at https://youtu.be/_yu7JZgiBA4.",
		"collection-title": "ICSE '16",
		"container-title": "Proceedings of the 38th International Conference on Software Engineering Companion",
		"DOI": "10.1145/2889160.2889171",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-4205-6",
		"note": "event-place: Austin, Texas",
		"page": "649–652",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "TASSAL: autofolding for source code summarization",
		"URL": "https://doi.org/10.1145/2889160.2889171",
		"author": [
			{
				"family": "Fowkes",
				"given": "Jaroslav"
			},
			{
				"family": "Chanthirasegaran",
				"given": "Pankajan"
			},
			{
				"family": "Ranca",
				"given": "Razvan"
			},
			{
				"family": "Allamanis",
				"given": "Miltiadis"
			},
			{
				"family": "Lapata",
				"given": "Mirella"
			},
			{
				"family": "Sutton",
				"given": "Charles"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2016"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/8HYQZU55",
		"type": "paper-conference",
		"abstract": "In this paper, we present an emerging source code summarization technique that uses topic modeling to select keywords and topics as summaries for source code. Our approach organizes the topics in source code into a hierarchy, with more general topics near the top of the hierarchy. In this way, we present the software's highest-level functionality first, before lower-level details. This is an advantage over previous approaches based on topic models, that only present groups of related keywords without a hierarchy. We conducted a preliminary user study that found our approach selects keywords and topics that the participants found to be accurate in a majority of cases.",
		"collection-title": "ICPC 2014",
		"container-title": "Proceedings of the 22nd International Conference on Program Comprehension",
		"DOI": "10.1145/2597008.2597793",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-2879-1",
		"note": "event-place: Hyderabad, India",
		"page": "291–294",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Improving topic model source code summarization",
		"URL": "https://doi.org/10.1145/2597008.2597793",
		"author": [
			{
				"family": "McBurney",
				"given": "Paul W."
			},
			{
				"family": "Liu",
				"given": "Cheng"
			},
			{
				"family": "McMillan",
				"given": "Collin"
			},
			{
				"family": "Weninger",
				"given": "Tim"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2014"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/S25TKB8H",
		"type": "paper-conference",
		"abstract": "Code summarization aims to generate brief natural language descriptions for source codes. The state-of-the-art approaches follow a transformer-based encoder-decoder architecture. As the source code is highly structured and follows strict grammars, its Abstract Syntax Tree (AST) is widely used for encoding structural information. However, ASTs are much longer than the corresponding source code. Existing approaches ignore the size constraint and simply feed the whole linearized AST into the encoders. We argue that such a simple process makes it difficult to extract the truly useful dependency relations from the overlong input sequence. It also incurs significant computational overhead since each node needs to apply self-attention to all other nodes in the AST. To encode the AST more effectively and efficiently, we propose AST-Trans in this paper which exploits two types of node relationships in the AST: ancestor-descendant and sibling relationships. It applies the tree-structured attention to dynamically allocate weights for relevant nodes and exclude irrelevant nodes based on these two relationships. We further propose an efficient implementation to support fast parallel computation for tree-structure attention. On the two code summarization datasets, experimental results show that AST-Trans significantly outperforms the state-of-the-arts while being times more efficient than standard transformers1.",
		"collection-title": "ICSE '22",
		"container-title": "Proceedings of the 44th International Conference on Software Engineering",
		"DOI": "10.1145/3510003.3510224",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9221-1",
		"note": "event-place: Pittsburgh, Pennsylvania",
		"page": "150–162",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AST-trans: code summarization with efficient tree-structured attention",
		"URL": "https://doi.org/10.1145/3510003.3510224",
		"author": [
			{
				"family": "Tang",
				"given": "Ze"
			},
			{
				"family": "Shen",
				"given": "Xiaoyu"
			},
			{
				"family": "Li",
				"given": "Chuanyi"
			},
			{
				"family": "Ge",
				"given": "Jidong"
			},
			{
				"family": "Huang",
				"given": "Liguo"
			},
			{
				"family": "Zhu",
				"given": "Zhelin"
			},
			{
				"family": "Luo",
				"given": "Bin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/ZEB88YYR",
		"type": "paper-conference",
		"abstract": "Code summarization generates brief natural language descriptions of source code pieces, which can assist developers in understanding code and reduce documentation workload. Recent neural models on code summarization are trained and evaluated on large-scale multi-project datasets consisting of independent code-summary pairs. Despite the technical advances, their effectiveness on a specific project is rarely explored. In practical scenarios, however, developers are more concerned with generating high-quality summaries for their working projects. And these projects may not maintain sufficient documentation, hence having few historical code-summary pairs. To this end, we investigate low-resource project-specific code summarization, a novel task more consistent with the developers’ requirements. To better characterize project-specific knowledge with limited training samples, we propose a meta transfer learning method by incorporating a lightweight fine-tuning mechanism into a meta-learning framework. Experimental results on nine real-world projects verify the superiority of our method over alternative ones and reveal how the project-specific knowledge is learned.",
		"collection-title": "ASE '22",
		"container-title": "Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering",
		"DOI": "10.1145/3551349.3556909",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9475-8",
		"note": "event-place: Rochester, MI, USA",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Low-Resources Project-Specific Code Summarization",
		"URL": "https://doi.org/10.1145/3551349.3556909",
		"author": [
			{
				"family": "Xie",
				"given": "Rui"
			},
			{
				"family": "Hu",
				"given": "Tianxiang"
			},
			{
				"family": "Ye",
				"given": "Wei"
			},
			{
				"family": "Zhang",
				"given": "Shikun"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/PYRGNF94",
		"type": "paper-conference",
		"abstract": "Code summarization with deep learning has been widely studied in recent years. Current deep learning models for code summarization generally follow the principle in neural machine translation and adopt the encoder-decoder framework, where the encoder learns the semantic representations from source code and the decoder transforms the learnt representations into human-readable text that describes the functionality of code snippets. Despite they achieve the new state-of-the-art performance, we notice that current models often either generate less fluent summaries, or fail to capture the core functionality, since they usually focus on a single type of code representations. As such we propose GypSum, a new deep learning model that learns hybrid representations using graph attention neural networks and a pre-trained programming and natural language model. We introduce particular edges related to the control flow of a code snippet into the abstract syntax tree for graph construction, and design two encoders to learn from the graph and the token sequence of source code, respectively. We modify the encoder-decoder sublayer in the Transformer's decoder to fuse the representations and propose a dual-copy mechanism to facilitate summary generation. Experimental results demonstrate the superior performance of GypSum over existing code summarization models.",
		"collection-title": "ICPC '22",
		"container-title": "Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension",
		"DOI": "10.1145/3524610.3527903",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9298-3",
		"note": "event-place: Virtual Event",
		"page": "12–23",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "GypSum: learning hybrid representations for code summarization",
		"URL": "https://doi.org/10.1145/3524610.3527903",
		"author": [
			{
				"family": "Wang",
				"given": "Yu"
			},
			{
				"family": "Dong",
				"given": "Yu"
			},
			{
				"family": "Lu",
				"given": "Xuesong"
			},
			{
				"family": "Zhou",
				"given": "Aoying"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/G4V5X53P",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2404.08018",
		"journalAbbreviation": "arXiv preprint arXiv:2404.08018",
		"title": "Analyzing the performance of large language models on code summarization",
		"author": [
			{
				"family": "Haldar",
				"given": "Rajarshi"
			},
			{
				"family": "Hockenmaier",
				"given": "Julia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/WTSPC9ZC",
		"type": "article-journal",
		"abstract": "Source code documentation is an important process for software project maintenance and management. The documentation process always consumes a lot of time and effort from human experts. The software project document should be concise and clear without any ambiguity. Existing source code documentation tools like JavaDoc are very limited in the market. Also, tools can identify only the predefined methods of corresponding programming languages. In this paper, a novel system is proposed to automate the source code documentation process for C programming language using the source code summarization technique of NLP. The core component of this proposed system i.e., Software Word Usage Model (SWUM) build using Context-Free Grammars and NLP preprocessing techniques. This system can successfully generate the documentation for a C program along with predefined and user-defined methods using Natural Language Generation technique. This proposed system can document a program in two major formats; method-based, abstract level and statement-based, detailed level. The proposed system efficiency is evaluated by comparing system-generated source code documentation with an expert generated documentation. Results obtained from that comparison shows that the proposed system can give better performance for small and medium-size software projects.",
		"container-title": "Third International Conference on Computing and Network Communications (CoCoNet'19)",
		"DOI": "10.1016/j.procs.2020.04.273",
		"ISSN": "1877-0509",
		"journalAbbreviation": "Procedia Computer Science",
		"page": "2522-2531",
		"title": "Automatic Source Code Documentation using Code Summarization Technique of NLP",
		"URL": "https://www.sciencedirect.com/science/article/pii/S1877050920312655",
		"volume": "171",
		"author": [
			{
				"family": "Arthur",
				"given": "Menaka Pushpa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020",
					1,
					1
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/MRF476N9",
		"type": "paper-conference",
		"event-title": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
		"page": "1-13",
		"title": "Evaluating Code Summarization Techniques: A New Metric and an Empirical Characterization",
		"author": [
			{
				"family": "Mastropaolo",
				"given": "Antonio"
			},
			{
				"family": "Ciniselli",
				"given": "Matteo"
			},
			{
				"family": "Di Penta",
				"given": "Massimiliano"
			},
			{
				"family": "Bavota",
				"given": "Gabriele"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/PHQ2NAWP",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2006.05405",
		"journalAbbreviation": "arXiv preprint arXiv:2006.05405",
		"title": "Retrieval-augmented generation for code summarization via hybrid gnn",
		"author": [
			{
				"family": "Liu",
				"given": "Shangqing"
			},
			{
				"family": "Chen",
				"given": "Yu"
			},
			{
				"family": "Xie",
				"given": "Xiaofei"
			},
			{
				"family": "Siow",
				"given": "Jingkai"
			},
			{
				"family": "Liu",
				"given": "Yang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/3YIJA8YL",
		"type": "paper-conference",
		"event-title": "Proceedings of the 36th international conference on Software engineering",
		"page": "390-401",
		"title": "Improving automated source code summarization via an eye-tracking study of programmers",
		"author": [
			{
				"family": "Rodeghero",
				"given": "Paige"
			},
			{
				"family": "McMillan",
				"given": "Collin"
			},
			{
				"family": "McBurney",
				"given": "Paul W"
			},
			{
				"family": "Bosch",
				"given": "Nigel"
			},
			{
				"family": "D'Mello",
				"given": "Sidney"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2014"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/6L44GTNR",
		"type": "paper-conference",
		"event-title": "2019 International Joint Conference on Neural Networks (IJCNN)",
		"ISBN": "1-7281-1985-5",
		"page": "1-8",
		"publisher": "IEEE",
		"title": "Automatic source code summarization with extended tree-lstm",
		"author": [
			{
				"family": "Shido",
				"given": "Yusuke"
			},
			{
				"family": "Kobayashi",
				"given": "Yasuaki"
			},
			{
				"family": "Yamamoto",
				"given": "Akihiro"
			},
			{
				"family": "Miyamoto",
				"given": "Atsushi"
			},
			{
				"family": "Matsumura",
				"given": "Tadayuki"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/QRRTN8UL",
		"type": "paper-conference",
		"event-title": "Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering",
		"page": "1-5",
		"title": "Few-shot training LLMs for project-specific code-summarization",
		"author": [
			{
				"family": "Ahmed",
				"given": "Toufique"
			},
			{
				"family": "Devanbu",
				"given": "Premkumar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/HT3PTZQC",
		"type": "article-journal",
		"abstract": "Automatically generating summaries for source code has emerged as a valuable task in software development. While state-of-the-art (SOTA) approaches have demonstrated significant efficacy in summarizing general code, they seldom concern code summarization for a specific project. Project-specific code summarization (PCS) poses special challenges due to the scarce availability of training data and the unique styles of different projects. In this paper, we empirically analyze the performance of Large Language Models (LLMs) on PCS tasks. Our study reveals that using appropriate prompts is an effective way to solicit LLMs for generating project-specific code summaries. Based on these findings, we propose a novel project-specific code summarization approach called P-CodeSum. P-CodeSum gathers a repository-level pool of (code, summary) examples to characterize the project-specific features. Then, it trains a neural prompt selector on a high-quality dataset crafted by LLMs using the example pool. The prompt selector offers relevant and high-quality prompts for LLMs to generate project-specific summaries. We evaluate against a variety of baseline approaches on six PCS datasets. Experimental results show that the P-CodeSum improves the performance by 5.9% (RLPG) to 101.51% (CodeBERT) on BLEU-4 compared to the state-of-the-art approaches in project-specific code summarization.",
		"container-title": "Journal of Systems and Software",
		"DOI": "10.1016/j.jss.2024.112149",
		"ISSN": "0164-1212",
		"journalAbbreviation": "Journal of Systems and Software",
		"page": "112149",
		"title": "Project-specific code summarization with in-context learning",
		"URL": "https://www.sciencedirect.com/science/article/pii/S0164121224001948",
		"volume": "216",
		"author": [
			{
				"family": "Yun",
				"given": "Shangbo"
			},
			{
				"family": "Lin",
				"given": "Shuhuai"
			},
			{
				"family": "Gu",
				"given": "Xiaodong"
			},
			{
				"family": "Shen",
				"given": "Beijun"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					1
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/M7GL9E2M",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2305.11074",
		"journalAbbreviation": "arXiv preprint arXiv:2305.11074",
		"title": "Tram: A Token-level Retrieval-augmented Mechanism for Source Code Summarization",
		"author": [
			{
				"family": "Ye",
				"given": "Tong"
			},
			{
				"family": "Wu",
				"given": "Lingfei"
			},
			{
				"family": "Ma",
				"given": "Tengfei"
			},
			{
				"family": "Zhang",
				"given": "Xuhong"
			},
			{
				"family": "Du",
				"given": "Yangkai"
			},
			{
				"family": "Liu",
				"given": "Peiyu"
			},
			{
				"family": "Ji",
				"given": "Shouling"
			},
			{
				"family": "Wang",
				"given": "Wenhai"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/T72FRV4M",
		"type": "article-journal",
		"container-title": "CoRR",
		"journalAbbreviation": "CoRR",
		"title": "Constructing effective in-context demonstration for code intelligence tasks: An empirical study",
		"author": [
			{
				"family": "Gao",
				"given": "Shuzheng"
			},
			{
				"family": "Wen",
				"given": "Xin-Cheng"
			},
			{
				"family": "Gao",
				"given": "Cuiyun"
			},
			{
				"family": "Wang",
				"given": "Wenxuan"
			},
			{
				"family": "Lyu",
				"given": "Michael R"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/P6CDCIVR",
		"type": "paper-conference",
		"event-title": "2023 IEEE/ACM 31st International Conference on Program Comprehension (ICPC)",
		"ISBN": "979-8-3503-3750-1",
		"page": "101-112",
		"publisher": "IEEE",
		"title": "Label Smoothing Improves Neural Source Code Summarization",
		"author": [
			{
				"family": "Haque",
				"given": "Sakib"
			},
			{
				"family": "Bansal",
				"given": "Aakash"
			},
			{
				"family": "McMillan",
				"given": "Collin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/XDI7JVGR",
		"type": "article-journal",
		"container-title": "International Journal of Software Engineering and Knowledge Engineering",
		"ISSN": "0218-1940",
		"issue": "11n12",
		"journalAbbreviation": "International Journal of Software Engineering and Knowledge Engineering",
		"page": "1765-1786",
		"title": "Enhancing Code Summarization with Graph Embedding and Pre-Trained Model",
		"volume": "33",
		"author": [
			{
				"family": "Li",
				"given": "Lixuan"
			},
			{
				"family": "Li",
				"given": "Jie"
			},
			{
				"family": "Xu",
				"given": "Yihui"
			},
			{
				"family": "Zhu",
				"given": "Hao"
			},
			{
				"family": "Zhang",
				"given": "Xiaofang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/CTIDLA4H",
		"type": "article-journal",
		"container-title": "Science of Computer Programming",
		"ISSN": "0167-6423",
		"journalAbbreviation": "Science of Computer Programming",
		"page": "102925",
		"title": "CLG-Trans: Contrastive learning for code summarization via graph attention-based transformer",
		"volume": "226",
		"author": [
			{
				"family": "Zeng",
				"given": "Jianwei"
			},
			{
				"family": "He",
				"given": "Yutong"
			},
			{
				"family": "Zhang",
				"given": "Tao"
			},
			{
				"family": "Xu",
				"given": "Zhou"
			},
			{
				"family": "Han",
				"given": "Qiang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/B334KTFE",
		"type": "paper-conference",
		"event-title": "2017 24th Asia-Pacific Software Engineering Conference (APSEC)",
		"ISBN": "1-5386-3681-6",
		"page": "199-208",
		"publisher": "IEEE",
		"title": "Method level text summarization for java code using nano-patterns",
		"author": [
			{
				"family": "Rai",
				"given": "Sawan"
			},
			{
				"family": "Gaikwad",
				"given": "Tejaswini"
			},
			{
				"family": "Jain",
				"given": "Sparshi"
			},
			{
				"family": "Gupta",
				"given": "Atul"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/B8JZCYQ4",
		"type": "paper-conference",
		"container-title": "2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)",
		"DOI": "10.1109/ICSE48619.2023.00013",
		"page": "5-16",
		"title": "One Adapter for All Programming Languages? Adapter Tuning for Code Search and Summarization",
		"author": [
			{
				"family": "Wang",
				"given": "Deze"
			},
			{
				"family": "Chen",
				"given": "Boxing"
			},
			{
				"family": "Li",
				"given": "Shanshan"
			},
			{
				"family": "Luo",
				"given": "Wei"
			},
			{
				"family": "Peng",
				"given": "Shaoliang"
			},
			{
				"family": "Dong",
				"given": "Wei"
			},
			{
				"family": "Liao",
				"given": "Xiangke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/GD58II9C",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2208.06318",
		"journalAbbreviation": "arXiv preprint arXiv:2208.06318",
		"title": "Towards Code Summarization of APIs Using NLP Techniques",
		"author": [
			{
				"family": "Naghshzan",
				"given": "AmirHossein"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/BAEAHMI8",
		"type": "paper-conference",
		"event-title": "Findings of the Association for Computational Linguistics: ACL 2023",
		"page": "11427-11441",
		"title": "BLOCSUM: Block scope-based source code summarization via shared block representation",
		"author": [
			{
				"family": "Choi",
				"given": "YunSeok"
			},
			{
				"family": "Kim",
				"given": "Hyojun"
			},
			{
				"family": "Lee",
				"given": "Jee-Hyong"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/NU265M9C",
		"type": "paper-conference",
		"container-title": "2024 IEEE International Conference on Software Maintenance and Evolution (ICSME)",
		"DOI": "10.1109/ICSME58944.2024.00073",
		"page": "689-700",
		"title": "Icing on the Cake: Automatic Code Summarization at Ericsson",
		"author": [
			{
				"family": "Sridhara",
				"given": "Giriprasad"
			},
			{
				"family": "Roychowdhury",
				"given": "Sujoy"
			},
			{
				"family": "Soman",
				"given": "Sumit"
			},
			{
				"family": "G.",
				"given": "Ranjani H."
			},
			{
				"family": "Britto",
				"given": "Ricardo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/ZTV2YN3L",
		"type": "paper-conference",
		"event-title": "2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",
		"ISBN": "1-6654-4207-7",
		"page": "3436-3441",
		"publisher": "IEEE",
		"title": "Transformer-XL with graph neural network for source code summarization",
		"author": [
			{
				"family": "Zhang",
				"given": "Xiaoling"
			},
			{
				"family": "Yang",
				"given": "Shouguo"
			},
			{
				"family": "Duan",
				"given": "Luqian"
			},
			{
				"family": "Lang",
				"given": "Zhe"
			},
			{
				"family": "Shi",
				"given": "Zhiqiang"
			},
			{
				"family": "Sun",
				"given": "Limin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/E49JPK3D",
		"type": "paper-conference",
		"container-title": "2020 IEEE 6th International Conference on Computer and Communications (ICCC)",
		"DOI": "10.1109/ICCC51575.2020.9345066",
		"page": "664-668",
		"title": "Utilizing Keywords in Source Code to Improve Code Summarization",
		"author": [
			{
				"family": "Liu",
				"given": "Peng-fei"
			},
			{
				"family": "Wang",
				"given": "Xiao-meng"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/NG5MYG4B",
		"type": "article-journal",
		"abstract": "Automatic source code summarization system aims to generate a valuable natural language description for a program, which can facilitate software development and maintenance, code categorization, and retrieval. However, previous sequence-based research did not consider the long-distance dependence and highly structured characteristics of source code simultaneously. In this article, we present a Transformer-based Graph-Augmented Source Code Summarization (GA-SCS), which can effectively incorporate inherent structural and textual features of source code to generate an effective code description. Specifically, we develop a graph-based structure feature extraction scheme leveraging abstract syntax tree and graph attention networks to mine global syntactic information. And then, to take full advantage of the lexical and syntactic information of code snippets, we extend the original attention to a syntax-informed self-attention mechanism in our encoder. In the training process, we also adopt a reinforcement learning strategy to enhance the readability and informativity of generated code summaries. We utilize the Java dataset and Python dataset to evaluate the performance of different models. Experimental results demonstrate that our GA-SCS model outperforms all competitive methods on BLEU, METEOR, ROUGE, and human evaluations.",
		"container-title": "ACM Trans. Asian Low-Resour. Lang. Inf. Process.",
		"DOI": "10.1145/3554820",
		"ISSN": "2375-4699",
		"issue": "2",
		"title": "GA-SCS: Graph-Augmented Source Code Summarization",
		"URL": "https://doi.org/10.1145/3554820",
		"volume": "22",
		"author": [
			{
				"family": "Zhang",
				"given": "Mengli"
			},
			{
				"family": "Zhou",
				"given": "Gang"
			},
			{
				"family": "Yu",
				"given": "Wanting"
			},
			{
				"family": "Huang",
				"given": "Ningbo"
			},
			{
				"family": "Liu",
				"given": "Wenfen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					2
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/WGGIX4XY",
		"type": "paper-conference",
		"container-title": "2023 IEEE/ACM 45th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)",
		"DOI": "10.1109/ICSE-Companion58688.2023.00091",
		"page": "328-329",
		"title": "Revisiting Information Retrieval and Deep Learning Approaches for Code Summarization",
		"author": [
			{
				"family": "Zhu",
				"given": "Tingwei"
			},
			{
				"family": "Li",
				"given": "Zhong"
			},
			{
				"family": "Pan",
				"given": "Minxue"
			},
			{
				"family": "Shi",
				"given": "Chaoxuan"
			},
			{
				"family": "Zhang",
				"given": "Tian"
			},
			{
				"family": "Pei",
				"given": "Yu"
			},
			{
				"family": "Li",
				"given": "Xuandong"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/FQXN8DNI",
		"type": "paper-conference",
		"container-title": "2024 International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA)",
		"DOI": "10.1109/HORA61326.2024.10550711",
		"page": "1-6",
		"title": "Source Code Summarization & Comment Generation with NLP : A New Index Proposal",
		"author": [
			{
				"family": "Kilic",
				"given": "M. Alp Eren"
			},
			{
				"family": "Adak",
				"given": "M. Fatih"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/G22GDBV9",
		"type": "paper-conference",
		"container-title": "International Conference on the Quality of Information and Communications Technology",
		"page": "391-398",
		"publisher": "Springer",
		"title": "Black-Box Reconstruction Attacks on LLMs: A Preliminary Study in Code Summarization",
		"author": [
			{
				"family": "Russodivito",
				"given": "Marco"
			},
			{
				"family": "Spina",
				"given": "Angelica"
			},
			{
				"family": "Scalabrino",
				"given": "Simone"
			},
			{
				"family": "Oliveto",
				"given": "Rocco"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/6K5II42C",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2412.17094",
		"journalAbbreviation": "arXiv preprint arXiv:2412.17094",
		"title": "Analysis on LLMs Performance for Code Summarization",
		"author": [
			{
				"family": "Akib",
				"given": "Md Ahnaf"
			},
			{
				"family": "Mazumder",
				"given": "Md Muktadir"
			},
			{
				"family": "Ahsan",
				"given": "Salman"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/3KSUF9VF",
		"type": "paper-conference",
		"container-title": "2024 IEEE International Conference on Software Maintenance and Evolution (ICSME)",
		"DOI": "10.1109/ICSME58944.2024.00051",
		"page": "487-498",
		"title": "PRECOS: Project-specific Retrieval for Better Code Summarization",
		"author": [
			{
				"family": "Zhu",
				"given": "Tingwei"
			},
			{
				"family": "Li",
				"given": "Zhong"
			},
			{
				"family": "Zhang",
				"given": "Tian"
			},
			{
				"family": "Pan",
				"given": "Minxue"
			},
			{
				"family": "Li",
				"given": "Xuandong"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/NS695ZQJ",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2111.08874",
		"title": "Gn-transformer: Fusing sequence and graph representation for improved code summarization",
		"author": [
			{
				"family": "Cheng",
				"given": "Junyan"
			},
			{
				"family": "Fostiropoulos",
				"given": "Iordanis"
			},
			{
				"family": "Boehm",
				"given": "Barry"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/LCKZUJLH",
		"type": "paper-conference",
		"event-title": "2019 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)",
		"ISBN": "1-7281-3012-3",
		"page": "1-6",
		"publisher": "IEEE",
		"title": "A code summarization approach for object oriented programs",
		"author": [
			{
				"family": "Mohsin",
				"given": "Ali Hameed"
			},
			{
				"family": "Hammad",
				"given": "Mustafa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/2A5CQMIP",
		"type": "paper-conference",
		"container-title": "2021 IEEE 21st International Conference on Software Quality, Reliability and Security (QRS)",
		"DOI": "10.1109/QRS54544.2021.00108",
		"page": "990-1001",
		"title": "Evaluating Code Summarization with Improved Correlation with Human Assessment",
		"author": [
			{
				"family": "Shen",
				"given": "Juanjuan"
			},
			{
				"family": "Zhou",
				"given": "Yu"
			},
			{
				"family": "Wang",
				"given": "Yongchao"
			},
			{
				"family": "Chen",
				"given": "Xiang"
			},
			{
				"family": "Han",
				"given": "Tingting"
			},
			{
				"family": "Chen",
				"given": "Taolue"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/954XWET4",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2307.07854",
		"journalAbbreviation": "arXiv preprint arXiv:2307.07854",
		"title": "Multilingual Adapter-based Knowledge Aggregation on Code Summarization for Low-Resource Languages",
		"author": [
			{
				"family": "Saberi",
				"given": "Iman"
			},
			{
				"family": "Fard",
				"given": "Fatemeh"
			},
			{
				"family": "Chen",
				"given": "Fuxiang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/BDAF6JUJ",
		"type": "article-journal",
		"container-title": "Applied Soft Computing",
		"ISSN": "1568-4946",
		"journalAbbreviation": "Applied Soft Computing",
		"page": "112238",
		"title": "FMCF: A fusing multiple code features approach based on Transformer for Solidity smart contracts source code summarization",
		"volume": "166",
		"author": [
			{
				"family": "Lei",
				"given": "Gang"
			},
			{
				"family": "Zhang",
				"given": "Donghua"
			},
			{
				"family": "Xiao",
				"given": "Jianmao"
			},
			{
				"family": "Fan",
				"given": "Guodong"
			},
			{
				"family": "Cao",
				"given": "Yuanlong"
			},
			{
				"family": "Feng",
				"given": "Zhiyong"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/BCYWDMMS",
		"type": "paper-conference",
		"abstract": "Software documentation largely consists of short, natural language summaries of the subroutines in the software. These summaries help programmers quickly understand what a subroutine does without having to read the source code him or herself. The task of writing these descriptions is called \"source code summarization\" and has been a target of research for several years. Recently, AI-based approaches have superseded older, heuristic-based approaches. Yet, to date these AI-based approaches assume that all the content needed to predict summaries is inside subroutine itself. This assumption limits performance because many subroutines cannot be understood without surrounding context. In this paper, we present an approach that models the file context of subroutines (i.e. other subroutines in the same file) and uses an attention mechanism to find words and concepts to use in summaries. We show in an experiment that our approach extends and improves several recent baselines.",
		"collection-title": "MSR '20",
		"container-title": "Proceedings of the 17th International Conference on Mining Software Repositories",
		"DOI": "10.1145/3379597.3387449",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7517-7",
		"note": "event-place: Seoul, Republic of Korea",
		"page": "300–310",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Improved Automatic Summarization of Subroutines via Attention to File Context",
		"URL": "https://doi.org/10.1145/3379597.3387449",
		"author": [
			{
				"family": "Haque",
				"given": "Sakib"
			},
			{
				"family": "LeClair",
				"given": "Alexander"
			},
			{
				"family": "Wu",
				"given": "Lingfei"
			},
			{
				"family": "McMillan",
				"given": "Collin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/ZG2HEA7C",
		"type": "article-journal",
		"container-title": "Software: Practice and Experience",
		"ISSN": "0038-0644",
		"issue": "3",
		"journalAbbreviation": "Software: Practice and Experience",
		"page": "465-482",
		"title": "Context‐based transfer learning for low resource code summarization",
		"volume": "54",
		"author": [
			{
				"family": "Guo",
				"given": "Yi"
			},
			{
				"family": "Chai",
				"given": "Yu"
			},
			{
				"family": "Zhang",
				"given": "Lehuan"
			},
			{
				"family": "Li",
				"given": "Hui"
			},
			{
				"family": "Luo",
				"given": "Mengzhi"
			},
			{
				"family": "Guo",
				"given": "Shikai"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/AJIYTNRC",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:1901.01186",
		"journalAbbreviation": "arXiv preprint arXiv:1901.01186",
		"title": "Supporting software documentation with source code summarization",
		"author": [
			{
				"family": "Al-Msie'deen",
				"given": "Ra'Fat"
			},
			{
				"family": "Blasi",
				"given": "Anas H"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/7YJWCWLU",
		"type": "article-journal",
		"container-title": "Journal of Software: Evolution and Process",
		"ISSN": "2047-7473",
		"issue": "6",
		"journalAbbreviation": "Journal of Software: Evolution and Process",
		"page": "e2620",
		"title": "PassSum: Leveraging paths of abstract syntax trees and self‐supervision for code summarization",
		"volume": "36",
		"author": [
			{
				"family": "Niu",
				"given": "Changan"
			},
			{
				"family": "Li",
				"given": "Chuanyi"
			},
			{
				"family": "Ng",
				"given": "Vincent"
			},
			{
				"family": "Ge",
				"given": "Jidong"
			},
			{
				"family": "Huang",
				"given": "Liguo"
			},
			{
				"family": "Luo",
				"given": "Bin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/EM9SAADB",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2402.04863",
		"journalAbbreviation": "arXiv preprint arXiv:2402.04863",
		"title": "SCLA: Automated Smart Contract Summarization via LLMs and Semantic Augmentation",
		"author": [
			{
				"family": "Mao",
				"given": "Yingjie"
			},
			{
				"family": "Li",
				"given": "Xiaoqi"
			},
			{
				"family": "Li",
				"given": "Wenkai"
			},
			{
				"family": "Wang",
				"given": "Xin"
			},
			{
				"family": "Xie",
				"given": "Lei"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/RVKRG5D6",
		"type": "paper-conference",
		"abstract": "Code summarization aims to automatically generate natural language descriptions for code snippets, which help people maintain and understand code snippets. Existing code summarization methods are mostly based on the encoder-decoder structure, where the encoder learns latent features from a code snippet and the decoder generates the corresponding summary based on the features. Such methods do not leverage project-specific information and tend to generate general summaries. However, in practice developers want the generated summaries to be project-specific, i.e., being consistent with the existing summaries in the same project on aspects such as sentence patterns and domain concepts. In this work, we investigate project-specific code summarization. We propose a two-stage method CSWPS, which can be seamlessly integrated into any existing encoder-decoder summarization model. In the first stage, CSWPS learns project-specific features from existing summaries in each project using multi-task learning. In the second stage, CSWPS samples from the project-specific features conditioned on the input source code and project information, and extracts the features most relevant to the input code. The features guide the decoder to generate a project-specific summary for the input code. By incorporating CSWPS into existing code summarization models, we can always improve their performance and achieve the new state-of-the-art. We also empirically show that the summaries generated by incorporating CSWPS are more project-specific, via feature visualization and human study. A replication package for this work is available at https://github.com/DaSESmartEdu/CSWPS.",
		"container-title": "Machine Learning and Knowledge Discovery in Databases. Applied Data Science Track",
		"event-place": "Cham",
		"ISBN": "978-3-031-70378-2",
		"page": "190–206",
		"publisher": "Springer Nature Switzerland",
		"publisher-place": "Cham",
		"title": "Code Summarization with Project-Specific Features",
		"author": [
			{
				"family": "Wang",
				"given": "Yu"
			},
			{
				"family": "Liu",
				"given": "Xin"
			},
			{
				"family": "Lu",
				"given": "Xuesong"
			},
			{
				"family": "Zhou",
				"given": "Aoying"
			}
		],
		"editor": [
			{
				"family": "Bifet",
				"given": "Albert"
			},
			{
				"family": "Krilavičius",
				"given": "Tomas"
			},
			{
				"family": "Miliou",
				"given": "Ioanna"
			},
			{
				"family": "Nowaczyk",
				"given": "Slawomir"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/F4GD9VQY",
		"type": "paper-conference",
		"container-title": "2021 IEEE 32nd International Symposium on Software Reliability Engineering (ISSRE)",
		"DOI": "10.1109/ISSRE52982.2021.00057",
		"page": "486-497",
		"title": "Improving Code Summarization Through Automated Quality Assurance",
		"author": [
			{
				"family": "Hu",
				"given": "Yuxing"
			},
			{
				"family": "Yan",
				"given": "Meng"
			},
			{
				"family": "Liu",
				"given": "Zhongxin"
			},
			{
				"family": "Chen",
				"given": "Qiuyuan"
			},
			{
				"family": "Wang",
				"given": "Bei"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/6K6TJJ6A",
		"type": "article-journal",
		"container-title": "Electronics",
		"ISSN": "2079-9292",
		"issue": "10",
		"journalAbbreviation": "Electronics",
		"page": "1816",
		"title": "CogCol: Code Graph-Based Contrastive Learning Model for Code Summarization",
		"volume": "13",
		"author": [
			{
				"family": "Shi",
				"given": "Yucen"
			},
			{
				"family": "Yin",
				"given": "Ying"
			},
			{
				"family": "Yu",
				"given": "Mingqian"
			},
			{
				"family": "Chu",
				"given": "Liangyu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/WUMP3SLX",
		"type": "article-journal",
		"container-title": "Automated Software Engineering",
		"ISSN": "0928-8910",
		"issue": "2",
		"journalAbbreviation": "Automated Software Engineering",
		"page": "62",
		"title": "Revisiting file context for source code summarization",
		"volume": "31",
		"author": [
			{
				"family": "Su",
				"given": "Chia-Yi"
			},
			{
				"family": "Bansal",
				"given": "Aakash"
			},
			{
				"family": "McMillan",
				"given": "Collin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/5MRNYLQR",
		"type": "paper-conference",
		"event-title": "2022 IEEE 22nd International Working Conference on Source Code Analysis and Manipulation (SCAM)",
		"ISBN": "1-6654-9609-6",
		"page": "132-142",
		"publisher": "IEEE",
		"title": "Multi-Modal Code Summarization with Retrieved Summary",
		"author": [
			{
				"family": "Lin",
				"given": "Lile"
			},
			{
				"family": "Huang",
				"given": "Zhiqiu"
			},
			{
				"family": "Yu",
				"given": "Yaoshen"
			},
			{
				"family": "Liu",
				"given": "Yapeng"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/KA3P927N",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2405.18573",
		"journalAbbreviation": "arXiv preprint arXiv:2405.18573",
		"title": "Programmer Visual Attention During Context-Aware Code Summarization",
		"author": [
			{
				"family": "Bansal",
				"given": "Aakash"
			},
			{
				"family": "Wallace",
				"given": "Robert"
			},
			{
				"family": "Karas",
				"given": "Zachary"
			},
			{
				"family": "Tang",
				"given": "Ningzhi"
			},
			{
				"family": "Huang",
				"given": "Yu"
			},
			{
				"family": "Li",
				"given": "Toby Jia-Jun"
			},
			{
				"family": "McMillan",
				"given": "Collin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/CVUEGQ3R",
		"type": "paper-conference",
		"event-title": "Proceedings of the 16th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
		"page": "171-182",
		"title": "MMF3: neural code summarization based on multi-modal fine-grained feature fusion",
		"author": [
			{
				"family": "Ma",
				"given": "Zheng"
			},
			{
				"family": "Gao",
				"given": "Yuexiu"
			},
			{
				"family": "Lyu",
				"given": "Lei"
			},
			{
				"family": "Lyu",
				"given": "Chen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/82FQFKDT",
		"type": "article-journal",
		"container-title": "Empirical Software Engineering",
		"ISSN": "1382-3256",
		"issue": "5",
		"journalAbbreviation": "Empirical Software Engineering",
		"page": "126",
		"title": "EnCoSum: enhanced semantic features for multi-scale multi-modal source code summarization",
		"volume": "28",
		"author": [
			{
				"family": "Gao",
				"given": "Yuexiu"
			},
			{
				"family": "Zhang",
				"given": "Hongyu"
			},
			{
				"family": "Lyu",
				"given": "Chen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/HSDTKVIY",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2303.02333",
		"journalAbbreviation": "arXiv preprint arXiv:2303.02333",
		"title": "Demystifying What Code Summarization Models Learned",
		"author": [
			{
				"family": "Wang",
				"given": "Yu"
			},
			{
				"family": "Wang",
				"given": "Ke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/4PKG7TNG",
		"type": "article-journal",
		"container-title": "Information and Software Technology",
		"ISSN": "0950-5849",
		"journalAbbreviation": "Information and Software Technology",
		"page": "106761",
		"title": "Summarizing source code with hierarchical code representation",
		"volume": "143",
		"author": [
			{
				"family": "Zhou",
				"given": "Ziyi"
			},
			{
				"family": "Yu",
				"given": "Huiqun"
			},
			{
				"family": "Fan",
				"given": "Guisheng"
			},
			{
				"family": "Huang",
				"given": "Zijie"
			},
			{
				"family": "Yang",
				"given": "Xingguang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/XCCW84JP",
		"type": "paper-conference",
		"event-title": "2018 Second international conference on inventive communication and computational technologies (ICICCT)",
		"ISBN": "1-5386-1974-1",
		"page": "1011-1016",
		"publisher": "IEEE",
		"title": "Class level code summarization based on dependencies and micro patterns",
		"author": [
			{
				"family": "Malhotra",
				"given": "Mrinaal"
			},
			{
				"family": "Chhabra",
				"given": "Jitender Kumar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/AGMETGRQ",
		"type": "article-journal",
		"container-title": "Neurocomputing",
		"page": "126777",
		"title": "Enhancing code summarization with action word prediction",
		"volume": "563",
		"author": [
			{
				"family": "Li",
				"given": "Mingchen"
			},
			{
				"family": "Yu",
				"given": "Huiqun"
			},
			{
				"family": "Fan",
				"given": "Guisheng"
			},
			{
				"family": "Zhou",
				"given": "Ziyi"
			},
			{
				"family": "Huang",
				"given": "Zijie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/JV6F9UZW",
		"type": "article-journal",
		"container-title": "Neural Computing and Applications",
		"ISSN": "0941-0643",
		"issue": "19",
		"journalAbbreviation": "Neural Computing and Applications",
		"page": "12571-12589",
		"title": "Adversarial training and ensemble learning for automatic code summarization",
		"volume": "33",
		"author": [
			{
				"family": "Zhou",
				"given": "Ziyi"
			},
			{
				"family": "Yu",
				"given": "Huiqun"
			},
			{
				"family": "Fan",
				"given": "Guisheng"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/PYFFIXPA",
		"type": "paper-conference",
		"abstract": "The lack of description of a given program code acts as a big hurdle to those developers new to the code base for its understanding. To tackle this problem, previous work on code summarization, the task of automatically generating code description given a piece of code reported that an auxiliary learning model trained to produce API (Application Programming Interface) embeddings showed promising results when applied to a downstream, code summarization model. However, different codes having different summaries can have the same set of API sequences. If we train a model to generate summaries given an API sequence, the model will not be able to learn effectively. Nevertheless, we note that the API sequence can still be useful and has not been actively utilized. This work proposes a novel multi-task approach that simultaneously trains two similar tasks: 1) summarizing a given code (code to summary), and 2) summarizing a given API sequence (API sequence to summary). We propose a novel code-level encoder based on BERT capable of expressing the semantics of code, and obtain representations for every line of code. Our work is the first code summarization work that utilizes a natural language-based contextual pre-trained language model in its encoder. We evaluate our approach using two common datasets (Java and Python) that have been widely used in previous studies. Our experimental results show that our multi-task approach improves over the baselines and achieves the new state-of-the-art.",
		"container-title": "Findings of the Association for Computational Linguistics: EMNLP 2021",
		"DOI": "10.18653/v1/2021.findings-emnlp.214",
		"event-place": "Punta Cana, Dominican Republic",
		"page": "2510–2520",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Punta Cana, Dominican Republic",
		"title": "Novel Natural Language Summarization of Program Code via Leveraging Multiple Input Representations",
		"URL": "https://aclanthology.org/2021.findings-emnlp.214/",
		"author": [
			{
				"family": "Chen",
				"given": "Fuxiang"
			},
			{
				"family": "Kim",
				"given": "Mijung"
			},
			{
				"family": "Choo",
				"given": "Jaegul"
			}
		],
		"editor": [
			{
				"family": "Moens",
				"given": "Marie-Francine"
			},
			{
				"family": "Huang",
				"given": "Xuanjing"
			},
			{
				"family": "Specia",
				"given": "Lucia"
			},
			{
				"family": "Yih",
				"given": "Scott Wen-tau"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					11
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/GFTI675E",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2401.14727",
		"journalAbbreviation": "arXiv preprint arXiv:2401.14727",
		"title": "SparseCoder: Identifier-Aware Sparse Transformer for File-Level Code Summarization",
		"author": [
			{
				"family": "Wang",
				"given": "Yanlin"
			},
			{
				"family": "Huang",
				"given": "Yanxian"
			},
			{
				"family": "Guo",
				"given": "Daya"
			},
			{
				"family": "Zhang",
				"given": "Hongyu"
			},
			{
				"family": "Zheng",
				"given": "Zibin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/4I6MXJ8U",
		"type": "paper-conference",
		"event-title": "2020 IEEE International Conference on Big Data and Smart Computing (BigComp)",
		"ISBN": "1-7281-6034-0",
		"page": "564-570",
		"publisher": "IEEE",
		"title": "Source code summarization using attention-based keyword memory networks",
		"author": [
			{
				"family": "Choi",
				"given": "YunSeok"
			},
			{
				"family": "Kim",
				"given": "Suah"
			},
			{
				"family": "Lee",
				"given": "Jee-Hyong"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/P98QHLVI",
		"type": "article-journal",
		"container-title": "Entropy",
		"ISSN": "1099-4300",
		"issue": "10",
		"journalAbbreviation": "Entropy",
		"page": "1372",
		"title": "Re_Trans: Combined retrieval and transformer model for source code summarization",
		"volume": "24",
		"author": [
			{
				"family": "Zhang",
				"given": "Chunyan"
			},
			{
				"family": "Zhou",
				"given": "Qinglei"
			},
			{
				"family": "Qiao",
				"given": "Meng"
			},
			{
				"family": "Tang",
				"given": "Ke"
			},
			{
				"family": "Xu",
				"given": "Lianqiu"
			},
			{
				"family": "Liu",
				"given": "Fudong"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/LMH8WWPV",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2003.03238",
		"journalAbbreviation": "arXiv preprint arXiv:2003.03238",
		"title": "TranS^ 3: A transformer-based framework for unifying code summarization and code search",
		"author": [
			{
				"family": "Wang",
				"given": "Wenhua"
			},
			{
				"family": "Zhang",
				"given": "Yuqun"
			},
			{
				"family": "Zeng",
				"given": "Zhengran"
			},
			{
				"family": "Xu",
				"given": "Guandong"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/FIH8WMG2",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2312.16066",
		"journalAbbreviation": "arXiv preprint arXiv:2312.16066",
		"title": "A Prompt Learning Framework for Source Code Summarization",
		"author": [
			{
				"family": "Sun",
				"given": "Weisong"
			},
			{
				"family": "Fang",
				"given": "Chunrong"
			},
			{
				"family": "You",
				"given": "Yudu"
			},
			{
				"family": "Chen",
				"given": "Yuchen"
			},
			{
				"family": "Liu",
				"given": "Yi"
			},
			{
				"family": "Wang",
				"given": "Chong"
			},
			{
				"family": "Zhang",
				"given": "Jian"
			},
			{
				"family": "Zhang",
				"given": "Quanjun"
			},
			{
				"family": "Qian",
				"given": "Hanwei"
			},
			{
				"family": "Zhao",
				"given": "Wei"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/NFHK8WG2",
		"type": "paper-conference",
		"abstract": "Automatic code summarization, also known as code comment generation, has been proven beneficial for developers to understand better and maintain software projects. However, few research works have investigated the robustness of such models. Robustness requires that the model sustains the quality of the output summaries in the presence of perturbations to the inputs. In this paper, we provide an in-depth study of the robustness of code summarization models. We propose CREATE (Code summaRization modEl’s Adversarial aTtackEr), an approach for performing adversarial attacks against the model. This approach can generate adversarial samples to mislead the model and explore its robustness while ensuring these samples are compilable and semantically similar. We attack mainstream code summarization models with a large-scale available Java dataset to evaluate the effectiveness and efficiency of our approach. The experimental results indicate that CREATE’s attack effectiveness and efficiency surpasses other baselines, causing a decrease in the quality of generated comments by at least 40%. Furthermore, we investigate the magnitude of perturbation caused by CREATE during adversarial attacks, and the results show that the similarity between the adversarial samples generated by CREATE and the input code is approximately 0.8, demonstrating that it induces more minor perturbations compared to other baselines. Finally, we utilize CREATE for adversarial training of the model. Through experimentation, this approach indeed effectively enhances the model’s robustness.",
		"collection-title": "EASE '24",
		"container-title": "Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering",
		"DOI": "10.1145/3661167.3661173",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-1701-7",
		"note": "event-place: Salerno, Italy",
		"page": "17–27",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Adversarial Attack and Robustness Improvement on Code Summarization",
		"URL": "https://doi.org/10.1145/3661167.3661173",
		"author": [
			{
				"family": "Ding",
				"given": "Xi"
			},
			{
				"family": "Huang",
				"given": "Yuan"
			},
			{
				"family": "Chen",
				"given": "Xiangping"
			},
			{
				"family": "Bian",
				"given": "Jing"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/PXK7WNRX",
		"type": "article-journal",
		"container-title": "Neural Computing and Applications",
		"ISSN": "0941-0643",
		"issue": "4",
		"journalAbbreviation": "Neural Computing and Applications",
		"page": "3373-3393",
		"title": "ClassSum: a deep learning model for class-level code summarization",
		"volume": "35",
		"author": [
			{
				"family": "Li",
				"given": "Mingchen"
			},
			{
				"family": "Yu",
				"given": "Huiqun"
			},
			{
				"family": "Fan",
				"given": "Guisheng"
			},
			{
				"family": "Zhou",
				"given": "Ziyi"
			},
			{
				"family": "Huang",
				"given": "Jiawen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/75AWD4B4",
		"type": "article-journal",
		"container-title": "Neurocomputing",
		"ISSN": "0925-2312",
		"journalAbbreviation": "Neurocomputing",
		"page": "126385",
		"title": "An data augmentation method for source code summarization",
		"volume": "549",
		"author": [
			{
				"family": "Song",
				"given": "Zixuan"
			},
			{
				"family": "Zeng",
				"given": "Hui"
			},
			{
				"family": "Shang",
				"given": "Xiuwei"
			},
			{
				"family": "Li",
				"given": "Guanxi"
			},
			{
				"family": "Li",
				"given": "Hui"
			},
			{
				"family": "Guo",
				"given": "Shikai"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/QDYLRV33",
		"type": "article-journal",
		"container-title": "IEEE Transactions on Reliability",
		"DOI": "10.1109/TR.2024.3392876",
		"page": "1-10",
		"title": "Interpretable Code Summarization",
		"author": [
			{
				"family": "Kamal",
				"given": "Md Sarwar"
			},
			{
				"family": "Nimmy",
				"given": "Sonia Farhana"
			},
			{
				"family": "Dey",
				"given": "Nilanjan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/2A4CRMC9",
		"type": "paper-conference",
		"abstract": "This paper delves into the intricacies of code summarization using advanced transformer-based language models. Through empirical studies, we evaluate the efficacy of code summarization by altering function and variable names to explore whether models truly understand code semantics or merely rely on textual cues. We have also introduced adversaries like dead code and commented code across three programming languages (Python, Javascript, and Java) to further scrutinize the model`s understanding. Ultimately, our research aims to offer valuable insights into the inner workings of transformer-based LMs, enhancing their ability to understand code and contributing to more efficient software development practices and maintenance workflows.",
		"container-title": "Proceedings of the 1st GenBench Workshop on (Benchmarking) Generalisation in NLP",
		"DOI": "10.18653/v1/2023.genbench-1.5",
		"event-place": "Singapore",
		"page": "65–75",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Singapore",
		"title": "Robust Code Summarization",
		"URL": "https://aclanthology.org/2023.genbench-1.5/",
		"author": [
			{
				"family": "Mondal",
				"given": "Debanjan"
			},
			{
				"family": "Lodha",
				"given": "Abhilasha"
			},
			{
				"family": "Sahoo",
				"given": "Ankita"
			},
			{
				"family": "Kumari",
				"given": "Beena"
			}
		],
		"editor": [
			{
				"family": "Hupkes",
				"given": "Dieuwke"
			},
			{
				"family": "Dankers",
				"given": "Verna"
			},
			{
				"family": "Batsuren",
				"given": "Khuyagbaatar"
			},
			{
				"family": "Sinha",
				"given": "Koustuv"
			},
			{
				"family": "Kazemnejad",
				"given": "Amirhossein"
			},
			{
				"family": "Christodoulopoulos",
				"given": "Christos"
			},
			{
				"family": "Cotterell",
				"given": "Ryan"
			},
			{
				"family": "Bruni",
				"given": "Elia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					12
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/39DBIR3I",
		"type": "article-journal",
		"container-title": "IEEE Transactions on Software Engineering",
		"ISSN": "0098-5589",
		"issue": "4",
		"journalAbbreviation": "IEEE Transactions on Software Engineering",
		"page": "3008-3031",
		"title": "Towards retrieval-based neural code summarization: A meta-learning approach",
		"volume": "49",
		"author": [
			{
				"family": "Zhou",
				"given": "Ziyi"
			},
			{
				"family": "Yu",
				"given": "Huiqun"
			},
			{
				"family": "Fan",
				"given": "Guisheng"
			},
			{
				"family": "Huang",
				"given": "Zijie"
			},
			{
				"family": "Yang",
				"given": "Kang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/VHWVDZYN",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2307.11709",
		"journalAbbreviation": "arXiv preprint arXiv:2307.11709",
		"title": "Statement-based memory for neural source code summarization",
		"author": [
			{
				"family": "Bansal",
				"given": "Aakash"
			},
			{
				"family": "Jiang",
				"given": "Siyuan"
			},
			{
				"family": "Haque",
				"given": "Sakib"
			},
			{
				"family": "McMillan",
				"given": "Collin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/HUSZ4U3A",
		"type": "article-journal",
		"container-title": "Journal of Software: Evolution and Process",
		"ISSN": "2047-7473",
		"issue": "11",
		"journalAbbreviation": "Journal of Software: Evolution and Process",
		"page": "e2706",
		"title": "Semantic similarity loss for neural source code summarization",
		"volume": "36",
		"author": [
			{
				"family": "Su",
				"given": "Chia‐Yi"
			},
			{
				"family": "McMillan",
				"given": "Collin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/3PAFLPV7",
		"type": "article-journal",
		"container-title": "Information and Software Technology",
		"ISSN": "0950-5849",
		"journalAbbreviation": "Information and Software Technology",
		"page": "107527",
		"title": "Cross-Modal Retrieval-enhanced code Summarization based on joint learning for retrieval and generation",
		"volume": "175",
		"author": [
			{
				"family": "Li",
				"given": "Lixuan"
			},
			{
				"family": "Liang",
				"given": "Bin"
			},
			{
				"family": "Chen",
				"given": "Lin"
			},
			{
				"family": "Zhang",
				"given": "Xiaofang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/W5LTZGNL",
		"type": "article-journal",
		"container-title": "Microsoft, Tech. Rep. MSR-TR-2020-16",
		"journalAbbreviation": "Microsoft, Tech. Rep. MSR-TR-2020-16",
		"title": "Cocogum: Contextual code summarization with multi-relational gnn on umls",
		"author": [
			{
				"family": "Wang",
				"given": "Yanlin"
			},
			{
				"family": "Du",
				"given": "Lun"
			},
			{
				"family": "Shi",
				"given": "Ensheng"
			},
			{
				"family": "Hu",
				"given": "Yuxuan"
			},
			{
				"family": "Han",
				"given": "Shi"
			},
			{
				"family": "Zhang",
				"given": "Dongmei"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/TM4MJBG4",
		"type": "paper-conference",
		"event-title": "Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension",
		"page": "47-51",
		"title": "Do Machines and Humans Focus on Similar Code? Exploring Explainability of Large Language Models in Code Summarization",
		"author": [
			{
				"family": "Li",
				"given": "Jiliang"
			},
			{
				"family": "Zhang",
				"given": "Yifan"
			},
			{
				"family": "Karas",
				"given": "Zachary"
			},
			{
				"family": "McMillan",
				"given": "Collin"
			},
			{
				"family": "Leach",
				"given": "Kevin"
			},
			{
				"family": "Huang",
				"given": "Yu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/KLT7A3AB",
		"type": "article-journal",
		"container-title": "Journal of Systems and Software",
		"ISSN": "0164-1212",
		"journalAbbreviation": "Journal of Systems and Software",
		"page": "112329",
		"title": "Understanding comment practices in Scratch: A study of comments in a block-based visual programming language",
		"author": [
			{
				"family": "Akanda",
				"given": "Wahiduzzaman"
			},
			{
				"family": "Clause",
				"given": "James"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2025"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/4RTDZ5HD",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2412.01333",
		"journalAbbreviation": "arXiv preprint arXiv:2412.01333",
		"title": "Can Large Language Models Serve as Evaluators for Code Summarization?",
		"author": [
			{
				"family": "Wu",
				"given": "Yang"
			},
			{
				"family": "Wan",
				"given": "Yao"
			},
			{
				"family": "Chu",
				"given": "Zhaoyang"
			},
			{
				"family": "Zhao",
				"given": "Wenting"
			},
			{
				"family": "Liu",
				"given": "Ye"
			},
			{
				"family": "Zhang",
				"given": "Hongyu"
			},
			{
				"family": "Shi",
				"given": "Xuanhua"
			},
			{
				"family": "Yu",
				"given": "Philip S"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/7NBHII7P",
		"type": "paper-conference",
		"container-title": "2021 IEEE/ACM 29th International Conference on Program Comprehension (ICPC)",
		"DOI": "10.1109/ICPC52881.2021.00022",
		"page": "138-148",
		"title": "Exploiting Method Names to Improve Code Summarization: A Deliberation Multi-Task Learning Approach",
		"author": [
			{
				"family": "Xie",
				"given": "Rui"
			},
			{
				"family": "Ye",
				"given": "Wei"
			},
			{
				"family": "Sun",
				"given": "Jinan"
			},
			{
				"family": "Zhang",
				"given": "Shikun"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/S7GVN6X6",
		"type": "article-journal",
		"container-title": "IEEE Transactions on Software Engineering",
		"DOI": "10.1109/TSE.2023.3256362",
		"issue": "6",
		"page": "3472-3486",
		"title": "CoSS: Leveraging Statement Semantics for Code Summarization",
		"volume": "49",
		"author": [
			{
				"family": "Shi",
				"given": "Chaochen"
			},
			{
				"family": "Cai",
				"given": "Borui"
			},
			{
				"family": "Zhao",
				"given": "Yao"
			},
			{
				"family": "Gao",
				"given": "Longxiang"
			},
			{
				"family": "Sood",
				"given": "Keshav"
			},
			{
				"family": "Xiang",
				"given": "Yong"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/3PC333DW",
		"type": "article-journal",
		"abstract": "(Source) Code summarization aims to automatically generate summaries/comments for given code snippets in the form of natural language. Such summaries play a key role in helping developers understand and maintain source code. Existing code summarization techniques can be categorized into extractive methods and abstractive methods. The extractive methods extract a subset of important statements and keywords from the code snippet using retrieval techniques and generate a summary that preserves factual details in important statements and keywords. However, such a subset may miss identifier or entity naming, and consequently, the naturalness of the generated summary is usually poor. The abstractive methods can generate human-written-like summaries leveraging encoder-decoder models. However, the generated summaries often miss important factual details.To generate human-written-like summaries with preserved factual details, we propose a novel extractive-and-abstractive framework. The extractive module in the framework performs the task of extractive code summarization, which takes in the code snippet and predicts important statements containing key factual details. The abstractive module in the framework performs the task of abstractive code summarization, which takes in the code snippet and important statements in parallel and generates a succinct and human-written-like natural language summary. We evaluate the effectiveness of our technique, called EACS, by conducting extensive experiments on three datasets involving six programming languages. Experimental results show that EACS significantly outperforms state-of-the-art techniques for all three widely used metrics, including BLEU, METEOR, and ROUGH-L. In addition, the human evaluation demonstrates that the summaries generated by EACS have higher naturalness and informativeness and are more relevant to given code snippets.",
		"container-title": "ACM Trans. Softw. Eng. Methodol.",
		"DOI": "10.1145/3632742",
		"ISSN": "1049-331X",
		"issue": "3",
		"title": "An Extractive-and-Abstractive Framework for Source Code Summarization",
		"URL": "https://doi.org/10.1145/3632742",
		"volume": "33",
		"author": [
			{
				"family": "Sun",
				"given": "Weisong"
			},
			{
				"family": "Fang",
				"given": "Chunrong"
			},
			{
				"family": "Chen",
				"given": "Yuchen"
			},
			{
				"family": "Zhang",
				"given": "Quanjun"
			},
			{
				"family": "Tao",
				"given": "Guanhong"
			},
			{
				"family": "You",
				"given": "Yudu"
			},
			{
				"family": "Han",
				"given": "Tingxu"
			},
			{
				"family": "Ge",
				"given": "Yifei"
			},
			{
				"family": "Hu",
				"given": "Yuling"
			},
			{
				"family": "Luo",
				"given": "Bin"
			},
			{
				"family": "Chen",
				"given": "Zhenyu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024",
					3
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/P8MSUCAS",
		"type": "article-journal",
		"container-title": "ACM Transactions on Software Engineering and Methodology",
		"ISSN": "1049-331X",
		"issue": "3",
		"journalAbbreviation": "ACM Transactions on Software Engineering and Methodology",
		"page": "1-37",
		"title": "Deep is better? An empirical comparison of information retrieval and deep learning approaches to code summarization",
		"volume": "33",
		"author": [
			{
				"family": "Zhu",
				"given": "Tingwei"
			},
			{
				"family": "Li",
				"given": "Zhong"
			},
			{
				"family": "Pan",
				"given": "Minxue"
			},
			{
				"family": "Shi",
				"given": "Chaoxuan"
			},
			{
				"family": "Zhang",
				"given": "Tian"
			},
			{
				"family": "Pei",
				"given": "Yu"
			},
			{
				"family": "Li",
				"given": "Xuandong"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/JC5RDDXK",
		"type": "paper-conference",
		"container-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"page": "486-500",
		"title": "Modeling hierarchical syntax structure with triplet position for source code summarization",
		"author": [
			{
				"family": "Guo",
				"given": "Juncai"
			},
			{
				"family": "Liu",
				"given": "Jin"
			},
			{
				"family": "Wan",
				"given": "Yao"
			},
			{
				"family": "Li",
				"given": "Li"
			},
			{
				"family": "Zhou",
				"given": "Pingyi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/YVNZGNQL",
		"type": "paper-conference",
		"event-title": "Proceedings of the 11th ACM Symposium on Eye Tracking Research & Applications",
		"page": "1-9",
		"title": "Using developer eye movements to externalize the mental model used in code summarization tasks",
		"author": [
			{
				"family": "Abid",
				"given": "Nahla J"
			},
			{
				"family": "Maletic",
				"given": "Jonathan I"
			},
			{
				"family": "Sharif",
				"given": "Bonita"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/JSCJJEIC",
		"type": "article-journal",
		"container-title": "Information and Software Technology",
		"ISSN": "0950-5849",
		"journalAbbreviation": "Information and Software Technology",
		"page": "106987",
		"title": "Keyword-guided abstractive code summarization via incorporating structural and contextual information",
		"volume": "150",
		"author": [
			{
				"family": "Cheng",
				"given": "Wuyan"
			},
			{
				"family": "Hu",
				"given": "Po"
			},
			{
				"family": "Wei",
				"given": "Shaozhi"
			},
			{
				"family": "Mo",
				"given": "Ran"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/C8KTWGCK",
		"type": "paper-conference",
		"event-title": "International conference on machine learning",
		"page": "2091-2100",
		"publisher": "PMLR",
		"title": "A convolutional attention network for extreme summarization of source code",
		"author": [
			{
				"family": "Allamanis",
				"given": "Miltiadis"
			},
			{
				"family": "Peng",
				"given": "Hao"
			},
			{
				"family": "Sutton",
				"given": "Charles"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2016"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/44LB9YVC",
		"type": "article-journal",
		"container-title": "Software: Practice and Experience",
		"ISSN": "0038-0644",
		"issue": "12",
		"journalAbbreviation": "Software: Practice and Experience",
		"page": "2313-2336",
		"title": "Effective approaches to combining lexical and syntactical information for code summarization",
		"volume": "50",
		"author": [
			{
				"family": "Zhou",
				"given": "Ziyi"
			},
			{
				"family": "Yu",
				"given": "Huiqun"
			},
			{
				"family": "Fan",
				"given": "Guisheng"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/HJ3NDWN7",
		"type": "article-journal",
		"container-title": "IEEE Access",
		"ISSN": "2169-3536",
		"journalAbbreviation": "IEEE Access",
		"page": "51155-51165",
		"title": "READSUM: Retrieval-Augmented Adaptive Transformer for Source Code Summarization",
		"volume": "11",
		"author": [
			{
				"family": "Choi",
				"given": "Yunseok"
			},
			{
				"family": "Na",
				"given": "Cheolwon"
			},
			{
				"family": "Kim",
				"given": "Hyojun"
			},
			{
				"family": "Lee",
				"given": "Jee-Hyong"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/352FJ78M",
		"type": "paper-conference",
		"abstract": "We propose Corder, a self-supervised contrastive learning framework for source code model. Corder is designed to alleviate the need of labeled data for code retrieval and code summarization tasks. The pre-trained model of Corder can be used in two ways: (1) it can produce vector representation of code which can be applied to code retrieval tasks that do not have labeled data; (2) it can be used in a fine-tuning process for tasks that might still require label data such as code summarization. The key innovation is that we train the source code model by asking it to recognize similar and dissimilar code snippets through acontrastive learning objective. To do so, we use a set of semantic-preserving transformation operators to generate code snippets that are syntactically diverse but semantically equivalent. Through extensive experiments, we have shown that the code models pretrained by Corder substantially outperform the other baselines for code-to-code retrieval, text-to-code retrieval, and code-to-text summarization tasks.",
		"collection-title": "SIGIR '21",
		"container-title": "Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval",
		"DOI": "10.1145/3404835.3462840",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8037-9",
		"note": "event-place: Virtual Event, Canada",
		"page": "511–521",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Self-Supervised Contrastive Learning for Code Retrieval and Summarization via Semantic-Preserving Transformations",
		"URL": "https://doi.org/10.1145/3404835.3462840",
		"author": [
			{
				"family": "Bui",
				"given": "Nghi D. Q."
			},
			{
				"family": "Yu",
				"given": "Yijun"
			},
			{
				"family": "Jiang",
				"given": "Lingxiao"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/XS6XTAEJ",
		"type": "article-journal",
		"container-title": "Proceedings of the ACM on Human-Computer Interaction",
		"ISSN": "2573-0142",
		"issue": "ETRA",
		"journalAbbreviation": "Proceedings of the ACM on Human-Computer Interaction",
		"page": "1-19",
		"title": "Towards modeling human attention from eye movements for neural source code summarization",
		"volume": "7",
		"author": [
			{
				"family": "Bansal",
				"given": "Aakash"
			},
			{
				"family": "Sharif",
				"given": "Bonita"
			},
			{
				"family": "McMillan",
				"given": "Collin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/A6P4KNWR",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:1904.00805",
		"journalAbbreviation": "arXiv preprint arXiv:1904.00805",
		"title": "A convolutional neural network for language-agnostic source code summarization",
		"author": [
			{
				"family": "Moore",
				"given": "Jessica"
			},
			{
				"family": "Gelman",
				"given": "Ben"
			},
			{
				"family": "Slater",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/FAAP6GQF",
		"type": "article-journal",
		"container-title": "IEEE Transactions on Software Engineering",
		"ISSN": "0098-5589",
		"issue": "9",
		"journalAbbreviation": "IEEE Transactions on Software Engineering",
		"page": "4268-4281",
		"title": "Function call graph context encoding for neural source code summarization",
		"volume": "49",
		"author": [
			{
				"family": "Bansal",
				"given": "Aakash"
			},
			{
				"family": "Eberhart",
				"given": "Zachary"
			},
			{
				"family": "Karas",
				"given": "Zachary"
			},
			{
				"family": "Huang",
				"given": "Yu"
			},
			{
				"family": "McMillan",
				"given": "Collin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/QIXSN9QQ",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2309.02326",
		"journalAbbreviation": "arXiv preprint arXiv:2309.02326",
		"title": "Revisiting File Context for Source Code Summarization",
		"author": [
			{
				"family": "Bansal",
				"given": "Aakash"
			},
			{
				"family": "Su",
				"given": "Chia-Yi"
			},
			{
				"family": "McMillan",
				"given": "Collin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/HTETXNGW",
		"type": "paper-conference",
		"event-title": "Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering",
		"page": "100-109",
		"title": "Code summarization without direct access to code-towards exploring federated llms for software engineering",
		"author": [
			{
				"family": "Kumar",
				"given": "Jahnavi"
			},
			{
				"family": "Chimalakonda",
				"given": "Sridhar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/JKRR3DWC",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2312.09601",
		"journalAbbreviation": "arXiv preprint arXiv:2312.09601",
		"title": "Binary code summarization: Benchmarking chatgpt/gpt-4 and other large language models",
		"author": [
			{
				"family": "Jin",
				"given": "Xin"
			},
			{
				"family": "Larson",
				"given": "Jonathan"
			},
			{
				"family": "Yang",
				"given": "Weiwei"
			},
			{
				"family": "Lin",
				"given": "Zhiqiang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/8HBKUNNT",
		"type": "article-journal",
		"container-title": "Applied Sciences",
		"ISSN": "2076-3417",
		"issue": "24",
		"journalAbbreviation": "Applied Sciences",
		"page": "12587",
		"title": "Bi-LSTM-based neural source code summarization",
		"volume": "12",
		"author": [
			{
				"family": "Aljumah",
				"given": "Sarah"
			},
			{
				"family": "Berriche",
				"given": "Lamia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/A227XCEA",
		"type": "article-journal",
		"container-title": "IEEE Access",
		"ISSN": "2169-3536",
		"journalAbbreviation": "IEEE Access",
		"page": "135591-135604",
		"title": "Fret: Functional reinforced transformer with bert for code summarization",
		"volume": "8",
		"author": [
			{
				"family": "Wang",
				"given": "Ruyun"
			},
			{
				"family": "Zhang",
				"given": "Hanwen"
			},
			{
				"family": "Lu",
				"given": "Guoliang"
			},
			{
				"family": "Lyu",
				"given": "Lei"
			},
			{
				"family": "Lyu",
				"given": "Chen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/SPL547HA",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2108.12987",
		"journalAbbreviation": "arXiv preprint arXiv:2108.12987",
		"title": "Cast: Enhancing code summarization with hierarchical splitting and reconstruction of abstract syntax trees",
		"author": [
			{
				"family": "Shi",
				"given": "Ensheng"
			},
			{
				"family": "Wang",
				"given": "Yanlin"
			},
			{
				"family": "Du",
				"given": "Lun"
			},
			{
				"family": "Zhang",
				"given": "Hongyu"
			},
			{
				"family": "Han",
				"given": "Shi"
			},
			{
				"family": "Zhang",
				"given": "Dongmei"
			},
			{
				"family": "Sun",
				"given": "Hongbin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/UPTGG9I9",
		"type": "article-journal",
		"container-title": "Journal of Systems and Software",
		"ISSN": "0164-1212",
		"journalAbbreviation": "Journal of Systems and Software",
		"page": "111746",
		"title": "Learning a holistic and comprehensive code representation for code summarization",
		"volume": "203",
		"author": [
			{
				"family": "Yang",
				"given": "Kaiyuan"
			},
			{
				"family": "Wang",
				"given": "Junfeng"
			},
			{
				"family": "Song",
				"given": "Zihua"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/WSQAUE7G",
		"type": "paper-conference",
		"event-title": "54th Annual Meeting of the Association for Computational Linguistics 2016",
		"page": "2073-2083",
		"publisher": "Association for Computational Linguistics",
		"title": "Summarizing source code using a neural attention model",
		"author": [
			{
				"family": "Iyer",
				"given": "Srinivasan"
			},
			{
				"family": "Konstas",
				"given": "Ioannis"
			},
			{
				"family": "Cheung",
				"given": "Alvin"
			},
			{
				"family": "Zettlemoyer",
				"given": "Luke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2016"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/3KD6UXRT",
		"type": "paper-conference",
		"event-title": "Proceedings of the 29th International Conference on Computational Linguistics",
		"page": "5966-5977",
		"title": "Boosting code summarization by embedding code structures",
		"author": [
			{
				"family": "Son",
				"given": "Jikyoeng"
			},
			{
				"family": "Hahn",
				"given": "Joonghyuk"
			},
			{
				"family": "Seo",
				"given": "HyeonTae"
			},
			{
				"family": "Han",
				"given": "Yo-Sub"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/HBQ3QZ5U",
		"type": "paper-conference",
		"event-title": "2024 IEEE 24th International Conference on Software Quality, Reliability and Security (QRS)",
		"ISBN": "979-8-3503-6563-4",
		"page": "216-227",
		"publisher": "IEEE",
		"title": "On the Effectiveness of Large Language Models in Statement-level Code Summarization",
		"author": [
			{
				"family": "Zhu",
				"given": "Jie"
			},
			{
				"family": "Miao",
				"given": "Yun"
			},
			{
				"family": "Xu",
				"given": "Tingting"
			},
			{
				"family": "Zhu",
				"given": "Junwu"
			},
			{
				"family": "Sun",
				"given": "Xiaolei"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/LXLHLVP6",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2108.11601",
		"journalAbbreviation": "arXiv preprint arXiv:2108.11601",
		"title": "Retrieval augmented code generation and summarization",
		"author": [
			{
				"family": "Parvez",
				"given": "Md Rizwan"
			},
			{
				"family": "Ahmad",
				"given": "Wasi Uddin"
			},
			{
				"family": "Chakraborty",
				"given": "Saikat"
			},
			{
				"family": "Ray",
				"given": "Baishakhi"
			},
			{
				"family": "Chang",
				"given": "Kai-Wei"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/LR948YV8",
		"type": "paper-conference",
		"event-title": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
		"page": "1-13",
		"title": "Automatic semantic augmentation of language model prompts (for code summarization)",
		"author": [
			{
				"family": "Ahmed",
				"given": "Toufique"
			},
			{
				"family": "Pai",
				"given": "Kunal Suresh"
			},
			{
				"family": "Devanbu",
				"given": "Premkumar"
			},
			{
				"family": "Barr",
				"given": "Earl"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/3I6CZQPW",
		"type": "article-journal",
		"title": "Summarizing source code with transferred api knowledge",
		"author": [
			{
				"family": "Hu",
				"given": "Xing"
			},
			{
				"family": "Li",
				"given": "Ge"
			},
			{
				"family": "Xia",
				"given": "Xin"
			},
			{
				"family": "Lo",
				"given": "David"
			},
			{
				"family": "Lu",
				"given": "Shuai"
			},
			{
				"family": "Jin",
				"given": "Zhi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/72SIUC6L",
		"type": "paper-conference",
		"container-title": "2021 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)",
		"DOI": "10.1109/SANER50967.2021.00038",
		"page": "330-341",
		"title": "Action Word Prediction for Neural Source Code Summarization",
		"author": [
			{
				"family": "Haque",
				"given": "Sakib"
			},
			{
				"family": "Bansal",
				"given": "Aakash"
			},
			{
				"family": "Wu",
				"given": "Lingfei"
			},
			{
				"family": "McMillan",
				"given": "Collin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/VRTQF8PR",
		"type": "article-journal",
		"container-title": "IEEE Transactions on Software Engineering",
		"ISSN": "0098-5589",
		"issue": "11",
		"journalAbbreviation": "IEEE Transactions on Software Engineering",
		"page": "1038-1054",
		"title": "An eye-tracking study of java programmers and application to source code summarization",
		"volume": "41",
		"author": [
			{
				"family": "Rodeghero",
				"given": "Paige"
			},
			{
				"family": "Liu",
				"given": "Cheng"
			},
			{
				"family": "McBurney",
				"given": "Paul W"
			},
			{
				"family": "McMillan",
				"given": "Collin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/47FJAT4L",
		"type": "article-journal",
		"abstract": "A code summary is a brief natural language description of source code. Summaries are usually only a single sentence long, and yet form the backbone of developer documentation. A short descriptions such as “changes all visible polygons to the color blue” can give a programmer a high-level idea of what code does without the effort of reading the code itself. Recently, products based on Large Language Models such as ChatGPT have demonstrated a strong ability to write these descriptions automatically. However, to use these tools, programmers must send their code to untrusted third parties for processing (e.g., via an API call). This loss of custody is not acceptable to many organizations. In this paper, we present an alternative: we train an open source model using sample output generated by GPT$$-$$3.5 in a process related to knowledge distillation. Our model is small enough (350 m parameters) to be run on a single 16gb GPU, yet we show in our evaluation that it is large enough to mimic GPT$$-$$3.5 on this task.",
		"container-title": "Automated Software Engineering",
		"DOI": "10.1007/s10515-024-00421-4",
		"ISSN": "1573-7535",
		"issue": "1",
		"journalAbbreviation": "Automated Software Engineering",
		"page": "22",
		"title": "Distilled GPT for source code summarization",
		"URL": "https://doi.org/10.1007/s10515-024-00421-4",
		"volume": "31",
		"author": [
			{
				"family": "Su",
				"given": "Chia-Yi"
			},
			{
				"family": "McMillan",
				"given": "Collin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					1
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/LPACJFYM",
		"type": "article-journal",
		"abstract": "Code summarization is the task of creating short, natural language descriptions of source code. It is an important part of code comprehension and a powerful method of documentation. Previous work has made progress in identifying where programmers focus in code as they write their own summaries (i.e., Writing). However, there is currently a gap in studying programmers’ attention as they read code with pre-written summaries (i.e., Reading). As a result, it is currently unknown how these two forms of code comprehension compare: Reading and Writing. Also, there is a limited understanding of programmer attention with respect to program semantics. We address these shortcomings with a human eye-tracking study (n = 27) comparing Reading and Writing. We examined programmers’ attention with respect to fine-grained program semantics, including their attention sequences (i.e., scan paths). We find distinctions in programmer attention across the comprehension tasks, similarities in reading patterns between them, and differences mediated by demographic factors. This can help guide code comprehension in both computer science education and automated code summarization. Furthermore, we mapped programmers’ gaze data onto the Abstract Syntax Tree to explore another representation of human attention. We find that visual behavior on this structure is not always consistent with that on source code.",
		"container-title": "ACM Trans. Softw. Eng. Methodol.",
		"DOI": "10.1145/3664808",
		"ISSN": "1049-331X",
		"issue": "7",
		"title": "A Tale of Two Comprehensions? Analyzing Student Programmer Attention during Code Summarization",
		"URL": "https://doi.org/10.1145/3664808",
		"volume": "33",
		"author": [
			{
				"family": "Karas",
				"given": "Zachary"
			},
			{
				"family": "Bansal",
				"given": "Aakash"
			},
			{
				"family": "Zhang",
				"given": "Yifan"
			},
			{
				"family": "Li",
				"given": "Toby"
			},
			{
				"family": "McMillan",
				"given": "Collin"
			},
			{
				"family": "Huang",
				"given": "Yu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024",
					8
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/FLYDRG9B",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2107.01933",
		"journalAbbreviation": "arXiv preprint arXiv:2107.01933",
		"title": "Cocosum: Contextual code summarization with multi-relational graph neural network",
		"author": [
			{
				"family": "Wang",
				"given": "Yanlin"
			},
			{
				"family": "Shi",
				"given": "Ensheng"
			},
			{
				"family": "Du",
				"given": "Lun"
			},
			{
				"family": "Yang",
				"given": "Xiaodi"
			},
			{
				"family": "Hu",
				"given": "Yuxuan"
			},
			{
				"family": "Han",
				"given": "Shi"
			},
			{
				"family": "Zhang",
				"given": "Hongyu"
			},
			{
				"family": "Zhang",
				"given": "Dongmei"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/ALIVWKJS",
		"type": "paper-conference",
		"abstract": "Code summarization generates brief natural language description given a source code snippet, while code retrieval fetches relevant source code given a natural language query. Since both tasks aim to model the association between natural language and programming language, recent studies have combined these two tasks to improve their performance. However, researchers have yet been able to effectively leverage the intrinsic connection between the two tasks as they train these tasks in a separate or pipeline manner, which means their performance can not be well balanced. In this paper, we propose a novel end-to-end model for the two tasks by introducing an additional code generation task. More specifically, we explicitly exploit the probabilistic correlation between code summarization and code generation with dual learning, and utilize the two encoders for code summarization and code generation to train the code retrieval task via multi-task learning. We have carried out extensive experiments on an existing dataset of SQL and Python, and results show that our model can significantly improve the results of the code retrieval task over the-state-of-art models, as well as achieve competitive performance in terms of BLEU score for the code summarization task.",
		"collection-title": "WWW '20",
		"container-title": "Proceedings of The Web Conference 2020",
		"DOI": "10.1145/3366423.3380295",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7023-3",
		"note": "event-place: Taipei, Taiwan",
		"page": "2309–2319",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Leveraging Code Generation to Improve Code Retrieval and Summarization via Dual Learning",
		"URL": "https://doi.org/10.1145/3366423.3380295",
		"author": [
			{
				"family": "Ye",
				"given": "Wei"
			},
			{
				"family": "Xie",
				"given": "Rui"
			},
			{
				"family": "Zhang",
				"given": "Jinglei"
			},
			{
				"family": "Hu",
				"given": "Tianxiang"
			},
			{
				"family": "Wang",
				"given": "Xiaoyin"
			},
			{
				"family": "Zhang",
				"given": "Shikun"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/W4G2B7JM",
		"type": "paper-conference",
		"container-title": "2021 IEEE/ACM 29th International Conference on Program Comprehension (ICPC)",
		"DOI": "10.1109/ICPC52881.2021.00026",
		"page": "184-195",
		"title": "Improving Code Summarization with Block-wise Abstract Syntax Tree Splitting",
		"author": [
			{
				"family": "Lin",
				"given": "Chen"
			},
			{
				"family": "Ouyang",
				"given": "Zhichao"
			},
			{
				"family": "Zhuang",
				"given": "Junqing"
			},
			{
				"family": "Chen",
				"given": "Jianqiang"
			},
			{
				"family": "Li",
				"given": "Hui"
			},
			{
				"family": "Wu",
				"given": "Rongxin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/7UPE7S2T",
		"type": "paper-conference",
		"container-title": "2023 IEEE/ACM 31st International Conference on Program Comprehension (ICPC)",
		"DOI": "10.1109/ICPC58990.2023.00027",
		"page": "125-134",
		"title": "Naturalness in Source Code Summarization. How Significant is it?",
		"author": [
			{
				"family": "Ferretti",
				"given": "Claudio"
			},
			{
				"family": "Saletta",
				"given": "Martina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/BUY47WI9",
		"type": "paper-conference",
		"abstract": "Code summarization aims at generating natural language abstraction for source code, and it can be of great help for program comprehension and software maintenance. The current code summarization approaches have made progress with neural-network. However, most of these methods focus on learning the semantic and syntax of source code snippets, ignoring the dependency of codes. In this paper, we propose a novel method based on neural-network model using the knowledge of the call dependency between source code and its related codes. We extract call dependencies from the source code, transform it as a token sequence of method names, and leverage the Seq2Seq model for code summarization using the combination of source code and call dependency information. About 100,000 code data is collected from 1,000 open source Java proejects on github for experiment. The large-scale code experiment shows that by considering not only the code itself but also the codes it called, the code summarization model can be improved with the BLEU score to 33.08.",
		"collection-title": "Internetware '19",
		"container-title": "Proceedings of the 11th Asia-Pacific Symposium on Internetware",
		"DOI": "10.1145/3361242.3362774",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7701-0",
		"note": "event-place: Fukuoka, Japan",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A Neural-Network based Code Summarization Approach by Using Source Code and its Call Dependencies",
		"URL": "https://doi.org/10.1145/3361242.3362774",
		"author": [
			{
				"family": "Liu",
				"given": "Bohong"
			},
			{
				"family": "Wang",
				"given": "Tao"
			},
			{
				"family": "Zhang",
				"given": "Xunhui"
			},
			{
				"family": "Fan",
				"given": "Qiang"
			},
			{
				"family": "Yin",
				"given": "Gang"
			},
			{
				"family": "Deng",
				"given": "Jinsheng"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/JQTJRCU2",
		"type": "paper-conference",
		"container-title": "2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)",
		"DOI": "10.1109/SANER53432.2022.00013",
		"page": "13-24",
		"title": "Source Code Summarization with Structural Relative Position Guided Transformer",
		"author": [
			{
				"family": "Gong",
				"given": "Zi"
			},
			{
				"family": "Gao",
				"given": "Cuiyun"
			},
			{
				"family": "Wang",
				"given": "Yasheng"
			},
			{
				"family": "Gu",
				"given": "Wenchao"
			},
			{
				"family": "Peng",
				"given": "Yun"
			},
			{
				"family": "Xu",
				"given": "Zenglin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/QSB7635B",
		"type": "article-journal",
		"abstract": "Neural code summarization leverages deep learning models to automatically generate brief natural language summaries of code snippets. The development of Transformer models has led to extensive use of attention during model design. While existing work has primarily and almost exclusively focused on static properties of source code and related structural representations like the Abstract Syntax Tree (AST), few studies have considered human attention — that is, where programmers focus while examining and comprehending code. In this paper, we develop a method for incorporating human attention into machine attention to enhance neural code summarization. To facilitate this incorporation and vindicate this hypothesis, we introduce EyeTrans, which consists of three steps: (1) we conduct an extensive eye-tracking human study to collect and pre-analyze data for model training, (2) we devise a data-centric approach to integrate human attention with machine attention in the Transformer architecture, and (3) we conduct comprehensive experiments on two code summarization tasks to demonstrate the effectiveness of incorporating human attention into Transformers. Integrating human attention leads to an improvement of up to 29.91% in Functional Summarization and up to 6.39% in General Code Summarization performance, demonstrating the substantial benefits of this combination. We further explore performance in terms of robustness and efficiency by creating challenging summarization scenarios in which EyeTrans exhibits interesting properties. We also visualize the attention map to depict the simplifying effect of machine attention in the Transformer by incorporating human attention. This work has the potential to propel AI research in software engineering by introducing more human-centered approaches and data.",
		"container-title": "Proc. ACM Softw. Eng.",
		"DOI": "10.1145/3643732",
		"issue": "FSE",
		"title": "EyeTrans: Merging Human and Machine Attention for Neural Code Summarization",
		"URL": "https://doi.org/10.1145/3643732",
		"volume": "1",
		"author": [
			{
				"family": "Zhang",
				"given": "Yifan"
			},
			{
				"family": "Li",
				"given": "Jiliang"
			},
			{
				"family": "Karas",
				"given": "Zachary"
			},
			{
				"family": "Bansal",
				"given": "Aakash"
			},
			{
				"family": "Li",
				"given": "Toby Jia-Jun"
			},
			{
				"family": "McMillan",
				"given": "Collin"
			},
			{
				"family": "Leach",
				"given": "Kevin"
			},
			{
				"family": "Huang",
				"given": "Yu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024",
					7
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/KFKEBC82",
		"type": "paper-conference",
		"abstract": "Code retrieval and summarization are two tasks often employed by software developers to reuse code that spreads over online repositories. In this paper, we present a neural framework that allows bidirectional mapping between source code and natural language to improve these two tasks. Our framework, BVAE, is designed to have two Variational AutoEncoders (VAEs) to model bimodal data: C-VAE for source code and L-VAE for natural language. Both VAEs are trained jointly to reconstruct their input as much as possible with regularization that captures the closeness between the latent variables of code and description. BVAE could learn semantic vector representations for both code and description and generate completely new descriptions for arbitrary code snippets. We design two instance models of BVAE for retrieval and summarization tasks respectively and evaluate their performance on a benchmark which involves two programming languages: C# and SQL. Experiments demonstrate BVAE’s potential on the two tasks.",
		"collection-title": "ASE '18",
		"container-title": "Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering",
		"DOI": "10.1145/3238147.3240471",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-5937-5",
		"note": "event-place: Montpellier, France",
		"page": "826–831",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A neural framework for retrieval and summarization of source code",
		"URL": "https://doi.org/10.1145/3238147.3240471",
		"author": [
			{
				"family": "Chen",
				"given": "Qingying"
			},
			{
				"family": "Zhou",
				"given": "Minghui"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/CSGI9YX6",
		"type": "article-journal",
		"container-title": "ArXiv",
		"title": "Can LLMs Replace Manual Annotation of Software Engineering Artifacts?",
		"URL": "https://api.semanticscholar.org/CorpusID:271855028",
		"volume": "abs/2408.05534",
		"author": [
			{
				"family": "Ahmed",
				"given": "Toufique"
			},
			{
				"family": "Devanbu",
				"given": "Prem"
			},
			{
				"family": "Treude",
				"given": "Christoph"
			},
			{
				"family": "Pradel",
				"given": "Michael"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/BFAF7SZD",
		"type": "paper-conference",
		"container-title": "Annual Meeting of the Association for Computational Linguistics",
		"title": "UniXcoder: Unified Cross-Modal Pre-training for Code Representation",
		"URL": "https://api.semanticscholar.org/CorpusID:247315559",
		"author": [
			{
				"family": "Guo",
				"given": "Daya"
			},
			{
				"family": "Lu",
				"given": "Shuai"
			},
			{
				"family": "Duan",
				"given": "Nan"
			},
			{
				"family": "Wang",
				"given": "Yanlin"
			},
			{
				"family": "Zhou",
				"given": "Ming"
			},
			{
				"family": "Yin",
				"given": "Jian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/ARTPMXAX",
		"type": "paper-conference",
		"container-title": "2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)",
		"DOI": "10.1109/ICSME.2018.00063",
		"page": "539-543",
		"title": "Toward Automatic Summarization of Arbitrary Java Statements for Novice Programmers",
		"author": [
			{
				"family": "Hassan",
				"given": "Mohammed"
			},
			{
				"family": "Hill",
				"given": "Emily"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/DL6JFN2R",
		"type": "paper-conference",
		"abstract": "When deep learning meets big code, a key question is how to efficiently learn a distributed representation for source code that can capture its semantics effectively. We propose to use tree-based convolution over API-enhanced AST. To demonstrate the effectiveness of our approach, we apply it to detect semantic clones—code fragments with similar semantics but dissimilar syntax. Experiment results show that our approach outperforms an existing state-of-the-art approach that uses tree-based LSTM, with an increase of 0.39 and 0.12 in F1-score on OJClone and BigCloneBench respectively. We further propose architectures that incorporate our approach for code search and code summarization.",
		"collection-title": "CF '19",
		"container-title": "Proceedings of the 16th ACM International Conference on Computing Frontiers",
		"DOI": "10.1145/3310273.3321560",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6685-4",
		"note": "event-place: Alghero, Italy",
		"page": "174–182",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Capturing source code semantics via tree-based convolution over API-enhanced AST",
		"URL": "https://doi.org/10.1145/3310273.3321560",
		"author": [
			{
				"family": "Chen",
				"given": "Long"
			},
			{
				"family": "Ye",
				"given": "Wei"
			},
			{
				"family": "Zhang",
				"given": "Shikun"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/FDW5I3Q2",
		"type": "paper-conference",
		"container-title": "Conference on Empirical Methods in Natural Language Processing",
		"title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
		"URL": "https://api.semanticscholar.org/CorpusID:258685677",
		"author": [
			{
				"family": "Wang",
				"given": "Yue"
			},
			{
				"family": "Le",
				"given": "Hung"
			},
			{
				"family": "Gotmare",
				"given": "Akhilesh Deepak"
			},
			{
				"family": "Bui",
				"given": "Nghi D. Q."
			},
			{
				"family": "Li",
				"given": "Junnan"
			},
			{
				"family": "Hoi",
				"given": "Steven C. H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/LXUMSA8M",
		"type": "article-journal",
		"container-title": "IEEE Access",
		"DOI": "10.1109/ACCESS.2019.2931579",
		"page": "111411-111428",
		"title": "A Survey of Automatic Generation of Source Code Comments: Algorithms and Techniques",
		"volume": "7",
		"author": [
			{
				"family": "Song",
				"given": "Xiaotao"
			},
			{
				"family": "Sun",
				"given": "Hailong"
			},
			{
				"family": "Wang",
				"given": "Xu"
			},
			{
				"family": "Yan",
				"given": "Jiafei"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/4K26MWV5",
		"type": "article-journal",
		"container-title": "ArXiv",
		"title": "An Empirical Study on Capability of Large Language Models in Understanding Code Semantics",
		"URL": "https://api.semanticscholar.org/CorpusID:271039689",
		"volume": "abs/2407.03611",
		"author": [
			{
				"family": "Nguyen",
				"given": "Thu-Trang"
			},
			{
				"family": "Vu",
				"given": "Thanh Trong"
			},
			{
				"family": "Vo",
				"given": "Hieu Dinh"
			},
			{
				"family": "Nguyen",
				"given": "Son"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/MDCMUNLG",
		"type": "article-journal",
		"container-title": "ArXiv",
		"title": "LLMs as Evaluators: A Novel Approach to Evaluate Bug Report Summarization",
		"URL": "https://api.semanticscholar.org/CorpusID:272368407",
		"volume": "abs/2409.00630",
		"author": [
			{
				"family": "Kumar",
				"given": "Abhishek"
			},
			{
				"family": "Haiduc",
				"given": "Sonia"
			},
			{
				"family": "Das",
				"given": "Partha Pratim"
			},
			{
				"family": "Chakrabarti",
				"given": "Partha Pratim"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/USVTHR3A",
		"type": "article-journal",
		"container-title": "CoRR",
		"note": "arXiv: 2102.04664",
		"title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation",
		"URL": "https://arxiv.org/abs/2102.04664",
		"volume": "abs/2102.04664",
		"author": [
			{
				"family": "Lu",
				"given": "Shuai"
			},
			{
				"family": "Guo",
				"given": "Daya"
			},
			{
				"family": "Ren",
				"given": "Shuo"
			},
			{
				"family": "Huang",
				"given": "Junjie"
			},
			{
				"family": "Svyatkovskiy",
				"given": "Alexey"
			},
			{
				"family": "Blanco",
				"given": "Ambrosio"
			},
			{
				"family": "Clement",
				"given": "Colin B."
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Jiang",
				"given": "Daxin"
			},
			{
				"family": "Tang",
				"given": "Duyu"
			},
			{
				"family": "Li",
				"given": "Ge"
			},
			{
				"family": "Zhou",
				"given": "Lidong"
			},
			{
				"family": "Shou",
				"given": "Linjun"
			},
			{
				"family": "Zhou",
				"given": "Long"
			},
			{
				"family": "Tufano",
				"given": "Michele"
			},
			{
				"family": "Gong",
				"given": "Ming"
			},
			{
				"family": "Zhou",
				"given": "Ming"
			},
			{
				"family": "Duan",
				"given": "Nan"
			},
			{
				"family": "Sundaresan",
				"given": "Neel"
			},
			{
				"family": "Deng",
				"given": "Shao Kun"
			},
			{
				"family": "Fu",
				"given": "Shengyu"
			},
			{
				"family": "Liu",
				"given": "Shujie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/ACWPXADY",
		"type": "paper-conference",
		"abstract": "Code summarization is the task of automatically generating natural language descriptions from source code. Recently, pre-trained language models have gained significant popularity in code summarization due to their capacity to capture richer semantic representations of both code and natural language. Nonetheless, contemporary code summarization models grapple with two fundamental limitations. (1) Some tokens in the code are irrelevant to the natural language description and damage the alignment of the representation spaces for code and language. (2) Most approaches are based on the encoder-decoder framework, which is often plagued by the exposure bias problem, hampering the effectiveness of their decoding sampling strategies. To address the two challenges, we propose a novel pipeline framework named Reduce Redundancy then Rerank (Reˆ3). Specifically, a redundancy reduction component is introduced to eliminate redundant information in code representation space. Moreover, a re-ranking model is incorporated to select more suitable summary candidates, alleviating the exposure bias problem. The experimental results show the effectiveness of Reˆ3 over some state-of-the-art approaches across six different datasets from the CodeSearchNet benchmark.",
		"container-title": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
		"event-place": "Torino, Italia",
		"page": "13722–13733",
		"publisher": "ELRA and ICCL",
		"publisher-place": "Torino, Italia",
		"title": "Reduce Redundancy Then Rerank: Enhancing Code Summarization with a Novel Pipeline Framework",
		"URL": "https://aclanthology.org/2024.lrec-main.1198/",
		"author": [
			{
				"family": "Hu",
				"given": "Xiaoyu"
			},
			{
				"family": "Zhang",
				"given": "Xu"
			},
			{
				"family": "Lin",
				"given": "Zexu"
			},
			{
				"family": "Zhou",
				"given": "Deyu"
			}
		],
		"editor": [
			{
				"family": "Calzolari",
				"given": "Nicoletta"
			},
			{
				"family": "Kan",
				"given": "Min-Yen"
			},
			{
				"family": "Hoste",
				"given": "Veronique"
			},
			{
				"family": "Lenci",
				"given": "Alessandro"
			},
			{
				"family": "Sakti",
				"given": "Sakriani"
			},
			{
				"family": "Xue",
				"given": "Nianwen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024",
					5
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/5652YH5T",
		"type": "paper-conference",
		"container-title": "2015 9th Malaysian Software Engineering Conference (MySEC)",
		"DOI": "10.1109/MySEC.2015.7475208",
		"page": "129-134",
		"title": "Semantic-based extraction approach for generating source code summary towards program comprehension",
		"author": [
			{
				"family": "Kadar",
				"given": "Rozita"
			},
			{
				"family": "Syed-Mohamad",
				"given": "Sharifah Mashita"
			},
			{
				"family": "Abdul Rashid",
				"given": "Nur'Aini"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/2G43UQXB",
		"type": "article-journal",
		"container-title": "CoRR",
		"note": "arXiv: 2109.00859",
		"title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
		"URL": "https://arxiv.org/abs/2109.00859",
		"volume": "abs/2109.00859",
		"author": [
			{
				"family": "Wang",
				"given": "Yue"
			},
			{
				"family": "Wang",
				"given": "Weishi"
			},
			{
				"family": "Joty",
				"given": "Shafiq R."
			},
			{
				"family": "Hoi",
				"given": "Steven C. H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/EEP4EGJB",
		"type": "article-journal",
		"container-title": "IEEE Transactions on Software Engineering",
		"DOI": "10.1109/TSE.2022.3233901",
		"issue": "4",
		"page": "2839-2855",
		"title": "GraphSearchNet: Enhancing GNNs via Capturing Global Dependencies for Semantic Code Search",
		"volume": "49",
		"author": [
			{
				"family": "Liu",
				"given": "Shangqing"
			},
			{
				"family": "Xie",
				"given": "Xiaofei"
			},
			{
				"family": "Siow",
				"given": "Jingkai"
			},
			{
				"family": "Ma",
				"given": "Lei"
			},
			{
				"family": "Meng",
				"given": "Guozhu"
			},
			{
				"family": "Liu",
				"given": "Yang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/8L6N8JXI",
		"type": "article-journal",
		"container-title": "ArXiv",
		"title": "XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence",
		"URL": "https://api.semanticscholar.org/CorpusID:249848115",
		"volume": "abs/2206.08474",
		"author": [
			{
				"family": "Zhu",
				"given": "Ming"
			},
			{
				"family": "Jain",
				"given": "Aneesh"
			},
			{
				"family": "Suresh",
				"given": "Karthik"
			},
			{
				"family": "Ravindran",
				"given": "Roshan"
			},
			{
				"family": "Tipirneni",
				"given": "Sindhu"
			},
			{
				"family": "Reddy",
				"given": "Chandan K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/DLU3KLPQ",
		"type": "paper-conference",
		"abstract": "Source code documentation is an important artifact for efficient software development. Code documentation could greatly benefit from automation since manual documentation is often labouring, resource and time-intensive. In this paper, we employed Codex for automatic code documentation creation. Codex is a GPT-3 based model pre-trained on both natural and programming languages. We find that Codex outperforms existing techniques even with basic settings like one-shot learning (i.e., providing only one example for training). Codex achieves an overall BLEU score of 20.6 for six different programming languages (11.2% improvement over earlier state-of-the-art techniques). Thus, Codex shows promise and warrants in-depth future studies for automatic code documentation generation to support diverse development tasks.",
		"collection-title": "ASE '22",
		"container-title": "Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering",
		"DOI": "10.1145/3551349.3559548",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9475-8",
		"note": "event-place: Rochester, MI, USA",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Automatic Code Documentation Generation Using GPT-3",
		"URL": "https://doi.org/10.1145/3551349.3559548",
		"author": [
			{
				"family": "Khan",
				"given": "Junaed Younus"
			},
			{
				"family": "Uddin",
				"given": "Gias"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/FZ4CWYDQ",
		"type": "book",
		"abstract": "Understanding and analyzing source code effectively is a critical aspect of modern software development, given the growing complexity and scale of software systems. The novel integration of graph alignment into an open-source LLM, specifically adapted for JavaScript, introduces a significant advancement in capturing the intricate structural and semantic relationships within the code. Through leveraging graph-based representations such as Abstract Syntax Trees (ASTs), the model demonstrates an enhanced ability to comprehend code syntax, identify functional patterns, and detect anomalies, thereby surpassing traditional token-based models. Experimental evaluations across various code understanding tasks, including code summarization, function naming, and bug detection, reveal substantial improvements in accuracy and generalization capabilities, showcasing the efficacy of the graph-aligned approach. Comparative analysis with baseline models further establishes the superiority of the proposed method, indicating its potential to serve as a robust foundation for the development of more sophisticated automated code analysis tools. The implications of this research extend to the broader domain of software engineering, where the enhanced understanding of source code can facilitate more efficient development, debugging, and maintenance processes.",
		"note": "DOI: 10.21203/rs.3.rs-5106829/v1",
		"title": "Enhancing JavaScript Source Code Understanding with Graph-Aligned Large Language Models",
		"author": [
			{
				"family": "Vadoce",
				"given": "Theodor"
			},
			{
				"family": "Pritchard",
				"given": "James"
			},
			{
				"family": "Fairbanks",
				"given": "Callum"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024",
					9,
					18
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/VJKDR6MI",
		"type": "paper-conference",
		"container-title": "Conference on Empirical Methods in Natural Language Processing",
		"title": "Exploring Distributional Shifts in Large Language Models for Code Analysis",
		"URL": "https://api.semanticscholar.org/CorpusID:257557735",
		"author": [
			{
				"family": "Arakelyan",
				"given": "Shushan"
			},
			{
				"family": "Das",
				"given": "Rocktim Jyoti"
			},
			{
				"family": "Mao",
				"given": "Yi"
			},
			{
				"family": "Ren",
				"given": "Xiang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/RTPBAFD6",
		"type": "paper-conference",
		"abstract": "Pre-trained code representation models such as CodeBERT have demonstrated superior performance in a variety of software engineering tasks, yet they are often heavy in complexity, quadratically with the length of the input sequence. Our empirical analysis of CodeBERT's attention reveals that CodeBERT pays more attention to certain types of tokens and statements such as keywords and data-relevant statements. Based on these findings, we propose DietCode, which aims at lightweight leverage of large pre-trained models for source code. DietCode simplifies the input program of CodeBERT with three strategies, namely, word dropout, frequency filtering, and an attention-based strategy that selects statements and tokens that receive the most attention weights during pre-training. Hence, it gives a substantial reduction in the computational cost without hampering the model performance. Experimental results on two downstream tasks show that DietCode provides comparable results to CodeBERT with 40% less computational cost in fine-tuning and testing.",
		"collection-title": "ESEC/FSE 2022",
		"container-title": "Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
		"DOI": "10.1145/3540250.3549094",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9413-0",
		"note": "event-place: Singapore, Singapore",
		"page": "1073–1084",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Diet code is healthy: simplifying programs for pre-trained models of code",
		"URL": "https://doi.org/10.1145/3540250.3549094",
		"author": [
			{
				"family": "Zhang",
				"given": "Zhaowei"
			},
			{
				"family": "Zhang",
				"given": "Hongyu"
			},
			{
				"family": "Shen",
				"given": "Beijun"
			},
			{
				"family": "Gu",
				"given": "Xiaodong"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/JCA49W9B",
		"type": "paper-conference",
		"container-title": "2016 3rd International Conference on Advanced Computing and Communication Systems (ICACCS)",
		"DOI": "10.1109/ICACCS.2016.7586385",
		"page": "1-5",
		"title": "Entity based source code summarization (EBSCS)",
		"volume": "01",
		"author": [
			{
				"family": "K",
				"given": "Chitti",
				"dropping-particle": "babu"
			},
			{
				"family": "C",
				"given": "Kavitha"
			},
			{
				"family": "N",
				"given": "SankarRam"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2016"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/F93Q6HCI",
		"type": "paper-conference",
		"abstract": "Recently, many pre-trained language models for source code have been proposed to model the context of code and serve as a basis for downstream code intelligence tasks such as code completion, code search, and code summarization. These models leverage masked pre-training and Transformer and have achieved promising results. However, currently there is still little progress regarding interpretability of existing pre-trained code models. It is not clear why these models work and what feature correlations they can capture. In this paper, we conduct a thorough structural analysis aiming to provide an interpretation of pre-trained language models for source code (e.g., CodeBERT, and GraphCodeBERT) from three distinctive perspectives: (1) attention analysis, (2) probing on the word embedding, and (3) syntax tree induction. Through comprehensive analysis, this paper reveals several insightful findings that may inspire future studies: (1) Attention aligns strongly with the syntax structure of code. (2) Pre-training language models of code can preserve the syntax structure of code in the intermediate representations of each Transformer layer. (3) The pre-trained models of code have the ability of inducing syntax trees of code. Theses findings suggest that it may be helpful to incorporate the syntax structure of code into the process of pre-training for better code representations.",
		"collection-title": "ICSE '22",
		"container-title": "Proceedings of the 44th International Conference on Software Engineering",
		"DOI": "10.1145/3510003.3510050",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9221-1",
		"note": "event-place: Pittsburgh, Pennsylvania",
		"page": "2377–2388",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What do they capture? a structural analysis of pre-trained language models for source code",
		"URL": "https://doi.org/10.1145/3510003.3510050",
		"author": [
			{
				"family": "Wan",
				"given": "Yao"
			},
			{
				"family": "Zhao",
				"given": "Wei"
			},
			{
				"family": "Zhang",
				"given": "Hongyu"
			},
			{
				"family": "Sui",
				"given": "Yulei"
			},
			{
				"family": "Xu",
				"given": "Guandong"
			},
			{
				"family": "Jin",
				"given": "Hai"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/R6NCFEUP",
		"type": "article-journal",
		"container-title": "IEEE Transactions on Reliability",
		"DOI": "10.1109/TR.2022.3154773",
		"issue": "1",
		"page": "258-273",
		"title": "SeTransformer: A Transformer-Based Code Semantic Parser for Code Comment Generation",
		"volume": "72",
		"author": [
			{
				"family": "Li",
				"given": "Zheng"
			},
			{
				"family": "Wu",
				"given": "Yonghao"
			},
			{
				"family": "Peng",
				"given": "Bin"
			},
			{
				"family": "Chen",
				"given": "Xiang"
			},
			{
				"family": "Sun",
				"given": "Zeyu"
			},
			{
				"family": "Liu",
				"given": "Yong"
			},
			{
				"family": "Paul",
				"given": "Doyle"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/98QM9H38",
		"type": "paper-conference",
		"DOI": "10.18293/SEKE2023-192",
		"page": "304-309",
		"title": "GraphPLBART: Code Summarization Based on Graph Embedding and Pre-Trained Model",
		"author": [
			{
				"family": "Li",
				"given": "Jie"
			},
			{
				"family": "Li",
				"given": "Lixuan"
			},
			{
				"family": "Zhu",
				"given": "Hao"
			},
			{
				"family": "Zhang",
				"given": "Xiaofang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					7
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/M2JIX8EW",
		"type": "paper-conference",
		"container-title": "2023 4th International Conference on Intelligent Design（ICID）",
		"DOI": "10.1109/ICID60307.2023.10396770",
		"page": "315-318",
		"title": "Improving Code Summarization Performance with Model Fusion",
		"author": [
			{
				"family": "Guo",
				"given": "Yuqing"
			},
			{
				"family": "Su",
				"given": "Hang"
			},
			{
				"family": "Gao",
				"given": "Hongyu"
			},
			{
				"family": "Sun",
				"given": "Qingcheng"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/DULRCI38",
		"type": "paper-conference",
		"container-title": "2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)",
		"DOI": "10.1109/ICSE43902.2021.00041",
		"page": "336-347",
		"title": "Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks",
		"author": [
			{
				"family": "Mastropaolo",
				"given": "Antonio"
			},
			{
				"family": "Scalabrino",
				"given": "Simone"
			},
			{
				"family": "Cooper",
				"given": "Nathan"
			},
			{
				"family": "Nader Palacio",
				"given": "David"
			},
			{
				"family": "Poshyvanyk",
				"given": "Denys"
			},
			{
				"family": "Oliveto",
				"given": "Rocco"
			},
			{
				"family": "Bavota",
				"given": "Gabriele"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/3H7FBDJV",
		"type": "article-journal",
		"container-title": "CoRR",
		"note": "arXiv: 2103.01025",
		"title": "Neural Code Summarization",
		"URL": "https://arxiv.org/abs/2103.01025",
		"volume": "abs/2103.01025",
		"author": [
			{
				"family": "Shrivastava",
				"given": "Piyush"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/RT9ELNLV",
		"type": "paper-conference",
		"container-title": "2024 3rd International Conference on Electronics and Information Technology (EIT)",
		"DOI": "10.1109/EIT63098.2024.10762491",
		"page": "713-720",
		"title": "Fusion of Contrastive Learning and Heterogeneous Graph for Code Summarization",
		"author": [
			{
				"family": "Zhong",
				"given": "Caiting"
			},
			{
				"family": "Wei",
				"given": "Le"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/MGRSLZ7U",
		"type": "article-journal",
		"container-title": "arXiv e-prints",
		"DOI": "10.48550/arXiv.2005.00653",
		"page": "arXiv:2005.00653",
		"title": "A Transformer-based Approach for Source Code Summarization",
		"author": [
			{
				"family": "Uddin Ahmad",
				"given": "Wasi"
			},
			{
				"family": "Chakraborty",
				"given": "Saikat"
			},
			{
				"family": "Ray",
				"given": "Baishakhi"
			},
			{
				"family": "Chang",
				"given": "Kai-Wei"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020",
					5
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/D3AXXTLB",
		"type": "article-journal",
		"container-title": "CoRR",
		"note": "arXiv: 2010.03150",
		"title": "PyMT5: multi-mode translation of natural language and Python code with transformers",
		"URL": "https://arxiv.org/abs/2010.03150",
		"volume": "abs/2010.03150",
		"author": [
			{
				"family": "Clement",
				"given": "Colin B."
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Timcheck",
				"given": "Jonathan"
			},
			{
				"family": "Svyatkovskiy",
				"given": "Alexey"
			},
			{
				"family": "Sundaresan",
				"given": "Neel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/3B96QD5I",
		"type": "article-journal",
		"abstract": "With the rise of deep learning methods, neural network architecture adopted from neural machine translation has been widely studied in code summarization by learning the sequential content of code. Given the inherent nature of programming languages, learning the representation of source code from the parsed structural information is also a typical way for constructing code summarization models. Recent studies show that the overall performance of the neural models for code summarization can be improved by utilizing sequential and structural information in a hybrid manner. However, both of these two kinds of information fed to the neural models for code summarization fail to embrace the semantics of source code snippets in an explicit way. Is it really a good way to just leave the semantics as hidden things in the source code and have the neural models capture whatever they can get? To observe the utilization of program semantics in automatic code summarization, we conducted an experimental study by analyzing the acceptability of the extreme code summaries generated from neural models. To make the models aligned in the same context for this experimental study and to focus on the observation of the semantics, we re-implement the neural models from three selected studies as extreme code summarization solutions. After an intuitive observation and exploration of the generated summaries with the models trained from a Java dataset, we identify five acceptability aspects: (1) function name format; (2) function naming style; (3) semantic level similarity; (4) the differences in hitting rate of representative words; and (5) the correlation between extreme code summaries with function body. Based on the false negative and false positive phenomena in the results, ablation experiments have shown that the use of program semantics has a positive effect on generating high-quality abstracts in neural models. Our work proves the potential of utilizing the program semantics explicitly in code summarization, and the possible directions are also indicated.",
		"archive": "Publicly Available Content Database",
		"archive_location": "2893799717",
		"container-title": "International Journal of Advanced Computer Science and Applications",
		"DOI": "10.14569/IJACSA.2023.0141077",
		"ISSN": "2158107X",
		"issue": "10",
		"language": "English",
		"title": "Exploring the Utilization of Program Semantics in Extreme Code Summarization: An Experimental Study Based on Acceptability Evaluation",
		"URL": "http://cyber.usask.ca/login?url=https://www.proquest.com/scholarly-journals/exploring-utilization-program-semantics-extreme/docview/2893799717/se-2?accountid=14739",
		"volume": "14",
		"author": [
			{
				"family": "Li",
				"given": "Jiuli"
			},
			{
				"family": "Liu",
				"given": "Yan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/Q56E2BWG",
		"type": "article-journal",
		"container-title": "CoRR",
		"note": "arXiv: 2009.08366",
		"title": "GraphCodeBERT: Pre-training Code Representations with Data Flow",
		"URL": "https://arxiv.org/abs/2009.08366",
		"volume": "abs/2009.08366",
		"author": [
			{
				"family": "Guo",
				"given": "Daya"
			},
			{
				"family": "Ren",
				"given": "Shuo"
			},
			{
				"family": "Lu",
				"given": "Shuai"
			},
			{
				"family": "Feng",
				"given": "Zhangyin"
			},
			{
				"family": "Tang",
				"given": "Duyu"
			},
			{
				"family": "Liu",
				"given": "Shujie"
			},
			{
				"family": "Zhou",
				"given": "Long"
			},
			{
				"family": "Duan",
				"given": "Nan"
			},
			{
				"family": "Svyatkovskiy",
				"given": "Alexey"
			},
			{
				"family": "Fu",
				"given": "Shengyu"
			},
			{
				"family": "Tufano",
				"given": "Michele"
			},
			{
				"family": "Deng",
				"given": "Shao Kun"
			},
			{
				"family": "Clement",
				"given": "Colin B."
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Sundaresan",
				"given": "Neel"
			},
			{
				"family": "Yin",
				"given": "Jian"
			},
			{
				"family": "Jiang",
				"given": "Daxin"
			},
			{
				"family": "Zhou",
				"given": "Ming"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/ERG4XJDD",
		"type": "article-journal",
		"container-title": "ArXiv",
		"title": "Unified Pre-training for Program Understanding and Generation",
		"URL": "https://api.semanticscholar.org/CorpusID:232185260",
		"volume": "abs/2103.06333",
		"author": [
			{
				"family": "Ahmad",
				"given": "Wasi Uddin"
			},
			{
				"family": "Chakraborty",
				"given": "Saikat"
			},
			{
				"family": "Ray",
				"given": "Baishakhi"
			},
			{
				"family": "Chang",
				"given": "Kai-Wei"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/VCGWE7L3",
		"type": "paper-conference",
		"container-title": "2023 Congress in Computer Science, Computer Engineering, & Applied Computing (CSCE)",
		"DOI": "10.1109/CSCE60160.2023.00416",
		"page": "2600-2607",
		"title": "VADER-SC: A Model Agnostic Tool for Large Scale, AI Driven Automated Source Code Summarization",
		"author": [
			{
				"family": "Horne",
				"given": "Dwight"
			},
			{
				"family": "Pierson",
				"given": "Anthony"
			},
			{
				"family": "Hedary",
				"given": "Elvis"
			},
			{
				"family": "Freddo",
				"given": "Garrett"
			},
			{
				"family": "Trejo",
				"given": "Luis"
			},
			{
				"family": "Matis",
				"given": "Mark"
			},
			{
				"family": "Mask",
				"given": "Lonnie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/V4D6H65F",
		"type": "article-journal",
		"container-title": "ArXiv",
		"title": "Probing Pretrained Models of Source Codes",
		"URL": "https://api.semanticscholar.org/CorpusID:246996634",
		"volume": "abs/2202.08975",
		"author": [
			{
				"family": "Troshin",
				"given": "Sergey"
			},
			{
				"family": "Chirkova",
				"given": "Nadezhda"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/HN5L3ATN",
		"type": "article-journal",
		"abstract": "&lt;p&gt;        We propose a framework to automatically generate descriptive comments for source code blocks. While this problem has been studied by many researchers previously, their methods are mostly based on fixed template and achieves poor results. Our framework does not rely on any template, but makes use of a new recursive neural network called CodeRNN to extract features from the source code and embed them into one vector. When this vector representation is input to a new recurrent neural network (Code-GRU), the overall framework generates text descriptions of the code with accuracy (Rouge-2 value) significantly higher than other learning-based approaches such as sequence-to-sequence model. The Code-RNN model can also be used in other scenario where the representation of code is required.      &lt;/p&gt;",
		"container-title": "Proceedings of the AAAI Conference on Artificial Intelligence",
		"DOI": "10.1609/aaai.v32i1.11963",
		"issue": "1",
		"journalAbbreviation": "AAAI",
		"title": "Automatic Generation of Text Descriptive Comments for Code Blocks",
		"URL": "https://ojs.aaai.org/index.php/AAAI/article/view/11963",
		"volume": "32",
		"author": [
			{
				"family": "Liang",
				"given": "Yuding"
			},
			{
				"family": "Zhu",
				"given": "Kenny"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2025",
					1,
					7
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					4,
					27
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/2BN3LKYR",
		"type": "article-journal",
		"abstract": "Pre-trained programming language (PL) models (such as CodeT5, CodeBERT, GraphCodeBERT, etc.,) have the potential to automate software engineering tasks involving code understanding and code generation. However, these models operate in the natural channel of code, i.e., primarily concerned with the human understanding of code. They are not robust to changes in the input and thus, are potentially susceptible to adversarial attacks in the natural channel. We propose, Code Attack, a simple yet effective black-box attack model that uses code structure to generate effective, efficient, and imperceptible adversarial code samples and demonstrates the vulnerabilities of the state-of-the-art PL models to code-specific adversarial attacks. We evaluate the transferability of CodeAttack on several code-code (translation and repair) and code-NL (summarization) tasks across different programming languages. Code Attack outperforms state-of-the-art adversarial NLP attack models to achieve the best overall drop in performance while being more efficient, imperceptible, consistent, and fluent. The code can be found at https://github.com/reddy-lab-code-research/CodeAttack.",
		"container-title": "Proceedings of the AAAI Conference on Artificial Intelligence",
		"DOI": "10.1609/aaai.v37i12.26739",
		"issue": "12",
		"journalAbbreviation": "AAAI",
		"page": "14892-14900",
		"title": "CodeAttack: Code-Based Adversarial Attacks for Pre-trained Programming Language Models",
		"URL": "https://ojs.aaai.org/index.php/AAAI/article/view/26739",
		"volume": "37",
		"author": [
			{
				"family": "Jha",
				"given": "Akshita"
			},
			{
				"family": "Reddy",
				"given": "Chandan K."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2025",
					1,
					7
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					26
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/GF6SGW5F",
		"type": "article-journal",
		"container-title": "Computer Engineering & Science",
		"issue": "04",
		"page": "667-675",
		"title": "<span>A code summarization generation model&nbsp;</span><span>fusing multi-structure data</span>",
		"URL": "http:",
		"volume": "46",
		"author": [
			{
				"family": "[YU Tian-ci",
				"given": "GAO Shang]"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/AYJZKBRR",
		"type": "article-journal",
		"container-title": "ArXiv",
		"title": "COMCAT: Leveraging Human Judgment to Improve Automatic Documentation and Summarization",
		"URL": "https://api.semanticscholar.org/CorpusID:271269925",
		"volume": "abs/2407.13648",
		"author": [
			{
				"family": "Grandel",
				"given": "Skyler"
			},
			{
				"family": "Andersen",
				"given": "Scott Thomas"
			},
			{
				"family": "Huang",
				"given": "Yu"
			},
			{
				"family": "Leach",
				"given": "Kevin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/867LFJ5M",
		"type": "paper-conference",
		"abstract": "Automatically obtaining descriptions of the functions of code snippets in natural language, i.e., code summarization, is an important issue in software engineering. In recent years, abstract syntax tree (AST)-based code summarization models for modelling syntactic structures have continued to emerge. In addition to syntactics, the semantics of the code are gradually gaining attention. In this paper, we propose a code summarization approach that incorporates local-ADG and AST, called GTsum. In particular, we introduce a novel local-ADG-based approach that can effectively filter out irrelevant semantics in the modelling of the semantic structure of code. GTsum learns semantics and syntactics in the local-ADG and AST through graph convolutional networks (GCNs) and then fuses them using Transformer. We evaluate our model on two Java language datasets with several metrics. The results demonstrate that our model achieves state-of-the-art performance compared to the existing models.",
		"container-title": "Neural Information Processing",
		"event-place": "Cham",
		"ISBN": "978-3-030-92307-5",
		"page": "290–297",
		"publisher": "Springer International Publishing",
		"publisher-place": "Cham",
		"title": "Multi-modal Code Summarization Fusing Local API Dependency Graph and AST",
		"author": [
			{
				"family": "Gao",
				"given": "Xuejian"
			},
			{
				"family": "Jiang",
				"given": "Xue"
			},
			{
				"family": "Wu",
				"given": "Qiong"
			},
			{
				"family": "Wang",
				"given": "Xiao"
			},
			{
				"family": "Lyu",
				"given": "Chen"
			},
			{
				"family": "Lyu",
				"given": "Lei"
			}
		],
		"editor": [
			{
				"family": "Mantoro",
				"given": "Teddy"
			},
			{
				"family": "Lee",
				"given": "Minho"
			},
			{
				"family": "Ayu",
				"given": "Media Anugerah"
			},
			{
				"family": "Wong",
				"given": "Kok Wai"
			},
			{
				"family": "Hidayanto",
				"given": "Achmad Nizar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/JU72WC5P",
		"type": "paper-conference",
		"abstract": "Recent years have seen the successful application of large pre-trained models to code representation learning, resulting in substantial improvements on many code-related downstream tasks. But there are issues surrounding their application to SE tasks. First, the majority of the pre-trained models focus on pre-training only the encoder of the Transformer. For generation tasks that are addressed using models with the encoder-decoder architecture, however, there is no reason why the decoder should be left out during pre-training. Second, many existing pre-trained models, including state-of-the-art models such as T5-learning, simply reuse the pretraining tasks designed for natural languages. Moreover, to learn the natural language description of source code needed eventually for code-related tasks such as code summarization, existing pretraining tasks require a bilingual corpus composed of source code and the associated natural language description, which severely limits the amount of data for pre-training. To this end, we propose SPT-Code, a sequence-to-sequence pre-trained model for source code. In order to pre-train SPT-Code in a sequence-to-sequence manner and address the aforementioned weaknesses associated with existing pre-training tasks, we introduce three pre-training tasks that are specifically designed to enable SPT-Code to learn knowledge of source code, the corresponding code structure, as well as a natural language description of the code without relying on any bilingual corpus, and eventually exploit these three sources of information when it is applied to downstream tasks. Experimental results demonstrate that SPT-Code achieves state-of-the-art performance on five code-related downstream tasks after fine-tuning.",
		"collection-title": "ICSE '22",
		"container-title": "Proceedings of the 44th International Conference on Software Engineering",
		"DOI": "10.1145/3510003.3510096",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9221-1",
		"note": "event-place: Pittsburgh, Pennsylvania",
		"page": "2006–2018",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "SPT-code: sequence-to-sequence pre-training for learning source code representations",
		"URL": "https://doi.org/10.1145/3510003.3510096",
		"author": [
			{
				"family": "Niu",
				"given": "Changan"
			},
			{
				"family": "Li",
				"given": "Chuanyi"
			},
			{
				"family": "Ng",
				"given": "Vincent"
			},
			{
				"family": "Ge",
				"given": "Jidong"
			},
			{
				"family": "Huang",
				"given": "Liguo"
			},
			{
				"family": "Luo",
				"given": "Bin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/2SWH33IQ",
		"type": "paper-conference",
		"abstract": "Code summarization aims to condense source code into concise and efficient summaries, crucial for enhancing code comprehension and maintainability. Deep learning and transfer learning techniques have significantly advanced state-of-the-art code summarization. However, constructing high-quality training datasets for these models remains challenging due to the high cost of manual annotation. Active learning methods offer a promising solution by intelligently selecting informative samples for annotation, reducing annotation costs while improving model performance. This paper explores the integration of active learning techniques into code summarization tasks, emphasizing the need to balance sample diversity and uncertainty in sample selection. Inspired by recent research, a novel active learning framework is proposed, leveraging both uncertainty and diversity metrics to construct effective training datasets. This framework provides a foundation for future research in project-specific code summarization.",
		"container-title": "Knowledge Science, Engineering and Management",
		"event-place": "Singapore",
		"ISBN": "978-981-97-5489-2",
		"page": "48–57",
		"publisher": "Springer Nature Singapore",
		"publisher-place": "Singapore",
		"title": "Active Learning for Low-Resource Project-Specific Code Summarization",
		"author": [
			{
				"family": "Xing",
				"given": "Chengli"
			},
			{
				"family": "Hu",
				"given": "Tianxiang"
			},
			{
				"family": "Liao",
				"given": "Ninglin"
			},
			{
				"family": "Zhang",
				"given": "Minghui"
			},
			{
				"family": "Du",
				"given": "Dongdong"
			},
			{
				"family": "Wu",
				"given": "Yupeng"
			},
			{
				"family": "Gao",
				"given": "Qing"
			}
		],
		"editor": [
			{
				"family": "Cao",
				"given": "Cungeng"
			},
			{
				"family": "Chen",
				"given": "Huajun"
			},
			{
				"family": "Zhao",
				"given": "Liang"
			},
			{
				"family": "Arshad",
				"given": "Junaid"
			},
			{
				"family": "Asyhari",
				"given": "Taufiq"
			},
			{
				"family": "Wang",
				"given": "Yonghao"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/QZ5PHG4L",
		"type": "paper-conference",
		"abstract": "Various automated code summarization techniques enhance the efficiency and accuracy of code annotations, enabling succinct natural language comments for code snippets. Recently, large language models (LLMs) have significantly improved natural language processing tasks. Among them, ChatGPT, based on the GPT-3.5 architecture, has gained widespread attention in academia and industry. Previous studies have also tested the ability of ChatGPT in code summarization, designing heuristic questions to explore an appropriate prompt that can guide ChatGPT to generate comments. In contrast, we have designed a more targeted and adaptive suggestion word strategy to study the impact of prompt design on model generation summary. Additionally, we have made extensive data fine-tuning to enhance ChatGPT's ability in code summarization tasks. The experimental results demonstrate that our prompt strategy has significantly improved the quality of code summaries generated by ChatGPT compared to previous studies, but still falls short of the SOTA model.",
		"container-title": "International Conference on Algorithms, High Performance Computing, and Artificial Intelligence (AHPCAI 2024)",
		"DOI": "10.1117/12.3051717",
		"page": "134032I",
		"publisher": "SPIE",
		"title": "Exploring ChatGPT's code summarization capabilities: an empirical study",
		"URL": "https://doi.org/10.1117/12.3051717",
		"volume": "13403",
		"author": [
			{
				"family": "Wang",
				"given": "Kang"
			},
			{
				"family": "Shen",
				"given": "Guohua"
			},
			{
				"family": "Huang",
				"given": "Zhiqiu"
			},
			{
				"family": "Zhang",
				"given": "Xinbo"
			}
		],
		"editor": [
			{
				"family": "Hu",
				"given": "Liang"
			},
			{
				"family": "Loskot",
				"given": "Pavel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/IZ96K45S",
		"type": "paper-conference",
		"container-title": "2024 International Conference on Intelligent and Innovative Technologies in Computing, Electrical and Electronics (IITCEE)",
		"DOI": "10.1109/IITCEE59897.2024.10467731",
		"page": "1-5",
		"title": "Enhancing Dynamic Abstract Generation for Code Summarization using CODE-OAN Models and DBWTFF Frames",
		"author": [
			{
				"family": "Bhatt",
				"given": "Chandradeep"
			},
			{
				"family": "Sharma",
				"given": "Manish"
			},
			{
				"family": "Sharma",
				"given": "Sanjay"
			},
			{
				"family": "Gupta",
				"given": "Shefali"
			},
			{
				"family": "Singh",
				"given": "Teekam"
			},
			{
				"family": "Kumar",
				"given": "Mukesh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/47L4VB5S",
		"type": "paper-conference",
		"container-title": "Advances in Neural Information Processing Systems",
		"page": "9343–9354",
		"publisher": "Curran Associates, Inc.",
		"title": "Integrating Tree Path in Transformer for Code Representation",
		"URL": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4e0223a87610176ef0d24ef6d2dcde3a-Paper.pdf",
		"volume": "34",
		"author": [
			{
				"family": "Peng",
				"given": "Han"
			},
			{
				"family": "Li",
				"given": "Ge"
			},
			{
				"family": "Wang",
				"given": "Wenhan"
			},
			{
				"family": "Zhao",
				"given": "YunFei"
			},
			{
				"family": "Jin",
				"given": "Zhi"
			}
		],
		"editor": [
			{
				"family": "Ranzato",
				"given": "M."
			},
			{
				"family": "Beygelzimer",
				"given": "A."
			},
			{
				"family": "Dauphin",
				"given": "Y."
			},
			{
				"family": "Liang",
				"given": "P. S."
			},
			{
				"family": "Vaughan",
				"given": "J. Wortman"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/NFJ4U2HB",
		"type": "article-journal",
		"container-title": "ArXiv",
		"title": "Context-aware Code Summary Generation",
		"URL": "https://api.semanticscholar.org/CorpusID:271903472",
		"volume": "abs/2408.09006",
		"author": [
			{
				"family": "Su",
				"given": "Chia-Yi"
			},
			{
				"family": "Bansal",
				"given": "Aakash"
			},
			{
				"family": "Huang",
				"given": "Yu"
			},
			{
				"family": "Li",
				"given": "Toby Jia-Jun"
			},
			{
				"family": "McMillan",
				"given": "Collin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/RNFK47WP",
		"type": "paper-conference",
		"abstract": "Large Language Models (LLMs) have become widely used across diverse NLP tasks and domains, demonstrating their adaptability and effectiveness. In the realm of Electronic Design Automation (EDA), LLMs show promise for tasks like Register-Transfer Level (RTL) code generation and summarization. However, despite the proliferation of LLMs for general code-related tasks, there's a dearth of research focused on evaluating and refining these models for hardware description languages (HDLs), notably VHDL. In this study, we evaluate the performance of existing code LLMs for VHDL code generation and summarization using various metrics and two datasets - VHDL-Eval and VHDL-Xform. The latter, an in-house dataset, aims to gauge LLMs' understanding of functionally equivalent code. Our findings reveal consistent underperformance of these models across different metrics, underscoring a significant gap in their suitability for this domain. To address this challenge, we propose Chain-of-Descriptions (CoDes), a novel approach to enhance the performance of LLMs for VHDL code generation and summarization tasks. CoDes involves generating a series of intermediate descriptive steps based on: (i) the problem statement for code generation, and (ii) the VHDL code for summarization. These steps are then integrated with the original input prompt (problem statement or code) and provided as input to the LLMs to generate the final output. Our experiments demonstrate that the CoDes approach significantly surpasses the standard prompting strategy across various metrics on both datasets. This method not only improves the quality of VHDL code generation and summarization but also serves as a framework for future research aimed at enhancing code LLMs for VHDL.",
		"collection-title": "MLCAD '24",
		"container-title": "Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD",
		"DOI": "10.1145/3670474.3685966",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0699-8",
		"note": "event-place: Salt Lake City, UT, USA",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization",
		"URL": "https://doi.org/10.1145/3670474.3685966",
		"author": [
			{
				"family": "Vijayaraghavan",
				"given": "Prashanth"
			},
			{
				"family": "Nitsure",
				"given": "Apoorva"
			},
			{
				"family": "Mackin",
				"given": "Charles"
			},
			{
				"family": "Shi",
				"given": "Luyao"
			},
			{
				"family": "Ambrogio",
				"given": "Stefano"
			},
			{
				"family": "Haran",
				"given": "Arvind"
			},
			{
				"family": "Paruthi",
				"given": "Viresh"
			},
			{
				"family": "Elzein",
				"given": "Ali"
			},
			{
				"family": "Coops",
				"given": "Dan"
			},
			{
				"family": "Beymer",
				"given": "David"
			},
			{
				"family": "Baldwin",
				"given": "Tyler"
			},
			{
				"family": "Degan",
				"given": "Ehsan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/8GFC3Y5A",
		"type": "paper-conference",
		"abstract": "Program understanding is necessary for most software engineering tasks. Internal and external documentation help during this process. Unfortunately, this documentation is often missing or outdated. An alternative to solve this situation is automatically summarizing software artifacts. In the case of source code, a few approaches have been proposed to generate natural language descriptions of fine-grained elements of the code. This research focuses on the automatic generation of generic natural language summaries of complex code artifacts, such as, classes and change sets. In addition, these generic summaries will be adapted to support specific maintenance tasks.",
		"collection-title": "ICSE Companion 2014",
		"container-title": "Companion Proceedings of the 36th International Conference on Software Engineering",
		"DOI": "10.1145/2591062.2591096",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-2768-8",
		"note": "event-place: Hyderabad, India",
		"page": "654–657",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Summarization of complex software artifacts",
		"URL": "https://doi.org/10.1145/2591062.2591096",
		"author": [
			{
				"family": "Moreno",
				"given": "Laura"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2014"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/J4VGBQTZ",
		"type": "paper-conference",
		"container-title": "2022 IEEE 22nd International Conference on Software Quality, Reliability and Security (QRS)",
		"DOI": "10.1109/QRS57517.2022.00099",
		"page": "948-957",
		"title": "RetCom: Information Retrieval-Enhanced Automatic Source-Code Summarization",
		"author": [
			{
				"family": "Zhang",
				"given": "Yubo"
			},
			{
				"family": "Liu",
				"given": "Yanfang"
			},
			{
				"family": "Fan",
				"given": "Xinxin"
			},
			{
				"family": "Lu",
				"given": "Yunfeng"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/A7LVM3XT",
		"type": "paper-conference",
		"container-title": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21",
		"DOI": "10.24963/ijcai.2021/717",
		"page": "5020–5023",
		"publisher": "International Joint Conferences on Artificial Intelligence Organization",
		"title": "Graph-Augmented Code Summarization in Computational Notebooks",
		"URL": "https://doi.org/10.24963/ijcai.2021/717",
		"author": [
			{
				"family": "Wang",
				"given": "April"
			},
			{
				"family": "Wang",
				"given": "Dakuo"
			},
			{
				"family": "Liu",
				"given": "Xuye"
			},
			{
				"family": "Wu",
				"given": "Lingfei"
			}
		],
		"editor": [
			{
				"family": "Zhou",
				"given": "Zhi-Hua"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					8
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/ZKZSRPMH",
		"type": "article-journal",
		"container-title": "CoRR",
		"note": "arXiv: 2007.04973",
		"title": "Contrastive Code Representation Learning",
		"URL": "https://arxiv.org/abs/2007.04973",
		"volume": "abs/2007.04973",
		"author": [
			{
				"family": "Jain",
				"given": "Paras"
			},
			{
				"family": "Jain",
				"given": "Ajay"
			},
			{
				"family": "Zhang",
				"given": "Tianjun"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Gonzalez",
				"given": "Joseph E."
			},
			{
				"family": "Stoica",
				"given": "Ion"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/Q4JALPEU",
		"type": "paper-conference",
		"abstract": "Code comments are one of the pillars of software reuse, as they assist program comprehension activities and reduce maintenance costs. Code comments are prevalent with missing, outdated, and mismatched issues, whereas code summarization could help by generating concise comments. With the development of deep learning, the emergence of pre-trained models has brought notable progress in code summarization. However, these Transformer-based models prefer literal representations of code and may struggle to utilize structural features, which may affect the models. Structured features are common to codes; real code is always in a project. Therefore, in this work, we evaluate the models by the observations of the models' cross-project generalization performance, including metrics, tendency observations, and iterative observations. The results show that current models perform surprisingly poorly, with BLEU scores over 50% lower, while ROUGE and METEOR scores are over 30% lower on cross-project test sets. Current models may rely more on project-specific than generic code features to understand code. We link this to the model's performance on different code channels. It is suggested that the model performance from different code channels be evaluated and the implicit code channels further explored.",
		"container-title": "Reuse and Software Quality",
		"event-place": "Cham",
		"ISBN": "978-3-031-66459-5",
		"page": "3–17",
		"publisher": "Springer Nature Switzerland",
		"publisher-place": "Cham",
		"title": "The Unexpected Blocking of Code Understanding in AI-Based Code Summarization: Observations and Concerns from a Study on Cross-Project Learning Performance",
		"author": [
			{
				"family": "Zhang",
				"given": "Sixuan"
			},
			{
				"family": "Liu",
				"given": "Yan"
			}
		],
		"editor": [
			{
				"family": "Achilleos",
				"given": "Achilleas"
			},
			{
				"family": "Fuentes",
				"given": "Lidia"
			},
			{
				"family": "Papadopoulos",
				"given": "George Angelos"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/3MAYFM4H",
		"type": "article-journal",
		"container-title": "IEEE Transactions on Software Engineering",
		"DOI": "10.1109/TSE.2022.3183297",
		"issue": "4",
		"page": "1580-1598",
		"title": "Using Transfer Learning for Code-Related Tasks",
		"volume": "49",
		"author": [
			{
				"family": "Mastropaolo",
				"given": "Antonio"
			},
			{
				"family": "Cooper",
				"given": "Nathan"
			},
			{
				"family": "Palacio",
				"given": "David Nader"
			},
			{
				"family": "Scalabrino",
				"given": "Simone"
			},
			{
				"family": "Poshyvanyk",
				"given": "Denys"
			},
			{
				"family": "Oliveto",
				"given": "Rocco"
			},
			{
				"family": "Bavota",
				"given": "Gabriele"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/HF77EIL2",
		"type": "paper-conference",
		"container-title": "2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
		"DOI": "10.1109/ASE51524.2021.9678927",
		"page": "1332-1336",
		"title": "What do pre-trained code models know about code?",
		"author": [
			{
				"family": "Karmakar",
				"given": "Anjan"
			},
			{
				"family": "Robbes",
				"given": "Romain"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/C78QMFM3",
		"type": "article-journal",
		"container-title": "CoRR",
		"note": "arXiv: 2103.11318",
		"title": "Language-Agnostic Representation Learning of Source Code from Structure and Context",
		"URL": "https://arxiv.org/abs/2103.11318",
		"volume": "abs/2103.11318",
		"author": [
			{
				"family": "Zügner",
				"given": "Daniel"
			},
			{
				"family": "Kirschstein",
				"given": "Tobias"
			},
			{
				"family": "Catasta",
				"given": "Michele"
			},
			{
				"family": "Leskovec",
				"given": "Jure"
			},
			{
				"family": "Günnemann",
				"given": "Stephan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/CVX55FFK",
		"type": "article-journal",
		"abstract": "Code embedding, as an emerging paradigm for source code analysis, has attracted much attention over the past few years. It aims to represent code semantics through distributed vector representations, which can be used to support a variety of program analysis tasks (e.g., code summarization and semantic labeling). However, existing code embedding approaches are intraprocedural, alias-unaware and ignoring the asymmetric transitivity of directed graphs abstracted from source code, thus they are still ineffective in preserving the structural information of code. This paper presents Flow2Vec, a new code embedding approach that precisely preserves interprocedural program dependence (a.k.a value-flows). By approximating the high-order proximity, i.e., the asymmetric transitivity of value-flows, Flow2Vec embeds control-flows and alias-aware data-flows of a program in a low-dimensional vector space. Our value-flow embedding is formulated as matrix multiplication to preserve context-sensitive transitivity through CFL reachability by filtering out infeasible value-flow paths. We have evaluated Flow2Vec using 32 popular open-source projects. Results from our experiments show that Flow2Vec successfully boosts the performance of two recent code embedding approaches codevec and codeseq for two client applications, i.e., code classification and code summarization. For code classification, Flow2Vec improves codevec with an average increase of 21.2%, 20.1% and 20.7% in precision, recall and F1, respectively. For code summarization, Flow2Vec outperforms codeseq by an average of 13.2%, 18.8% and 16.0% in precision, recall and F1, respectively.",
		"container-title": "Proc. ACM Program. Lang.",
		"DOI": "10.1145/3428301",
		"issue": "OOPSLA",
		"title": "Flow2Vec: value-flow-based precise code embedding",
		"URL": "https://doi.org/10.1145/3428301",
		"volume": "4",
		"author": [
			{
				"family": "Sui",
				"given": "Yulei"
			},
			{
				"family": "Cheng",
				"given": "Xiao"
			},
			{
				"family": "Zhang",
				"given": "Guanqin"
			},
			{
				"family": "Wang",
				"given": "Haoyu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020",
					11
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/KV572PDA",
		"type": "paper-conference",
		"abstract": "Source code summarization is the process of generating a concise and generalized natural language summary from a given source code, which can facilitate software developers to comprehend and use the code better. Currently, most research on source code summarization generation focuses on either converting the source code into abstract syntax tree (AST) sequences or directly converting it into code segments and then feeding these representations into deep learning models. However, these single representation approaches ignore the semantic features of source code and destroy the structure of the abstract syntax tree, which affects the quality of the generated source code summarization. In this paper, we propose a novel source code summarization approach that fuses multiple code features into self-consistency output (FCSO). Our approach is based on a graph neural network encoder and a CodeBERT encoder with a self-attention mechanism. It extracts the sentence feature attention vector and the AST feature attention vector of the source code for feature fusion. Then, it inputs them into the Transformer decoder. Furthermore, to generate more accurate source code summaries, we adopt a new decoding strategy called self-consistency. It samples different inference paths, uses a penalty mechanism to calculate their similarity scores, and ultimately selects the most consistent answer. Our experimental results demonstrate that our proposed approach outperforms standard baseline approaches. On the Python dataset, the BLEU score, METEOR score, and ROUGE_L score increase by 11.13}}\\backslash%}}%, 9.12}}\\backslash%}}%, and 7.88}}\\backslash%}}%, respectively. These results show that our approach provides a promising direction for future research on source code summarization.",
		"container-title": "Algorithms and Architectures for Parallel Processing",
		"event-place": "Singapore",
		"ISBN": "978-981-97-0801-7",
		"page": "112–129",
		"publisher": "Springer Nature Singapore",
		"publisher-place": "Singapore",
		"title": "FCSO: Source Code Summarization by Fusing Multiple Code Features and Ensuring Self-consistency Output",
		"author": [
			{
				"family": "Zhang",
				"given": "Donghua"
			},
			{
				"family": "Lei",
				"given": "Gang"
			},
			{
				"family": "Xiao",
				"given": "Jianmao"
			},
			{
				"family": "Xu",
				"given": "Zhipeng"
			},
			{
				"family": "Fan",
				"given": "Guodong"
			},
			{
				"family": "Chen",
				"given": "Shizhan"
			},
			{
				"family": "Cao",
				"given": "Yuanlong"
			}
		],
		"editor": [
			{
				"family": "Tari",
				"given": "Zahir"
			},
			{
				"family": "Li",
				"given": "Keqiu"
			},
			{
				"family": "Wu",
				"given": "Hongyi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/MET3UUR3",
		"type": "article-journal",
		"abstract": "Source code documentation often contains summaries of source code written by authors. Recently, automatic source code summarization tools have emerged that generate summaries without requiring author intervention. These summaries are designed for readers to be able to understand the high-level concepts of the source code. Unfortunately, there is no agreed upon understanding of what makes up a “good summary.” This paper presents an empirical study examining summaries of source code written by authors, readers, and automatic source code summarization tools. This empirical study examines the textual similarity between source code and summaries of source code using Short Text Semantic Similarity metrics. We found that readers use source code in their summaries more than authors do. Additionally, this study finds that accuracy of a human written summary can be estimated by the textual similarity of that summary to the source code.",
		"container-title": "Empirical Software Engineering",
		"DOI": "10.1007/s10664-014-9344-6",
		"ISSN": "1573-7616",
		"issue": "1",
		"journalAbbreviation": "Empirical Software Engineering",
		"page": "17-42",
		"title": "An empirical study of the textual similarity between source code and source code summaries",
		"URL": "https://doi.org/10.1007/s10664-014-9344-6",
		"volume": "21",
		"author": [
			{
				"family": "McBurney",
				"given": "Paul W."
			},
			{
				"family": "McMillan",
				"given": "Collin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2016",
					2,
					1
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/4TAJ45G9",
		"type": "paper-conference",
		"container-title": "2023 IEEE International Conference on Data and Software Engineering (ICoDSE)",
		"DOI": "10.1109/ICoDSE59534.2023.10292037",
		"page": "7-12",
		"title": "Achieving High-Level Software Component Summarization via Hierarchical Chain-of-Thought Prompting and Static Code Analysis",
		"author": [
			{
				"family": "Rukmono",
				"given": "Satrio Adi"
			},
			{
				"family": "Ochoa",
				"given": "Lina"
			},
			{
				"family": "Chaudron",
				"given": "Michel R.V."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/KGZTRKIZ",
		"type": "paper-conference",
		"container-title": "2024 4th International Conference on Consumer Electronics and Computer Engineering (ICCECE)",
		"DOI": "10.1109/ICCECE61317.2024.10504240",
		"page": "389-393",
		"title": "MMCS: A Code Summarization Approach Based on Multi-Modal Feature Enhancement",
		"author": [
			{
				"family": "Zhou",
				"given": "Zhaolei"
			},
			{
				"family": "Liu",
				"given": "Lei"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/WSIBAV4I",
		"type": "paper-conference",
		"container-title": "2013 IEEE International Conference on Software Maintenance",
		"DOI": "10.1109/ICSM.2013.65",
		"page": "432-435",
		"title": "Task-Driven Software Summarization",
		"author": [
			{
				"family": "Binkley",
				"given": "Dave"
			},
			{
				"family": "Lawrie",
				"given": "Dawn"
			},
			{
				"family": "Hill",
				"given": "Emily"
			},
			{
				"family": "Burge",
				"given": "Janet"
			},
			{
				"family": "Harris",
				"given": "Ian"
			},
			{
				"family": "Hebig",
				"given": "Regina"
			},
			{
				"family": "Keszocze",
				"given": "Oliver"
			},
			{
				"family": "Reed",
				"given": "Karl"
			},
			{
				"family": "Slankas",
				"given": "John"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2013"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/DP9JEJZ7",
		"type": "paper-conference",
		"abstract": "Pre-trained models have been shown effective in many code intelligence tasks. These models are pre-trained on large-scale unlabeled corpus and then fine-tuned in downstream tasks. However, as the inputs to pre-training and downstream tasks are in different forms, it is hard to fully explore the knowledge of pre-trained models. Besides, the performance of fine-tuning strongly relies on the amount of downstream data, while in practice, the scenarios with scarce data are common. Recent studies in the natural language processing (NLP) field show that prompt tuning, a new paradigm for tuning, alleviates the above issues and achieves promising results in various NLP tasks. In prompt tuning, the prompts inserted during tuning provide task-specific knowledge, which is especially beneficial for tasks with relatively scarce data. In this paper, we empirically evaluate the usage and effect of prompt tuning in code intelligence tasks. We conduct prompt tuning on popular pre-trained models CodeBERT and CodeT5 and experiment with three code intelligence tasks including defect prediction, code summarization, and code translation. Our experimental results show that prompt tuning consistently outperforms fine-tuning in all three tasks. In addition, prompt tuning shows great potential in low-resource scenarios, e.g., improving the BLEU scores of fine-tuning by more than 26% on average for code summarization. Our results suggest that instead of fine-tuning, we could adapt prompt tuning for code intelligence tasks to achieve better performance, especially when lacking task-specific data.",
		"collection-title": "ESEC/FSE 2022",
		"container-title": "Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
		"DOI": "10.1145/3540250.3549113",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9413-0",
		"note": "event-place: Singapore, Singapore",
		"page": "382–394",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence",
		"URL": "https://doi.org/10.1145/3540250.3549113",
		"author": [
			{
				"family": "Wang",
				"given": "Chaozheng"
			},
			{
				"family": "Yang",
				"given": "Yuanhang"
			},
			{
				"family": "Gao",
				"given": "Cuiyun"
			},
			{
				"family": "Peng",
				"given": "Yun"
			},
			{
				"family": "Zhang",
				"given": "Hongyu"
			},
			{
				"family": "Lyu",
				"given": "Michael R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/6HM9PTET",
		"type": "paper-conference",
		"abstract": "Source code summaries are a vital tool for the understanding and maintenance of source code as they can be used to explain code in simple terms. However, source code with missing, incorrect, or outdated summaries is a common occurrence in production code. Automatic source code summarisation seeks to solve these issues by generating up-to-date summaries of source code methods. Recent work in automatically generating source code summaries uses neural networks for generating summaries; commonly Sequence-to-Sequence or Transformer models, pretrained on method-summary pairs. The most common method of evaluating the quality of these summaries is comparing the machine-generated summaries against human-written summaries. Summaries can be evaluated using n-gram-based translation metrics such as BLEU, METEOR, or ROUGE-L. However, these metrics alone can be unreliable and new Natural Language Generation metrics based on large pretrained language models provide an alternative. In this paper, we propose a method of improving the evaluation of a model by improving the preprocessing of the data used to train it, as well as proposing evaluating the model with a metric based off a language model, pretrained on a Natural Language (English) alongside traditional metrics. Our evaluation suggests our model has been improved by cleaning and preprocessing the data used in model training. The addition of a pretrained language model metric alongside traditional metrics shows that both produce results which can be used to evaluate neural source code summarisation.",
		"container-title": "Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)",
		"DOI": "10.18653/v1/2022.gem-1.28",
		"event-place": "Abu Dhabi, United Arab Emirates (Hybrid)",
		"page": "326–335",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Abu Dhabi, United Arab Emirates (Hybrid)",
		"title": "Improved Evaluation of Automatic Source Code Summarisation",
		"URL": "https://aclanthology.org/2022.gem-1.28/",
		"author": [
			{
				"family": "Phillips",
				"given": "Jesse"
			},
			{
				"family": "Bowes",
				"given": "David"
			},
			{
				"family": "El-Haj",
				"given": "Mahmoud"
			},
			{
				"family": "Hall",
				"given": "Tracy"
			}
		],
		"editor": [
			{
				"family": "Bosselut",
				"given": "Antoine"
			},
			{
				"family": "Chandu",
				"given": "Khyathi"
			},
			{
				"family": "Dhole",
				"given": "Kaustubh"
			},
			{
				"family": "Gangal",
				"given": "Varun"
			},
			{
				"family": "Gehrmann",
				"given": "Sebastian"
			},
			{
				"family": "Jernite",
				"given": "Yacine"
			},
			{
				"family": "Novikova",
				"given": "Jekaterina"
			},
			{
				"family": "Perez-Beltrachini",
				"given": "Laura"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022",
					12
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/ADPZXRW6",
		"type": "article-journal",
		"container-title": "ArXiv",
		"title": "Large Language Models for Code Summarization",
		"URL": "https://api.semanticscholar.org/CorpusID:270095161",
		"volume": "abs/2405.19032",
		"author": [
			{
				"family": "Szalontai",
				"given": "Balázs"
			},
			{
				"family": "Szalay",
				"given": "GergHo"
			},
			{
				"family": "M'arton",
				"given": "Tam'as"
			},
			{
				"family": "Sike",
				"given": "Anna"
			},
			{
				"family": "Pint'er",
				"given": "Bal'azs"
			},
			{
				"family": "Gregorics",
				"given": "Tibor"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/58IXWLWY",
		"type": "article-journal",
		"container-title": "ArXiv",
		"title": "code2seq: Generating Sequences from Structured Representations of Code",
		"URL": "https://api.semanticscholar.org/CorpusID:51926976",
		"volume": "abs/1808.01400",
		"author": [
			{
				"family": "Alon",
				"given": "Uri"
			},
			{
				"family": "Brody",
				"given": "Shaked"
			},
			{
				"family": "Levy",
				"given": "Omer"
			},
			{
				"family": "Yahav",
				"given": "Eran"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/KP3HNJGJ",
		"type": "paper-conference",
		"container-title": "2024 International Joint Conference on Neural Networks (IJCNN)",
		"DOI": "10.1109/IJCNN60899.2024.10650905",
		"page": "1-8",
		"title": "Leveraging In-and-Cross Project Pseudo-Summaries for Project-Specific Code Summarization",
		"author": [
			{
				"family": "Wu",
				"given": "Yupeng"
			},
			{
				"family": "Hu",
				"given": "Tianxiang"
			},
			{
				"family": "Liao",
				"given": "Ninglin"
			},
			{
				"family": "Xie",
				"given": "Rui"
			},
			{
				"family": "Zhang",
				"given": "Minghui"
			},
			{
				"family": "Du",
				"given": "Dongdong"
			},
			{
				"family": "Lin",
				"given": "Shujun"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/S7EGAJDW",
		"type": "article-journal",
		"container-title": "IEEE Access",
		"DOI": "10.1109/ACCESS.2024.3354390",
		"page": "9871-9889",
		"title": "Integrating Non-Fourier and AST-Structural Relative Position Representations Into Transformer-Based Model for Source Code Summarization",
		"volume": "12",
		"author": [
			{
				"family": "Liang",
				"given": "Hsiang-Mei"
			},
			{
				"family": "Huang",
				"given": "Chin-Yu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/EIEMFXAM",
		"type": "paper-conference",
		"container-title": "2024 International Joint Conference on Neural Networks (IJCNN)",
		"DOI": "10.1109/IJCNN60899.2024.10650776",
		"page": "1-8",
		"title": "iiPCS: Intent-Based In-Context Learning for Project-Specific Code Summarization",
		"author": [
			{
				"family": "Wang",
				"given": "Yu"
			},
			{
				"family": "Liu",
				"given": "Xin"
			},
			{
				"family": "Lu",
				"given": "Xuesong"
			},
			{
				"family": "Zhou",
				"given": "Aoying"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/A2EPNI7G",
		"type": "paper-conference",
		"container-title": "2015 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)",
		"DOI": "10.1109/ESEM.2015.7321188",
		"page": "1-10",
		"title": "An Empirical Study on the Patterns of Eye Movement during Summarization Tasks",
		"author": [
			{
				"family": "Rodeghero",
				"given": "Paige"
			},
			{
				"family": "McMillan",
				"given": "Collin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/PWMTAPU9",
		"type": "article-journal",
		"abstract": "Source code summarization focuses on generating qualified natural language descriptions of a code snippet (e.g., functionality, usage and version). In an actual development environment, descriptions of the code are missing or not consistent with the code due to human factors, which makes it difficult for developers to comprehend and conduct subsequent maintenance. Some existing methods generate summaries from the sequence information of code without considering the structural information. Recently, researchers have adopted the Graph Neural Networks (GNNs) to capture the structural information with modified Abstract Syntax Trees (ASTs) to comprehensively represent a source code, but the alignment method of the two information encoder is hard to decide. In this paper, we propose a source code summarization model named SSCS, a unified transformer-based encoder–decoder architecture, for capturing structural and sequence information. SSCS is designed upon a structure-induced transformer with three main novel improvements. SSCS captures the structural information in a multi-scale aspect with an adapted fusion strategy and adopts a hierarchical encoding strategy to capture the textual information from the perspective of the document. Moreover, SSCS utilizes a bidirectional decoder which generates a summary from opposite direction to balance the generation performance between prefix and suffix. We conduct experiments on two public Java and Python datasets to evaluate our method and the result show that SSCS outperforms the state-of-art code summarization methods.",
		"container-title": "Entropy",
		"DOI": "10.3390/e25040570",
		"ISSN": "1099-4300",
		"issue": "4",
		"title": "Structure and Sequence Aligned Code Summarization with Prefix and Suffix Balanced Strategy",
		"URL": "https://www.mdpi.com/1099-4300/25/4/570",
		"volume": "25",
		"author": [
			{
				"family": "Zeng",
				"given": "Jianhui"
			},
			{
				"family": "Qu",
				"given": "Zhiheng"
			},
			{
				"family": "Cai",
				"given": "Bo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/ILJFAGXT",
		"type": "article-journal",
		"container-title": "ArXiv",
		"title": "AST-MHSA : Code Summarization using Multi-Head Self-Attention",
		"URL": "https://api.semanticscholar.org/CorpusID:260775844",
		"volume": "abs/2308.05646",
		"author": [
			{
				"family": "Nagaraj",
				"given": "Yeshwanth"
			},
			{
				"family": "Gupta",
				"given": "Ujjwal Das"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/V7QIET8Z",
		"type": "paper-conference",
		"event-title": "Proceedings of the 26th conference on program comprehension",
		"page": "200-210",
		"title": "Deep code comment generation",
		"author": [
			{
				"family": "Hu",
				"given": "Xing"
			},
			{
				"family": "Li",
				"given": "Ge"
			},
			{
				"family": "Xia",
				"given": "Xin"
			},
			{
				"family": "Lo",
				"given": "David"
			},
			{
				"family": "Jin",
				"given": "Zhi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/L6C59MFP",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:1811.01824",
		"journalAbbreviation": "arXiv preprint arXiv:1811.01824",
		"title": "Structured neural summarization",
		"author": [
			{
				"family": "Fernandes",
				"given": "Patrick"
			},
			{
				"family": "Allamanis",
				"given": "Miltiadis"
			},
			{
				"family": "Brockschmidt",
				"given": "Marc"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/H9SD2UUN",
		"type": "article-journal",
		"container-title": "Journal of Theoretical and Applied Information Technology",
		"issue": "21",
		"journalAbbreviation": "Journal of Theoretical and Applied Information Technology",
		"title": "EVALUATING RNNS AND TRANSFORMERS FOR CODE-RELATED TASKS INCLUDING BUG DETECTION, CODE COMPLETION, AND SUMMARIZATION",
		"volume": "102",
		"author": [
			{
				"family": "PRASAD",
				"given": "RAGHUPATHY DURGA"
			},
			{
				"family": "SRIVENKATESH",
				"given": "MUKTEVI"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/QQ35LBQX",
		"type": "paper-conference",
		"event-title": "2023 International Joint Conference on Neural Networks (IJCNN)",
		"ISBN": "1-6654-8867-0",
		"page": "1-7",
		"publisher": "IEEE",
		"title": "Enhancing source code summarization from structure and semantics",
		"author": [
			{
				"family": "Lu",
				"given": "Xurong"
			},
			{
				"family": "Niu",
				"given": "Jun"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/S7EFLUTU",
		"type": "article-journal",
		"container-title": "Journal of Systems and Software",
		"ISSN": "0164-1212",
		"journalAbbreviation": "Journal of Systems and Software",
		"page": "111495",
		"title": "GT-SimNet: Improving code automatic summarization via multi-modal similarity networks",
		"volume": "194",
		"author": [
			{
				"family": "Gao",
				"given": "Xuejian"
			},
			{
				"family": "Jiang",
				"given": "Xue"
			},
			{
				"family": "Wu",
				"given": "Qiong"
			},
			{
				"family": "Wang",
				"given": "Xiao"
			},
			{
				"family": "Lyu",
				"given": "Chen"
			},
			{
				"family": "Lyu",
				"given": "Lei"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/8VJ79G3I",
		"type": "paper-conference",
		"container-title": "2023 IEEE/ACM 31st International Conference on Program Comprehension (ICPC)",
		"DOI": "10.1109/ICPC58990.2023.00024",
		"page": "89-100",
		"title": "An Extensive Study of the Structure Features in Transformer-based Code Semantic Summarization",
		"author": [
			{
				"family": "Yang",
				"given": "Kang"
			},
			{
				"family": "Mao",
				"given": "Xinjun"
			},
			{
				"family": "Wang",
				"given": "Shangwen"
			},
			{
				"family": "Qin",
				"given": "Yihao"
			},
			{
				"family": "Zhang",
				"given": "Tanghaoran"
			},
			{
				"family": "Lu",
				"given": "Yao"
			},
			{
				"family": "Al-Sabahi",
				"given": "Kamal"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/YBPAKAHP",
		"type": "article-journal",
		"container-title": "Wuhan University Journal of Natural Sciences",
		"ISSN": "1007-1202",
		"issue": "6",
		"journalAbbreviation": "Wuhan University Journal of Natural Sciences",
		"page": "474-482",
		"title": "Improve Code Summarization via Prompt-Tuning CodeT5",
		"volume": "28",
		"author": [
			{
				"family": "Huanzhen",
				"given": "LI"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/PW4L5TQ8",
		"type": "paper-conference",
		"event-title": "2022 International Joint Conference on Neural Networks (IJCNN)",
		"ISBN": "1-7281-8671-4",
		"page": "1-8",
		"publisher": "IEEE",
		"title": "Summarizing source code from structure and context",
		"author": [
			{
				"family": "Hou",
				"given": "Shifu"
			},
			{
				"family": "Chen",
				"given": "Lingwei"
			},
			{
				"family": "Ye",
				"given": "Yanfang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/HDLWS8YX",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2105.08645",
		"journalAbbreviation": "arXiv preprint arXiv:2105.08645",
		"title": "Cotext: Multi-task learning with code-text transformer",
		"author": [
			{
				"family": "Phan",
				"given": "Long"
			},
			{
				"family": "Tran",
				"given": "Hieu"
			},
			{
				"family": "Le",
				"given": "Daniel"
			},
			{
				"family": "Nguyen",
				"given": "Hieu"
			},
			{
				"family": "Anibal",
				"given": "James"
			},
			{
				"family": "Peltekian",
				"given": "Alec"
			},
			{
				"family": "Ye",
				"given": "Yanfang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/NQ8ZXC56",
		"type": "paper-conference",
		"event-title": "European Conference on Information Retrieval",
		"page": "439-447",
		"publisher": "Springer",
		"title": "Leveraging Comment Retrieval for Code Summarization",
		"author": [
			{
				"family": "Hou",
				"given": "Shifu"
			},
			{
				"family": "Chen",
				"given": "Lingwei"
			},
			{
				"family": "Ju",
				"given": "Mingxuan"
			},
			{
				"family": "Ye",
				"given": "Yanfang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/F7AFB6BT",
		"type": "article-journal",
		"container-title": "Electronics",
		"ISSN": "2079-9292",
		"issue": "4",
		"journalAbbreviation": "Electronics",
		"page": "765",
		"title": "Statement-Grained Hierarchy Enhanced Code Summarization",
		"volume": "13",
		"author": [
			{
				"family": "Zhang",
				"given": "Qianjin"
			},
			{
				"family": "Jin",
				"given": "Dahai"
			},
			{
				"family": "Wang",
				"given": "Yawen"
			},
			{
				"family": "Gong",
				"given": "Yunzhan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/VC2M663R",
		"type": "paper-conference",
		"event-title": "IET Conference Proceedings CP777",
		"ISBN": "1-83953-522-9",
		"page": "437-443",
		"publisher": "IET",
		"title": "A source code summarization technique for object oriented classes",
		"volume": "2020",
		"author": [
			{
				"family": "Yusuf",
				"given": "Afrah"
			},
			{
				"family": "Hammad",
				"given": "Mustafa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/HKLDSHUZ",
		"type": "paper-conference",
		"event-title": "2023 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)",
		"ISBN": "1-6654-5278-1",
		"page": "768-772",
		"publisher": "IEEE",
		"title": "Leveraging and evaluating automatic code summarization for JPA program comprehension",
		"author": [
			{
				"family": "Mayer",
				"given": "Richard"
			},
			{
				"family": "Moser",
				"given": "Michael"
			},
			{
				"family": "Geist",
				"given": "Verena"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/9J9PREYX",
		"type": "article-journal",
		"title": "Source-code Summarization of Java Methods Using Control-Flow Graphs",
		"author": [
			{
				"family": "Beyene",
				"given": "Michael"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/UNA67Z65",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2309.09558",
		"journalAbbreviation": "arXiv preprint arXiv:2309.09558",
		"title": "Summarization is (almost) dead",
		"author": [
			{
				"family": "Pu",
				"given": "Xiao"
			},
			{
				"family": "Gao",
				"given": "Mingqi"
			},
			{
				"family": "Wan",
				"given": "Xiaojun"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/BIJGWYBD",
		"type": "paper-conference",
		"event-title": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Lond, UK",
		"title": "A comprehensive study of staqc for deep code summarization",
		"author": [
			{
				"family": "Peddamail",
				"given": "Jayavardhan Reddy"
			},
			{
				"family": "Yao",
				"given": "Ziyu"
			},
			{
				"family": "Wang",
				"given": "Zhen"
			},
			{
				"family": "Sun",
				"given": "Huan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/H3PESTE4",
		"type": "paper-conference",
		"container-title": "2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)",
		"DOI": "10.1109/SANER53432.2022.00052",
		"page": "361-372",
		"title": "DualSC: Automatic Generation and Summarization of Shellcode via Transformer and Dual Learning",
		"author": [
			{
				"family": "Yang",
				"given": "Guang"
			},
			{
				"family": "Chen",
				"given": "Xiang"
			},
			{
				"family": "Zhou",
				"given": "Yanlin"
			},
			{
				"family": "Yu",
				"given": "Chi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/CNWQAA3M",
		"type": "paper-conference",
		"abstract": "When comprehending code, a helping hand may come from the natural language comments documenting it that, unfortunately, are not always there. To support developers in such a scenario, several techniques have been presented to automatically generate natural language summaries for a given code. Most recent approaches exploit deep learning (DL) to automatically document classes or functions, while little effort has been devoted to more fine-grained documentation (e.g., documenting code snippets or even a single statement). Such a design choice is dictated by the availability of training data: For example, in the case of Java, it is easy to create datasets composed of pairs <method, javadoc> that can be fed to DL models to teach them how to summarize a method. Such a comment-to-code linking is instead non-trivial when it comes to inner comments documenting a few statements. In this work, we take all the steps needed to train a DL model to automatically document code snippets. First, we manually built a dataset featuring 6.6k comments that have been (i) classified based on their type (e.g., code summary, TODO), and (ii) linked to the code statements they document. Second, we used such a dataset to train a multi-task DL model taking as input a comment and being able to (i) classify whether it represents a \"code summary\" or not, and (ii) link it to the code statements it documents. Our model identifies code summaries with 84% accuracy and is able to link them to the documented lines of code with recall and precision higher than 80%. Third, we run this model on 10k projects, identifying and linking code summaries to the documented code. This unlocked the possibility of building a large-scale dataset of documented code snippets that have then been used to train a new DL model able to automatically document code snippets. A comparison with state-of-the-art baselines shows the superiority of the proposed approach, which however, is still far from representing an accurate solution for snippet summarization.",
		"collection-title": "ICPC '24",
		"container-title": "Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension",
		"DOI": "10.1145/3643916.3644400",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0586-1",
		"note": "event-place: Lisbon, Portugal",
		"page": "1–12",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards Summarizing Code Snippets Using Pre-Trained Transformers",
		"URL": "https://doi.org/10.1145/3643916.3644400",
		"author": [
			{
				"family": "Mastropaolo",
				"given": "Antonio"
			},
			{
				"family": "Ciniselli",
				"given": "Matteo"
			},
			{
				"family": "Pascarella",
				"given": "Luca"
			},
			{
				"family": "Tufano",
				"given": "Rosalia"
			},
			{
				"family": "Aghajani",
				"given": "Emad"
			},
			{
				"family": "Bavota",
				"given": "Gabriele"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/QC4RAYYS",
		"type": "article-journal",
		"container-title": "IEEE Transactions on Artificial Intelligence",
		"ISSN": "2691-4581",
		"journalAbbreviation": "IEEE Transactions on Artificial Intelligence",
		"title": "Improving Code Summarization with Tree Transformer Enhanced by Position-related Syntax Complement",
		"author": [
			{
				"family": "Song",
				"given": "Jie"
			},
			{
				"family": "Zhang",
				"given": "Zexing"
			},
			{
				"family": "Tang",
				"given": "Zirui"
			},
			{
				"family": "Feng",
				"given": "Shi"
			},
			{
				"family": "Gu",
				"given": "Yu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/SL4XNGHB",
		"type": "paper-conference",
		"event-title": "2024 IEEE 48th Annual Computers, Software, and Applications Conference (COMPSAC)",
		"ISBN": "979-8-3503-7696-8",
		"page": "1344-1349",
		"publisher": "IEEE",
		"title": "Context-Focused Prompt Tuning Pre-Trained Code Models to Improve Code Summarization",
		"author": [
			{
				"family": "Pan",
				"given": "Xinglu"
			},
			{
				"family": "Liu",
				"given": "Chenxiao"
			},
			{
				"family": "Zou",
				"given": "Yanzhen"
			},
			{
				"family": "Zhao",
				"given": "Xianlin"
			},
			{
				"family": "Xie",
				"given": "Bing"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/GLDW3YKI",
		"type": "paper-conference",
		"event-title": "2023 International Joint Conference on Neural Networks (IJCNN)",
		"ISBN": "1-6654-8867-0",
		"page": "01-09",
		"publisher": "IEEE",
		"title": "A Semantic and Structural Transformer for Code Summarization Generation",
		"author": [
			{
				"family": "Ji",
				"given": "Ruyi"
			},
			{
				"family": "Tong",
				"given": "Zhenyu"
			},
			{
				"family": "Luo",
				"given": "Tiejian"
			},
			{
				"family": "Liu",
				"given": "Jing"
			},
			{
				"family": "Zhang",
				"given": "Libo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/KS2JL6SP",
		"type": "paper-conference",
		"event-title": "2022 IEEE 13th International Symposium on Parallel Architectures, Algorithms and Programming (PAAP)",
		"ISBN": "1-6654-5218-8",
		"page": "1-6",
		"publisher": "IEEE",
		"title": "Do Not Have Enough Data? An Easy Data Augmentation for Code Summarization",
		"author": [
			{
				"family": "Song",
				"given": "Zixuan"
			},
			{
				"family": "Shang",
				"given": "Xiuwei"
			},
			{
				"family": "Li",
				"given": "Mengxuan"
			},
			{
				"family": "Chen",
				"given": "Rong"
			},
			{
				"family": "Li",
				"given": "Hui"
			},
			{
				"family": "Guo",
				"given": "Shikai"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/U7JC3YT2",
		"type": "article-journal",
		"container-title": "Applied Intelligence",
		"ISSN": "0924-669X",
		"issue": "2",
		"journalAbbreviation": "Applied Intelligence",
		"page": "211",
		"title": "Leveraging meta-data of code for adapting prompt tuning for code summarization",
		"volume": "55",
		"author": [
			{
				"family": "Jiang",
				"given": "Zhihua"
			},
			{
				"family": "Wang",
				"given": "Di"
			},
			{
				"family": "Rao",
				"given": "Dongning"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2025"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/Y2JZZA62",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2407.10424",
		"journalAbbreviation": "arXiv preprint arXiv:2407.10424",
		"title": "Codev: Empowering llms for verilog generation through multi-level summarization",
		"author": [
			{
				"family": "Zhao",
				"given": "Yang"
			},
			{
				"family": "Huang",
				"given": "Di"
			},
			{
				"family": "Li",
				"given": "Chongxiao"
			},
			{
				"family": "Jin",
				"given": "Pengwei"
			},
			{
				"family": "Nan",
				"given": "Ziyuan"
			},
			{
				"family": "Ma",
				"given": "Tianyun"
			},
			{
				"family": "Qi",
				"given": "Lei"
			},
			{
				"family": "Pan",
				"given": "Yansong"
			},
			{
				"family": "Zhang",
				"given": "Zhenxing"
			},
			{
				"family": "Zhang",
				"given": "Rui"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/DD7LTZBM",
		"type": "paper-conference",
		"container-title": "2021 IEEE/ACM 43rd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)",
		"DOI": "10.1109/ICSE-Companion52605.2021.00141",
		"page": "330-331",
		"title": "Advances in Code Summarization",
		"author": [
			{
				"family": "Desai",
				"given": "Utkarsh"
			},
			{
				"family": "Sridhara",
				"given": "Giriprasad"
			},
			{
				"family": "Tamilselvam",
				"given": "Srikanth"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/TB6HGUUZ",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2410.14102",
		"journalAbbreviation": "arXiv preprint arXiv:2410.14102",
		"title": "Beyond Dataset Watermarking: Model-Level Copyright Protection for Code Summarization Models",
		"author": [
			{
				"family": "Zhang",
				"given": "Jiale"
			},
			{
				"family": "Li",
				"given": "Haoxuan"
			},
			{
				"family": "Wu",
				"given": "Di"
			},
			{
				"family": "Sun",
				"given": "Xiaobing"
			},
			{
				"family": "Lu",
				"given": "Qinghua"
			},
			{
				"family": "Long",
				"given": "Guodong"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/XMYMZ65C",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2310.16314",
		"journalAbbreviation": "arXiv preprint arXiv:2310.16314",
		"title": "Understanding Code Semantics: An Evaluation of Transformer Models in Summarization",
		"author": [
			{
				"family": "Mondal",
				"given": "Debanjan"
			},
			{
				"family": "Lodha",
				"given": "Abhilasha"
			},
			{
				"family": "Sahoo",
				"given": "Ankita"
			},
			{
				"family": "Kumari",
				"given": "Beena"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/68SGPWK6",
		"type": "paper-conference",
		"event-title": "International Conference on Neural Information Processing",
		"page": "567-578",
		"publisher": "Springer",
		"title": "LenANet: A Length-Controllable Attention Network for Source Code Summarization",
		"author": [
			{
				"family": "Chen",
				"given": "Peng"
			},
			{
				"family": "Wu",
				"given": "Shaojuan"
			},
			{
				"family": "Chen",
				"given": "Ziqiang"
			},
			{
				"family": "Zhang",
				"given": "Jiarui"
			},
			{
				"family": "Zhang",
				"given": "Xiaowang"
			},
			{
				"family": "Feng",
				"given": "Zhiyong"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/KKRVC55S",
		"type": "article-journal",
		"container-title": "IEEE Software",
		"DOI": "10.1109/MS.2003.1241364",
		"issue": "6",
		"page": "35-39",
		"title": "How software engineers use documentation: the state of the practice",
		"volume": "20",
		"author": [
			{
				"family": "Lethbridge",
				"given": "T.C."
			},
			{
				"family": "Singer",
				"given": "J."
			},
			{
				"family": "Forward",
				"given": "A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2003"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/4PXXDLRT",
		"type": "article-journal",
		"title": "The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers",
		"author": [
			{
				"family": "Lee",
				"given": "Hao-Ping Hank"
			},
			{
				"family": "Sarkar",
				"given": "Advait"
			},
			{
				"family": "Tankelevitch",
				"given": "Lev"
			},
			{
				"family": "Drosos",
				"given": "Ian"
			},
			{
				"family": "Rintel",
				"given": "Sean"
			},
			{
				"family": "Banks",
				"given": "Richard"
			},
			{
				"family": "Wilson",
				"given": "Nicholas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2025"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/AMEHWWH2",
		"type": "paper-conference",
		"abstract": "Background: Accurate representation of developer expertise has always been an important research problem. While a number of studies proposed novel methods of representing expertise within individual projects, these methods are difficult to apply at an ecosystem level. However, with the focus of software development shifting from monolithic to modular, a method of representing developers' expertise in the context of the entire OSS development becomes necessary when, for example, a project tries to find new maintainers and look for developers with relevant skills. Aim: We aim to address this knowledge gap by proposing and constructing the Skill Space where each API, developer, and project is represented and postulate how the topology of this space should reflect what developers know (and projects need). Method: we use the World of Code infrastructure to extract the complete set of APIs in the files changed by open source developers and, based on that data, employ Doc2Vec embeddings for vector representations of APIs, developers, and projects. We then evaluate if these embeddings reflect the postulated topology of the Skill Space by predicting what new APIs/projects developers use/join, and whether or not their pull requests get accepted. We also check how the developers' representations in the Skill Space align with their self-reported API expertise. Result: Our results suggest that the proposed embeddings in the Skill Space appear to satisfy the postulated topology and we hope that such representations may aid in the construction of signals that increase trust (and efficiency) of open source ecosystems at large and may aid investigations of other phenomena related to developer proficiency and learning.",
		"container-title": "2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)",
		"DOI": "10.1109/ICSE43902.2021.00094",
		"page": "995-1007",
		"title": "Representation of Developer Expertise in Open Source Software",
		"author": [
			{
				"family": "Dey",
				"given": "Tapajit"
			},
			{
				"family": "Karnauch",
				"given": "Andrey"
			},
			{
				"family": "Mockus",
				"given": "Audris"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					5
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/KVV3Z79X",
		"type": "paper-conference",
		"abstract": "We propose Project Context for Code Summarization with LLMs (ProConSuL), a new framework to provide a large language model (LLM) with precise information about the code structure from program analysis methods such as a compiler or IDE language services and use task decomposition derived from the code structure. ProConSuL builds a call graph to provide the context from callees and uses a two-phase training method (SFT + preference alignment) to train the model to use the project context. We also provide a new evaluation benchmark for C/C++ functions and a set of proxy metrics. Experimental results demonstrate that ProConSuL allows to significantly improve code summaries and reduce the number of hallucinations compared to the base model (CodeLlama-7B-instruct). We make our code and dataset available at https://github.com/TypingCat13/ProConSuL.",
		"container-title": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track",
		"DOI": "10.18653/v1/2024.emnlp-industry.65",
		"event-place": "Miami, Florida, US",
		"page": "866–880",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Miami, Florida, US",
		"title": "ProConSuL: Project Context for Code Summarization with LLMs",
		"URL": "https://aclanthology.org/2024.emnlp-industry.65/",
		"author": [
			{
				"family": "Lomshakov",
				"given": "Vadim"
			},
			{
				"family": "Podivilov",
				"given": "Andrey"
			},
			{
				"family": "Savin",
				"given": "Sergey"
			},
			{
				"family": "Baryshnikov",
				"given": "Oleg"
			},
			{
				"family": "Lisevych",
				"given": "Alena"
			},
			{
				"family": "Nikolenko",
				"given": "Sergey"
			}
		],
		"editor": [
			{
				"family": "Dernoncourt",
				"given": "Franck"
			},
			{
				"family": "Preoţiuc-Pietro",
				"given": "Daniel"
			},
			{
				"family": "Shimorina",
				"given": "Anastasia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024",
					11
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/YYTDMNGL",
		"type": "article-journal",
		"abstract": "Prompt tuning alleviates the gap between pre-training and fine-tuning and achieves promising results in various natural language processing (NLP) tasks. However, it is nontrivial for adapting prompt tuning in intelligent code tasks since code-specific knowledge such as abstract syntax tree is usually hierarchy-structured and therefore is hard to be converted into plain text. Recent works (e.g., PT4Code) introduce simple task prompts along with a programming language indicator into prompt template, achieving improvement over non-prompting state-of-the-art code models (e.g., CodeT5). Inspired by this, we propose a novel code-specific prompt paradigm, meta-data prompt, which introduces semi-structured code’s meta-data (attribute-value pairs) into prompt template and facilitates the adaption of prompt tuning techniques into code tasks. Specifically, we find the usage of diverse meta-data attributes and their combinations and employ the OpenPrompt to implement a meta-data prompt based code model, PRIME (PRompt tunIng with MEta-data), via utilizing CodeT5 as the backbone model. We experiment PRIME with the source code summarization task on the publicly available CodeSearchNet benchmark. Results show that 1) using good meta-data can lead to an improvement on the model performance; 2) the proposed meta-data prompt can be combined with traditional task prompt for further improvement; 3) our best-performing model can consistently outperform CodeT5 by an absolute score of 0.73 and PT4Code by an absolute score of 0.48 regarding the averaged BLEU metric across six programming languages.",
		"container-title": "Applied Intelligence",
		"DOI": "10.1007/s10489-024-06197-0",
		"ISSN": "1573-7497",
		"issue": "3",
		"journalAbbreviation": "Applied Intelligence",
		"page": "211",
		"title": "Leveraging meta-data of code for adapting prompt tuning for code summarization",
		"URL": "https://doi.org/10.1007/s10489-024-06197-0",
		"volume": "55",
		"author": [
			{
				"family": "Jiang",
				"given": "Zhihua"
			},
			{
				"family": "Wang",
				"given": "Di"
			},
			{
				"family": "Rao",
				"given": "Dongning"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024",
					12,
					23
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/VBHADW9L",
		"type": "article-journal",
		"abstract": "Summary Code comments convey information about the programmers' intention in a more explicit but less rigorous manner than source code. This information can assist programmers in various tasks, such as code comprehension, reuse, and maintenance. To better understand the properties of the comments existing in the source code, we analyzed more than 450 000 comments across 136 popular open-source software systems coming different domains. We found that the methods involving header comments and internal comments were shown low percentages in software systems, ie, 4.4% and 10.27%, respectively. As an application of our findings, we propose an automatic approach to determine whether a method needs a header comment, known as commenting necessity identification. Specifically, we identify the important factors for determining the commenting necessity of a method and extract them as structural features, syntactic features, and textual features. Then, by applying machine learning techniques and noise-handling techniques, we achieve a precision of 88.5% on eight open-source software from GitHub. The encouraging experimental results demonstrate the feasibility and effectiveness of our approach.",
		"container-title": "Software: Practice and Experience",
		"DOI": "https://doi.org/10.1002/spe.2772",
		"issue": "3",
		"page": "227-245",
		"title": "Does your code need comment?",
		"URL": "https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2772",
		"volume": "50",
		"author": [
			{
				"family": "Huang",
				"given": "Yuan"
			},
			{
				"family": "Jia",
				"given": "Nan"
			},
			{
				"family": "Shu",
				"given": "Junhuai"
			},
			{
				"family": "Hu",
				"given": "Xinyu"
			},
			{
				"family": "Chen",
				"given": "Xiangping"
			},
			{
				"family": "Zhou",
				"given": "Qiang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/3R2EJXTJ",
		"type": "paper-conference",
		"abstract": "Code comments can provide a great data source for understanding programmer's needs and underlying implementation. Previous work has illustrated that code comments enhance the reliability and maintainability of the code, and engineers use them to interpret their code as well as help other developers understand the code intention better. In this paper, we studied comments from 7 python open source projects and contrived a taxonomy through an iterative process. To clarify comments characteristics, we deploy an effective and automated approach using supervised learning algorithms to classify code comments according to their different intentions. With our study, we find that there does exist a pattern across different python projects: Summary covers about 75% of comments. Finally, we conduct an evaluation on the behaviors of two different supervised learning classifiers and find that Decision Tree classifier is more effective on accuracy and runtime than Naive Bayes classifier in our research.",
		"container-title": "Web Information Systems and Applications",
		"event-place": "Cham",
		"ISBN": "978-3-030-02934-0",
		"page": "39–47",
		"publisher": "Springer International Publishing",
		"publisher-place": "Cham",
		"title": "Classifying Python Code Comments Based on Supervised Learning",
		"author": [
			{
				"family": "Zhang",
				"given": "Jingyi"
			},
			{
				"family": "Xu",
				"given": "Lei"
			},
			{
				"family": "Li",
				"given": "Yanhui"
			}
		],
		"editor": [
			{
				"family": "Meng",
				"given": "Xiaofeng"
			},
			{
				"family": "Li",
				"given": "Ruixuan"
			},
			{
				"family": "Wang",
				"given": "Kanliang"
			},
			{
				"family": "Niu",
				"given": "Baoning"
			},
			{
				"family": "Wang",
				"given": "Xin"
			},
			{
				"family": "Zhao",
				"given": "Gansen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/EYCR56GM",
		"type": "article-journal",
		"abstract": "Code comments are one of the most useful forms of documentation and metadata for understanding software implementation. Previous research on code comment classification has focused only on comments in English, typically extracted from a few programming languages. This paper addresses the problem of code comment classification not only in the monolingual setting, but also in the multilingual and cross-lingual one, in order to examine whether they can outperform the traditional monolingual approach. To tackle this task, we introduce a novel, publicly available code comment dataset, consisting of over 10,000 code comments collected from software projects written in eight programming languages (C, C++, C#, Java, JavaScript/TypeScript, PHP, Python, and SQL). About half of them are written in Serbian while the other half are written in English. This dataset was manually annotated according to a newly proposed taxonomy of code comment categories. We fine-tune and evaluate multiple monolingual and multilingual pre-trained neural language models on the code comment classification task and compare their performances to several baselines. The best results for Serbian comments are obtained using the monolingual neural model BERTić, trained on Serbian and closely related languages. On the other hand, the optimal choice for English is the multilingual neural model multilingual BERT, which successfully extracts useful patterns from data in both languages. Although the cross-lingual setting shows some promise for simple binary classification, it has yet to reach sufficiently high performance levels for practical use. We also analyze model performance across different programming languages.",
		"container-title": "Engineering Applications of Artificial Intelligence",
		"DOI": "https://doi.org/10.1016/j.engappai.2023.106485",
		"ISSN": "0952-1976",
		"page": "106485",
		"title": "Monolingual, multilingual and cross-lingual code comment classification",
		"URL": "https://www.sciencedirect.com/science/article/pii/S0952197623006693",
		"volume": "124",
		"author": [
			{
				"family": "Kostić",
				"given": "Marija"
			},
			{
				"family": "Batanović",
				"given": "Vuk"
			},
			{
				"family": "Nikolić",
				"given": "Boško"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/GZDFC57K",
		"type": "paper-conference",
		"container-title": "2018 25th Asia-Pacific Software Engineering Conference (APSEC)",
		"DOI": "10.1109/APSEC.2018.00047",
		"page": "325-334",
		"title": "Analyzing Code Comments to Boost Program Comprehension",
		"author": [
			{
				"family": "Shinyama",
				"given": "Yusuke"
			},
			{
				"family": "Arahori",
				"given": "Yoshitaka"
			},
			{
				"family": "Gondow",
				"given": "Katsuhiko"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/XN9DCLJZ",
		"type": "article-journal",
		"abstract": "High-quality source code comments are valuable for software development and maintenance, however, code often contains low-quality comments or lacks them altogether. We name such source code comments as suboptimal comments. Such suboptimal comments create challenges in code comprehension and maintenance. Despite substantial research on low-quality source code comments, empirical knowledge about commenting practices that produce suboptimal comments and reasons that lead to suboptimal comments are lacking. We help bridge this knowledge gap by investigating (1)&nbsp;independent comment changes (ICCs)—comment changes committed independently of code changes—which likely address suboptimal comments, (2)&nbsp;commenting guidelines, and (3)&nbsp;comment-checking tools and comment-generating tools, which are often employed to help commenting practice—especially to prevent suboptimal comments. We collect 24M+ comment changes from 4,392 open-source GitHub Java repositories and find that ICCs widely exist. The ICC ratio—proportion of ICCs among all comment changes—is 15.5%, with 98.7% of the repositories having ICC. Our thematic analysis of 3,533 randomly sampled ICCs provides a three-dimensional taxonomy for what is changed (four comment categories and 13 subcategories), how it changed (six commenting activity categories), and what factors are associated with the change (three factors). We investigate 600 repositories to understand the prevalence, content, impact, and violations of commenting guidelines. We find that only 15.5% of the 600 sampled repositories have any commenting guidelines. We provide the first taxonomy for elements in commenting guidelines: where and what to comment are particularly important. The repositories without such guidelines have a statistically significantly higher ICC ratio, indicating the negative impact of the lack of commenting guidelines. However, commenting guidelines are not strictly followed: 85.5% of checked repositories have violations. We also systematically study how developers use two kinds of tools, comment-checking tools and comment-generating tools, in the 4,392 repositories. We find that the use of Javadoc tool is negatively correlated with the ICC ratio, while the use of Checkstyle has no statistically significant correlation; the use of comment-generating tools leads to a higher ICC ratio. To conclude, we reveal issues and challenges in current commenting practice, which help understand how suboptimal comments are introduced. We propose potential research directions on comment location prediction, comment generation, and comment quality assessment; suggest how developers can formulate commenting guidelines and enforce rules with tools; and recommend how to enhance current comment-checking and comment-generating tools.",
		"container-title": "ACM Trans. Softw. Eng. Methodol.",
		"DOI": "10.1145/3546949",
		"ISSN": "1049-331X",
		"issue": "2",
		"title": "Suboptimal Comments in Java Projects: From Independent Comment Changes to Commenting Practices",
		"URL": "https://doi.org/10.1145/3546949",
		"volume": "32",
		"author": [
			{
				"family": "Wang",
				"given": "Chao"
			},
			{
				"family": "He",
				"given": "Hao"
			},
			{
				"family": "Pal",
				"given": "Uma"
			},
			{
				"family": "Marinov",
				"given": "Darko"
			},
			{
				"family": "Zhou",
				"given": "Minghui"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					3
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/7YLVK4B6",
		"type": "article-journal",
		"abstract": "Code summarization aims at generating a code comment given a block of source code and it is normally performed by training machine learning algorithms on existing code block-comment pairs. Code comments in practice have different intentions. For example, some code comments might explain how the methods work, while others explain why some methods are written. Previous works have shown that a relationship exists between a code block and the category of a comment associated with it. In this article, we aim to investigate to which extent we can exploit this relationship to improve code summarization performance. We first classify comments into six intention categories and manually label 20,000 code-comment pairs. These categories include “what,” “why,” “how-to-use,” “how-it-is-done,” “property,” and “others.” Based on this dataset, we conduct an experiment to investigate the performance of different state-of-the-art code summarization approaches on the categories. We find that the performance of different code summarization approaches varies substantially across the categories. Moreover, the category for which a code summarization model performs the best is different for the different models. In particular, no models perform the best for “why” and “property” comments among the six categories. We design a composite approach to demonstrate that comment category prediction can boost code summarization to reach better results. The approach leverages classified code-category labeled data to train a classifier to infer categories. Then it selects the most suitable models for inferred categories and outputs the composite results. Our composite approach outperforms other approaches that do not consider comment categories and obtains a relative improvement of 8.57% and 16.34% in terms of ROUGE-L and BLEU-4 score, respectively.",
		"container-title": "ACM Trans. Softw. Eng. Methodol.",
		"DOI": "10.1145/3434280",
		"ISSN": "1049-331X",
		"issue": "2",
		"title": "Why My Code Summarization Model Does Not Work: Code Comment Improvement with Category Prediction",
		"URL": "https://doi.org/10.1145/3434280",
		"volume": "30",
		"author": [
			{
				"family": "Chen",
				"given": "Qiuyuan"
			},
			{
				"family": "Xia",
				"given": "Xin"
			},
			{
				"family": "Hu",
				"given": "Han"
			},
			{
				"family": "Lo",
				"given": "David"
			},
			{
				"family": "Li",
				"given": "Shanping"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					2
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/6PCMKHSN",
		"type": "paper-conference",
		"container-title": "2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)",
		"DOI": "10.1109/MSR.2017.63",
		"page": "227-237",
		"title": "Classifying Code Comments in Java Open-Source Software Systems",
		"author": [
			{
				"family": "Pascarella",
				"given": "Luca"
			},
			{
				"family": "Bacchelli",
				"given": "Alberto"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/HMEKW9BX",
		"type": "article-journal",
		"abstract": "Code comments are a key software component containing information about the underlying implementation. Several studies have shown that code comments enhance the readability of the code. Nevertheless, not all the comments have the same goal and target audience. In this paper, we investigate how 14 diverse Java open and closed source software projects use code comments, with the aim of understanding their purpose. Through our analysis, we produce a taxonomy of source code comments; subsequently, we investigate how often each category occur by manually classifying more than 40,000 lines of code comments from the aforementioned projects. In addition, we investigate how to automatically classify code comments at line level into our taxonomy using machine learning; initial results are promising and suggest that an accurate classification is within reach, even when training the machine learner on projects different than the target one. Data and Materials [https://doi.org/10.5281/zenodo.2628361].",
		"container-title": "Empirical Software Engineering",
		"DOI": "10.1007/s10664-019-09694-w",
		"ISSN": "1573-7616",
		"issue": "3",
		"journalAbbreviation": "Empirical Software Engineering",
		"page": "1499-1537",
		"title": "Classifying code comments in Java software systems",
		"URL": "https://doi.org/10.1007/s10664-019-09694-w",
		"volume": "24",
		"author": [
			{
				"family": "Pascarella",
				"given": "Luca"
			},
			{
				"family": "Bruntink",
				"given": "Magiel"
			},
			{
				"family": "Bacchelli",
				"given": "Alberto"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019",
					6,
					1
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/LFUBPRTT",
		"type": "paper-conference",
		"event-title": "2018 ASEE Annual Conference & Exposition",
		"title": "Coding the coders: A qualitative investigation of students’ commenting patterns",
		"author": [
			{
				"family": "Mohammadi-Aragh",
				"given": "Mahnas Jean"
			},
			{
				"family": "Beck",
				"given": "Phyllis J"
			},
			{
				"family": "Barton",
				"given": "Amy K"
			},
			{
				"family": "Reese",
				"given": "Donna"
			},
			{
				"family": "Jones",
				"given": "Bryan A"
			},
			{
				"family": "Jankun-Kelly",
				"given": "Monika"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/SB7BZ5MG",
		"type": "paper-conference",
		"container-title": "2012 35th Annual IEEE Software Engineering Workshop",
		"DOI": "10.1109/SEW.2012.8",
		"page": "11-20",
		"title": "A Comment Analysis Approach for Program Comprehension",
		"author": [
			{
				"family": "Freitas",
				"given": "José Luís"
			},
			{
				"family": "Cruz",
				"given": "Daniela",
				"non-dropping-particle": "da"
			},
			{
				"family": "Henriques",
				"given": "Pedro Rangel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2012"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/LZPYIN8K",
		"type": "article-journal",
		"abstract": "Code comments are considered an efficient way to document the functionality of a particular block of code. Code commenting is a common practice among developers to explain the purpose of the code in order to improve code comprehension and readability. Researchers investigated the effect of code comments on software development tasks and demonstrated the use of comments in several ways, including maintenance, reusability, bug detection, etc. Given the importance of code comments, it becomes vital for novice developers to brush up on their code commenting skills. In this study, we initially investigated what types of comments novice students document in their source code and further categorized those comments using a machine learning approach. The work involves the initial manual classification of code comments and then building a machine learning model to classify student code comments automatically. The findings of our study revealed that novice developers/students’ comments are mainly related to Literal (26.66%) and Insufficient (26.66%). Further, we proposed and extended the taxonomy of such source code comments by adding a few more categories, i.e., License (5.18%), Profile (4.80%), Irrelevant (4.80%), Commented Code (4.44%), Autogenerated (1.48%), and Improper (1.10%). Moreover, we assessed our approach with three different machine-learning classifiers. Our implementation of machine learning models found that Decision Tree resulted in the overall highest accuracy, i.e., 85%. This study helps in predicting the type of code comments for a novice developer using a machine learning approach that can be implemented to generate automated feedback for students, thus saving teachers time for manual one-on-one feedback, which is a time-consuming activity.",
		"container-title": "Algorithms",
		"DOI": "10.3390/a16010053",
		"ISSN": "1999-4893",
		"issue": "1",
		"title": "Investigating Novice Developers’ Code Commenting Trends Using Machine Learning Techniques",
		"volume": "16",
		"author": [
			{
				"family": "Niazi",
				"given": "Tahira"
			},
			{
				"family": "Das",
				"given": "Teerath"
			},
			{
				"family": "Ahmed",
				"given": "Ghufran"
			},
			{
				"family": "Waqas",
				"given": "Syed M."
			},
			{
				"family": "Khan",
				"given": "Sumra"
			},
			{
				"family": "Khan",
				"given": "Suleman"
			},
			{
				"family": "Abdelatif",
				"given": "Ahmed A."
			},
			{
				"family": "Wasi",
				"given": "Shaukat"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/5PVGS3XL",
		"type": "paper-conference",
		"abstract": "Code comments are important for developers in program comprehension. In scenarios of comprehending and reusing a method, developers expect code comments to provide supplementary information beyond the method signature. However, the extent of such supplementary information varies a lot in different code comments. In this paper, we raise the awareness of the supplementary nature of method-level comments and propose a new metric named MESIA (Mean Supplementary Information Amount) to assess the extent of supplementary information that a code comment can provide. With the MESIA metric, we conduct experiments on a popular code-comment dataset and three common types of neural approaches to generate method-level comments. Our experimental results demonstrate the value of our proposed work with a number of findings. (1) Small-MESIA comments occupy around 20% of the dataset and mostly fall into only the WHAT comment category. (2) Being able to provide various kinds of essential information, large-MESIA comments in the dataset are difficult for existing neural approaches to generate. (3) We can improve the capability of existing neural approaches to generate large-MESIA comments by reducing the proportion of small-MESIA comments in the training set. (4) The retrained model can generate large-MESIA comments that convey essential meaningful supplementary information for methods in the small-MESIA test set, but will get a lower BLEU score in evaluation. These findings indicate that with good training data, auto-generated comments can sometimes even surpass human-written reference comments, and having no appropriate ground truth for evaluation is an issue that needs to be addressed by future work on automatic comment generation.",
		"collection-title": "ICPC '24",
		"container-title": "Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension",
		"DOI": "10.1145/3643916.3644401",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0586-1",
		"note": "event-place: Lisbon, Portugal",
		"page": "74–86",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "MESIA: Understanding and Leveraging Supplementary Nature of Method-level Comments for Automatic Comment Generation",
		"URL": "https://doi.org/10.1145/3643916.3644401",
		"author": [
			{
				"family": "Pan",
				"given": "Xinglu"
			},
			{
				"family": "Liu",
				"given": "Chenxiao"
			},
			{
				"family": "Zou",
				"given": "Yanzhen"
			},
			{
				"family": "Xie",
				"given": "Tao"
			},
			{
				"family": "Xie",
				"given": "Bing"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/YLHLMNJM",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2405.10243",
		"title": "DocuMint: Docstring Generation for Python using Small Language Models",
		"author": [
			{
				"family": "Poudel",
				"given": "Bibek"
			},
			{
				"family": "Cook",
				"given": "Adam"
			},
			{
				"family": "Traore",
				"given": "Sekou"
			},
			{
				"family": "Ameli",
				"given": "Shelah"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/C5LU7JM9",
		"type": "article-journal",
		"abstract": "A brief, fluent, and relevant summary can be helpful during program comprehension; however, such a summary does require significant human effort to produce. Often, good summaries are unavailable in software projects, which makes maintenance more difficult. There has been a considerable body of research into automated AI-based methods, using Large Language models (LLMs), to generate summaries of code; there also has been quite a bit of work on ways to measure the performance of such summarization methods, with special attention paid to how closely these AI-generated summaries resemble a summary a human might have produced. Measures such as BERTScore and BLEU have been suggested and evaluated with human-subject studies. However, LLM-generated summaries can be inaccurate, incomplete, etc: generally, too dissimilar to one that a good developer might write. Given an LLM-generated code summary, how can a user rationally judge if a summary is sufficiently good and reliable? Given just some input source code, and an LLM-generated summary, existing approaches can help judge brevity, fluency and relevance of the summary; however, it’s difficult to gauge whether an LLM-generated summary sufficiently resembles what a human might produce, without a “golden” human-produced summary to compare against. Prior research indicates that human-produced summaries are generally preferred by human-raters, so we explore this issue in this paper. We study this resemblance question as a calibration problem: given just the code & the summary from an LLM, can we compute a confidence measure, that provides a reliable indication of whether the summary sufficiently resembles what a human would have produced in this situation? We examine this question using several LLMs, for several languages, and in several different settings. Our investigation suggests approaches to provide reliable predictions of the likelihood that an LLM-generated summary would sufficiently resemble a summary a human might write for the same code.",
		"container-title": "Proc. ACM Softw. Eng.",
		"DOI": "10.1145/3729400",
		"issue": "FSE",
		"title": "Calibration of Large Language Models on Code Summarization",
		"URL": "https://doi.org/10.1145/3729400",
		"volume": "2",
		"author": [
			{
				"family": "Virk",
				"given": "Yuvraj"
			},
			{
				"family": "Devanbu",
				"given": "Premkumar"
			},
			{
				"family": "Ahmed",
				"given": "Toufique"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2025",
					6
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/N3ZY3UAM",
		"type": "paper-conference",
		"container-title": "2021 IEEE 21st International Working Conference on Source Code Analysis and Manipulation (SCAM)",
		"DOI": "10.1109/SCAM52516.2021.00027",
		"page": "153-164",
		"title": "What Do Developers Discuss about Code Comments?",
		"author": [
			{
				"family": "Rani",
				"given": "Pooja"
			},
			{
				"family": "Birrer",
				"given": "Mathias"
			},
			{
				"family": "Panichella",
				"given": "Sebastiano"
			},
			{
				"family": "Ghafari",
				"given": "Mohammad"
			},
			{
				"family": "Nierstrasz",
				"given": "Oscar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/DJ39RYJ2",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2409.14368",
		"journalAbbreviation": "arXiv preprint arXiv:2409.14368",
		"title": "Evaluating the Quality of Code Comments Generated by Large Language Models for Novice Programmers",
		"author": [
			{
				"family": "Fan",
				"given": "Aysa Xuemo"
			},
			{
				"family": "Narayanan",
				"given": "Arun Balajiee Lekshmi"
			},
			{
				"family": "Hassany",
				"given": "Mohammad"
			},
			{
				"family": "Ke",
				"given": "Jiaze"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/FN5JTV9W",
		"type": "paper-conference",
		"event-title": "Proceedings of the 44th international conference on software engineering",
		"page": "1693-1705",
		"title": "Practitioners' expectations on automated code comment generation",
		"author": [
			{
				"family": "Hu",
				"given": "Xing"
			},
			{
				"family": "Xia",
				"given": "Xin"
			},
			{
				"family": "Lo",
				"given": "David"
			},
			{
				"family": "Wan",
				"given": "Zhiyuan"
			},
			{
				"family": "Chen",
				"given": "Qiuyuan"
			},
			{
				"family": "Zimmermann",
				"given": "Thomas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/4Z69BF6X",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:1909.04352",
		"journalAbbreviation": "arXiv preprint arXiv:1909.04352",
		"title": "Automatic code summarization: A systematic literature review",
		"author": [
			{
				"family": "Zhu",
				"given": "Yuxiang"
			},
			{
				"family": "Pan",
				"given": "Minxue"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/CDW7M9N3",
		"type": "paper-conference",
		"event-title": "2015 ieee 23rd international conference on program comprehension",
		"ISBN": "1-4673-8159-4",
		"page": "255-265",
		"publisher": "IEEE",
		"title": "Eye movements in code reading: Relaxing the linear order",
		"author": [
			{
				"family": "Busjahn",
				"given": "Teresa"
			},
			{
				"family": "Bednarik",
				"given": "Roman"
			},
			{
				"family": "Begel",
				"given": "Andrew"
			},
			{
				"family": "Crosby",
				"given": "Martha"
			},
			{
				"family": "Paterson",
				"given": "James H"
			},
			{
				"family": "Schulte",
				"given": "Carsten"
			},
			{
				"family": "Sharif",
				"given": "Bonita"
			},
			{
				"family": "Tamm",
				"given": "Sascha"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/15747588/items/BR6U3WQM",
		"type": "paper-conference",
		"event-title": "PPIG",
		"page": "5",
		"title": "The roles beacons play in comprehension for novice and expert programmers.",
		"author": [
			{
				"family": "Crosby",
				"given": "Martha E"
			},
			{
				"family": "Scholtz",
				"given": "Jean"
			},
			{
				"family": "Wiedenbeck",
				"given": "Susan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2002"
				]
			]
		}
	}
]